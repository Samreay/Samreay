<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>tutorial on Samuel Hinton</title><link>https://cosmiccoding.com.au/categories/tutorial/</link><description>Recent content in tutorial on Samuel Hinton</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 08 Nov 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://cosmiccoding.com.au/categories/tutorial/index.xml" rel="self" type="application/rss+xml"/><item><title>DataFrame filtering with chaining</title><link>https://cosmiccoding.com.au/tutorials/filter_chaining/</link><pubDate>Tue, 08 Nov 2022 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/filter_chaining/</guid><description>If you&amp;rsquo;re like me, breaking a nice method chain to do filtering is annoying.
You might have a nice long list of operations, and then interupt it like so:
df = ( df_raw.set_index(&amp;#34;A&amp;#34;) .sort_index() .unstack() .groupby(&amp;#34;B&amp;#34;) .mean() ) df = df[df[&amp;#34;C&amp;#34;] == &amp;#34;some_value&amp;#34;] df = df.join(df2).reset_index() I&amp;rsquo;m a simple man, and I just want everything in a single chain!
Yes, in the trivial example above, you could just move the indexor into the chain, but what if you need to filter based a value in the dataframe at that instant, not in the raw dataframe?</description></item><item><title>RoyalRoad Data Infographic</title><link>https://cosmiccoding.com.au/tutorials/royalroad/</link><pubDate>Fri, 14 Oct 2022 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/royalroad/</guid><description>For those reading this, I assume you know what Royal Road is. For those may have misclicked: it&amp;rsquo;s a website where authors publish web serials, generally a chapter at a time, and generally at a freakishly fast pace (like one chapter a day). It&amp;rsquo;s home to many greats in the Progression Fantasy / LitRPG genre, and I have spent hundreds of hours as a consumer of these serials.
So I decided to try and extract some insights from Royal Road data about how page count, followers, ratings, etc, impact the serials success and patreon conversion.</description></item><item><title>Finding the best Wordle opening</title><link>https://cosmiccoding.com.au/tutorials/wordle/</link><pubDate>Wed, 26 Jan 2022 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/wordle/</guid><description>Recently, I&amp;mdash;like many others&amp;mdash;have indulged in the Wordle competitions with friends. Which raised the question of how to open and ensure that I crushed all my mates with my vastly superior score.
So here we are. About to do an unneccessary but fun deep dive into Wordle words.
TL;DR If you care both about getting letters, and getting them in the right location, start with &amp;ldquo;lares&amp;rdquo; or &amp;ldquo;tares&amp;rdquo;.
If you try &amp;ldquo;lares&amp;rdquo; and get nothing, try &amp;ldquo;tonic&amp;rdquo; or &amp;ldquo;point&amp;rdquo;.</description></item><item><title>Merging dicts with the union operator</title><link>https://cosmiccoding.com.au/tutorials/union_operators/</link><pubDate>Sat, 15 May 2021 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/union_operators/</guid><description>Merging two python dictionaries is a very common question on StackOverflow. Some of the better responses have been updated for later versions of Python, but I&amp;rsquo;ve seen this crop up a few times now to write it up here.
In Python 3.9+, merging two dictionaries is now staggeringly easy.
# New in Python 3.9 x = {&amp;#34;hello&amp;#34;: &amp;#34;there&amp;#34;, &amp;#34;general&amp;#34;: &amp;#34;kenobi&amp;#34;} y = {&amp;#34;general&amp;#34;: &amp;#34;potato&amp;#34;, &amp;#34;a&amp;#34;: &amp;#34;b&amp;#34;} z = x | y # z = {&amp;#39;hello&amp;#39;: &amp;#39;there&amp;#39;, &amp;#39;general&amp;#39;: &amp;#39;potato&amp;#39;, &amp;#39;a&amp;#39;: &amp;#39;b&amp;#39;} Notice that the order of preference is right to left.</description></item><item><title>Simple Multiprocessing in Python</title><link>https://cosmiccoding.com.au/tutorials/multiprocessing/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/multiprocessing/</guid><description>In this short writeup I&amp;rsquo;ll give examples of various multiprocessing libraries, how to use them with minimal setup, and what their strengths are.
If you want a TL;DR - I recommend trying out loky for single machine tasks, check out Ray for larger tasks.
# Loky, great for single machine parallelism from loky import get_reusable_executor executor = get_reusable_executor() results = list(executor.map(fn, jobs)) # Ray, great for distributing over machines import ray ray.</description></item><item><title>Low Poly Art</title><link>https://cosmiccoding.com.au/tutorials/lowpoly/</link><pubDate>Mon, 22 Mar 2021 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/lowpoly/</guid><description>So this is the goal for this tutorial - to turn a beautiful wallpaper (or three) into beautiful low poly art.
The approach we are going to take to get to this is fairly simple.
Load our image in Manipulate the image to highlight edges and areas of detail Draw potential vertices from those areas Calculate triangles from vertices Determine the colour of the triangles Plot the image Lets get our imports out of the way and then power through each section:</description></item><item><title>Genetic Algorithms 2: Painting Vermeer</title><link>https://cosmiccoding.com.au/tutorials/genetic_part_two/</link><pubDate>Sat, 20 Mar 2021 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/genetic_part_two/</guid><description>This is what we&amp;rsquo;re going to make in this tutorial. Building on from the previous discussion in part one, we now add population and genetic mixing into the algorithm.
In the prior article we evolved a painting by the process of having a single organism that we mutated over time. We aim to improve this algorithm in this step by adding multiple different organisms into the population, and allowing those organisms to mate and produce offspring.</description></item><item><title/><link>https://cosmiccoding.com.au/tutorials/genetic_part_one/</link><pubDate>Fri, 19 Mar 2021 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/genetic_part_one/</guid><description>This is what we&amp;rsquo;re going to make in this tutorial. It may not look like much, but then again, this is but the first step into genetic algorithms. If you&amp;rsquo;ve guessed that this is the Starry Night (or bothered to read the title, description or anything else), fantastic.
The idea behind Genetic Algorithms is simple - each algorithm can be evaluated to get its fitness, and algorithms are mutated, bred and culled, according to their performance.</description></item><item><title>Linear Regression Regularization Explained</title><link>https://cosmiccoding.com.au/tutorials/linear_regression_regularisation/</link><pubDate>Sun, 27 Dec 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/linear_regression_regularisation/</guid><description>In this small write up, I&amp;rsquo;ll be trying to fill a bit of a void I&amp;rsquo;ve seen online. That is, there are plenty of definitions of lasso, ridge, and elastic regularization, but most of them aren&amp;rsquo;t accompanied by useful examples showing when these terms become critically important! To address this, we&amp;rsquo;re going to look at regularization using three different use cases:
A perfect dataset! White noise on top of a direct linear relationship.</description></item><item><title>An Introduction to Gaussian Processes</title><link>https://cosmiccoding.com.au/tutorials/gaussian_processes/</link><pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/gaussian_processes/</guid><description>In this little write up, we&amp;rsquo;ll explore, construct and utilise Gaussian Processes for some simple interpolation models. The goal is - at the end - to know how they work under the hood, how they are trained, and how you can use them in weird and wonderful ways.
I&amp;rsquo;ll go through some basic interpolation, covariance and correlation concepts first. If they&amp;rsquo;re all familiar to you, scroll down the Gaussian Process section.</description></item><item><title>Principle Component Analaysis Explained</title><link>https://cosmiccoding.com.au/tutorials/pca/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/pca/</guid><description>In this small write up, we&amp;rsquo;ll cover Principal Component Analysis from its mathematical routes, visual explanations and how to effectively utilise PCA using the sklearn library. We&amp;rsquo;ll show PCA works by doing it manually in python without losing ourself in the mathematics!
What is PCA? PCA is a way of taking a dataset (a collection of points in some parameter/feature space) and determining what are the &amp;ldquo;principal components&amp;rdquo;. These principal components represent vectors that encapsulate the information in your dataset.</description></item><item><title>Fuzzy String Matching</title><link>https://cosmiccoding.com.au/tutorials/fuzzy_string_matching/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/fuzzy_string_matching/</guid><description>In this tweet, Steven Rich pointed out that Philadelphia is spelled at least 57 different ways in the PPP load data, and that this represents both a challenge to fix on the back-end, and a perfect example of why you should do as much work on the front-end to get better input.
In this write up, we&amp;rsquo;ll figure out an easy way of fixing up these spelling issues to produce a far better dataset to work on using the python library FuzzyWuzzy.</description></item><item><title>Visualising Patient Contributions for the CCCC</title><link>https://cosmiccoding.com.au/tutorials/cccc_institutions/</link><pubDate>Fri, 31 Jul 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/cccc_institutions/</guid><description>The COVID-19 Critical Care Consortium is an international collaboration of hundreds of hospital sites from dozens of countries around the world. Our sites have been slowly but steadily gathering data for critical cases of COVID-19 since earlier in the year, and we thought it would be good to show just how international the collaboration is.
To that end, in my role as one of the technical leads (responsible for the data pipeline and ingestion), I created out a simple (and no-risk) data product simply containing a list of our participant sites, and how many patients they have enrolled in our study at a given date.</description></item><item><title>Training a Neural Network Embedding Layer with Keras</title><link>https://cosmiccoding.com.au/tutorials/encoding_colours/</link><pubDate>Sun, 26 Jul 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/encoding_colours/</guid><description>This little write is designed to try and explain what embeddings are, and how we can train a naive version of an embedding to understand and visualise the process. We&amp;rsquo;ll do this using a colour dataset, Keras and good old-fashioned matplotlib.
Introduction Let&amp;rsquo;s start simple: What is an embedding?
An embedding is a way to represent some categorical feature (like a word), as a dense parameter. Specifically, this is normally a unit vector in a high dimensional hypersphere.</description></item><item><title>Gource - Visualisating Code Contributions</title><link>https://cosmiccoding.com.au/tutorials/gource/</link><pubDate>Fri, 17 Jul 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/gource/</guid><description>What is Gource? Gource is a way of visualising your git log, showing users and their contributions creating, modifying and deleting files. When run on my repository that was published under the name &amp;ldquo;Steve&amp;rdquo; (a Hierarchical Bayesian model for supernova cosmology&amp;quot;) and then piped through ffmpeg, this is what we get:
Using gource Its super easy, head to the gource website here and download the version for your operating system.</description></item><item><title>PR vs ROC Curves - Which to Use?</title><link>https://cosmiccoding.com.au/tutorials/pr_vs_roc_curves/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/pr_vs_roc_curves/</guid><description>PR curves and ROC diagrams are presented everywhere in the machine learning sphere, and are often used relatively interchangably. In many cases, this is fine, because they are both providing information on the general question &amp;ldquo;How good is my classifier?&amp;rdquo;. But like many things which are often fine to do, there are cases where we&amp;rsquo;d want to specifically show either a PR or ROC curve.
So in the next few sections, we&amp;rsquo;ll generate a mock dataset, generate PR and ROC curves for them for two different classifiers, go into the definition of both of PR and ROC curves (both the intuition and mathematics), and then we&amp;rsquo;ll go how they subtly differ with another code example.</description></item><item><title>An Introduction to Logistic Regression</title><link>https://cosmiccoding.com.au/tutorials/logistic_regression/</link><pubDate>Sat, 11 Jul 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/logistic_regression/</guid><description>In this small write up, we&amp;rsquo;ll cover logistic functions, probabilities vs odds, logit functions, and how to perform logistic regression in Python.
Logistic regression is a method of calculating the probability that an event will pass or fail. That is, we utilise it for dichotomous results - 0 and 1, pass or fail. Is this patient going to survive or not? Is this email spam or not? This is specifically called binary logistic regression, and is important to note because we can do logistic regression in other contexts.</description></item><item><title>Introduction to Naive Bayes</title><link>https://cosmiccoding.com.au/tutorials/naivebayes/</link><pubDate>Sat, 11 Jul 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/naivebayes/</guid><description>Naive Bayes is a simple, extraordinarily fast, and incredibly useful categorical classification tool. The underlying crux of Naive Bayes is that we assume feature independence, plug it into Bayes theorem, and the math that falls out is incredibly simple.
The background math Let&amp;rsquo;s start with the famous Bayes Theorem:
$$ P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)} $$
On the left hand side, we have the posterior: the probability of the output given the input.</description></item><item><title>Evolution of US COVID-19 cases</title><link>https://cosmiccoding.com.au/tutorials/us_covid_evolution/</link><pubDate>Thu, 09 Jul 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/us_covid_evolution/</guid><description>This plot was first inspired by this absolute masterpiece of a visualisation. I was chatting with David Morton about how much work it must have taken (coding up textures, exporting to blender, creating the model, rendering, compositing, loading back into code, annotating, so much work). David then went and created a python only version of the plot, and I decided I&amp;rsquo;d give it a crack too. We&amp;rsquo;ve used the same data, but the transformations, plotting and output are all different.</description></item><item><title>Which Big City has the best Weather?</title><link>https://cosmiccoding.com.au/tutorials/us_weather/</link><pubDate>Mon, 06 Jul 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/us_weather/</guid><description>Like many people, I might one day be moving to the United States for work. However, I would like to know where I should move too. Where is too hot? Where is it too wet? Where is my little slice of paradise??
This is what we&amp;rsquo;ll be making:
So using the NOAA Global Summary of the Day and the top 25 most populated cities in the US, we&amp;rsquo;ve got enough to make a plot.</description></item><item><title>Plotting US COVID-19 Incidence</title><link>https://cosmiccoding.com.au/tutorials/us_covid19_growth/</link><pubDate>Sun, 05 Jul 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/us_covid19_growth/</guid><description>One of my favourite subreddits is /r/dataisbeautiful. Today I saw a plot from that sub that was, on first glimpse, beautiful! And full credit to the author for getting the data and presenting it beautifully. However, I had some small issues with it. The first was that it was smoothing over a cumulative window (instead of showing what I think people expect, which is how many people caught COVID-19 on those dates).</description></item><item><title>Getting started with Boost, Python, and Numpy</title><link>https://cosmiccoding.com.au/tutorials/boost/</link><pubDate>Sat, 04 Jul 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/boost/</guid><description>As much as we might want all code that we work with to be professional-quality packages in the same language we are working in, it simply won&amp;rsquo;t be the case for the vast majority of us.
Recently, I was investigating how we could replace some perl wrappers that executed C code with something more pythonic. I came across boost - a package designed to try and make exposing C/C++ interfaces a bit simpler to python programs.</description></item><item><title>Trivial One Hot Encoding in Python</title><link>https://cosmiccoding.com.au/tutorials/one_hot_encoding/</link><pubDate>Thu, 25 Jun 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/one_hot_encoding/</guid><description>One hot encoding is something we do very commonly in machine learning, where we want to turn a categorical feature into a vector of ones and zeros that algorithms can make much easier sense of.
For example, take this toy example dataframe of people and their favourite food. At the moment, it&amp;rsquo;s useless to us.
import pandas as pd df = pd.DataFrame({ &amp;#34;Person&amp;#34;: [&amp;#34;Sam&amp;#34;, &amp;#34;Ali&amp;#34;, &amp;#34;Jane&amp;#34;, &amp;#34;John&amp;#34;], &amp;#34;FavFood&amp;#34;: [&amp;#34;Pizza&amp;#34;, &amp;#34;Vegetables&amp;#34;, &amp;#34;Cake&amp;#34;, &amp;#34;Happiness&amp;#34;] }).</description></item><item><title>Monte-Carlo Integration made easy</title><link>https://cosmiccoding.com.au/tutorials/monte_carlo_integration/</link><pubDate>Sun, 21 Jun 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/monte_carlo_integration/</guid><description>Integrating a function is tricky. A lot of the time, the math is beyond us. Or beyond me, at the very least, and so I turn to my computer, placing the burden on its silent, silicon shoulders. And yet this isn&amp;rsquo;t the end of it, because there are a host of ways to perform numerical integration.
Do we want the simple rectangle rule? The superior trapezoidal rule? Simpson&amp;rsquo;s rule? Do we want to adaptively sample?</description></item><item><title>Handy Python Decorators</title><link>https://cosmiccoding.com.au/tutorials/handy_python_decorators/</link><pubDate>Sat, 20 Jun 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/handy_python_decorators/</guid><description>Decorators are something that are criminally underused in the analysis codes I have seen in academia. So, give me a few seconds to try and espouse their virtues. First, if you are new to decorators, how they work is simple: they are a function that returns a function which has wrapped another function. Simple!
It&amp;rsquo;s easier to explain in code.
def decorator(fn): def wrapper(*args, **kwargs): print(&amp;#34;Look, I&amp;#39;ve added something here!&amp;#34;) return fn(*args, **kwargs) return wrapper @decorator def add(a, b): return a + b add(1, 2) Look, I've added something here!</description></item><item><title>A visual tutorial of Inversion Sampling</title><link>https://cosmiccoding.com.au/tutorials/inversion_sampling/</link><pubDate>Fri, 19 Jun 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/inversion_sampling/</guid><description>Inversion sampling is a simple and very efficient way to generate samples of some arbitrary probability function. Unlike rejection sampling, there are no rejected samples, and by leveraing common python libraries, we don&amp;rsquo;t even have to do any tricky integrals or function inversion.
$$ P(x) = 2 x $$
between 0 and 1. It just happens this does integrate to 1, what luck! Now, the CDF (cumulative distribution function) is the integral of the probability density function (what you see above), integrated from the lower bound - in our case the bounds are 0 to 1, so the CDF is</description></item><item><title>One Line Python Singletons</title><link>https://cosmiccoding.com.au/tutorials/simple_singletons/</link><pubDate>Thu, 18 Jun 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/simple_singletons/</guid><description>Singletons are the idea that you want to get an instance of a class, but you only ever want there to be a single instance of that class instantiated. A nice example might be a configuration class, or a coordinator class that is used by various pieces of you code, and you want to make sure that they&amp;rsquo;re all pointing to the same instance without having to pass the object around in eery invocation!</description></item><item><title>A visual tutorial of Rejection Sampling</title><link>https://cosmiccoding.com.au/tutorials/rejection_sampling/</link><pubDate>Tue, 16 Jun 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/rejection_sampling/</guid><description>Rejection sampling is the conceptually simplest way to generate samples of some arbitrary probability function without having to do any transformations. No integration, no trickery, you simply trade computational efficiency away to keep everything as simple as possible.
Let&amp;rsquo;s pick a super simple example: let&amp;rsquo;s say you want to sample from the function
$$ f(x) = 1.2 - x^4 $$
between 0 and 1. It just happens this does integrate to 1, what luck!</description></item><item><title>Removing Empty Columns</title><link>https://cosmiccoding.com.au/tutorials/removing_empty_data/</link><pubDate>Tue, 16 Jun 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/removing_empty_data/</guid><description>The sad reality of life as a data scientist is we spend too much time cleaning and processing data. And a lot of the time, our data contains some features which simply need to go in the bin. A column of 1000 values with only 2 entries is probably not going to be useful, after all. Let&amp;rsquo;s make some data to illustrate this:
import pandas as pd import numpy as np data = np.</description></item><item><title>Forward Modelling for Supernova Cosmology</title><link>https://cosmiccoding.com.au/tutorials/forwardmodelling/</link><pubDate>Wed, 08 Apr 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/forwardmodelling/</guid><description>So in this example, I&amp;rsquo;m simply trying to demonstrate the viability of forward models with supernova cosmology. In this, I&amp;rsquo;ll be simplifying a lot, and in this case, ignoring proper treatment of uncertainty contributions from Monte-Carlo uncertainty. We&amp;rsquo;ll start with a simple model in which simulated supernova have redshift, magnitude and colour, apply selection effects, and ensure that we can recover input cosmology without having to resimulate by exploiting the fact that our selection effects operate in the observer frame of reference.</description></item><item><title>A/B Test Significance in Python</title><link>https://cosmiccoding.com.au/tutorials/abtests/</link><pubDate>Sun, 12 Jan 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/abtests/</guid><description>Recently I was asked to talk about A/B tests for my Python for Statistical Analysis course. Given my travel schedule, leaving me bereft of my microphone, I thought it would be better to condense down A/B tests into a tutorial or two.
In this little write up, we&amp;rsquo;ll cover what an A/B test is, run through it in first principles with frequentist hypothesis testing, apply some existing scipy tests to speed the process up, and then at the end we&amp;rsquo;ll approach the problem in a Bayesian framework.</description></item><item><title>Visualising with Basemap</title><link>https://cosmiccoding.com.au/tutorials/desinstitutions/</link><pubDate>Sat, 10 Aug 2019 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/desinstitutions/</guid><description>I&amp;rsquo;m scheduled to give a talk for the Dark Energy Survey - a massive collection of researchers, engineers and institutions working together to try and constrain the nature of Dark Energy. To help give my presentations some pizzaz, I thought it would be nice to visualise the spread of institutions around the world that makes DES the wonderful organisation that it is.
Wheres the data Unfortunately&amp;hellip; I had to collect it by hand.</description></item><item><title>Propagating (non-gaussian) uncertainty</title><link>https://cosmiccoding.com.au/tutorials/propagating/</link><pubDate>Fri, 02 Aug 2019 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/propagating/</guid><description>You&amp;rsquo;ve fit a model to some data. You have some mean, covariance or samples characterising that fit, but now how can you clearly show the $1$ and $2\sigma$ confidence regions of this fit back onto your model. In other words, how do you get this sort of image back at the end:
Let&amp;rsquo;s run through a bunch of different ways, depending on how you&amp;rsquo;ve fit your model to the data. Firstly, let&amp;rsquo;s generate a model and some fake data to fit it with.</description></item><item><title>Bayesian Sample Selection Effects</title><link>https://cosmiccoding.com.au/tutorials/sampleselectionbias/</link><pubDate>Tue, 30 Jul 2019 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/sampleselectionbias/</guid><description>In a perfect world our experiments would capture all the data that exists. This is not a perfect world, and we miss a lot of data. Let&amp;rsquo;s consider one method of accounting for this in a Bayesian formalism - integrating it out.
Let&amp;rsquo;s begin with a motivational dataset.
So it looks like for our example data, we&amp;rsquo;ve got some gaussian-like distribution of $x$ observations, but at some point it seems like our instrument is unable to pick up the observations.</description></item><item><title>Bayesian Linear Regression in Python</title><link>https://cosmiccoding.com.au/tutorials/bayesianlinearregression/</link><pubDate>Sat, 27 Jul 2019 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/bayesianlinearregression/</guid><description>Bayesian linear regression is a common topic, but allow me to put my own spin on it. We&amp;rsquo;ll start at generating some data, defining a model, fitting it and plotting the results. It shouldn&amp;rsquo;t take long.
Generating Data Let&amp;rsquo;s start by generating some experimental data. For simplicity, let us assume some underlying process generates samples $f(x) = mx + c$ and our observations have some given Gaussian error $\sigma$.
import matplotlib.</description></item></channel></rss>