<!doctype html><html lang=en-gb><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>An Introduction to Gaussian Processes - Samuel Hinton</title><link rel=stylesheet href="https://cosmiccoding.com.au/css/main.min.7fbf65bef988e0cecd6d148a59756085deed3c480729d260d0a30b282b1d7da8.css" integrity="sha256-f79lvvmI4M7NbRSKWXVghd7tPEgHKdJg0KMLKCsdfag="><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel="shortcut icon" href=https://cosmiccoding.com.au/img/favicon.png type=image/x-icon></head><meta name=description content="Welcome to the wonderful world of non-parametric models and kernel functions."><meta name=robots content="noodp"><link rel=canonical href=https://cosmiccoding.com.au/tutorials/gaussian_processes/><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://cosmiccoding.com.au/tutorials/gaussian_processes/cover.png"><meta name=twitter:title content="An Introduction to Gaussian Processes"><meta name=twitter:description content="Welcome to the wonderful world of non-parametric models and kernel functions."><meta property="og:title" content="An Introduction to Gaussian Processes"><meta property="og:description" content="Welcome to the wonderful world of non-parametric models and kernel functions."><meta property="og:type" content="article"><meta property="og:url" content="https://cosmiccoding.com.au/tutorials/gaussian_processes/"><meta property="og:image" content="https://cosmiccoding.com.au/tutorials/gaussian_processes/cover.png"><meta property="article:section" content="tutorials"><meta property="article:published_time" content="2020-09-10T00:00:00+00:00"><meta property="article:modified_time" content="2020-09-10T00:00:00+00:00"><meta property="article:section" content="tutorial"><meta property="article:published_time" content="2020-09-10 00:00:00 +0000 UTC"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"An Introduction to Gaussian Processes","genre":"tutorial","url":"https:\/\/cosmiccoding.com.au\/tutorials\/gaussian_processes\/","datePublished":"2020-09-10 00:00:00 \u002b0000 UTC","description":"Welcome to the wonderful world of non-parametric models and kernel functions.","author":{"@type":"Person","name":""}}</script><body><div class="flex flex-col min-h-screen overflow-hidden"><header class="absolute w-full z-30"><div class="max-w-6xl mx-auto px-4 sm:px-6"><div class="flex items-center justify-between h-20"><div class="flex-shrink-0 mr-4"><a class=block href=/ aria-label="Samuel Hinton"><h2 class=logo>SRH</h2></a></div><nav class="hidden md:flex md:flex-grow"><ul class="flex flex-grow justify-end flex-wrap items-center"><li><a href=/#books class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Books</a></li><li><a href=/reviews class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Reviews</a></li><li><a href=/tutorials class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Tutorials</a></li><li><a href=/blogs class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Blog</a></li><li><a href=/#courses class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Courses</a></li><li><a href=/static/resume/HintonCV.pdf class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">CV</a></li></ul></nav><div class=md:hidden x-data="{ expanded: false }"><button class=hamburger :class="{ 'active': expanded }" @click.stop="expanded = !expanded" aria-controls=mobile-nav :aria-expanded=expanded>
<span class=sr-only>Menu</span><svg class="w-6 h-6 fill-current text-gray-300 hover:text-gray-200 transition duration-150 ease-in-out" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><rect y="4" width="24" height="2" rx="1"/><rect y="11" width="24" height="2" rx="1"/><rect y="18" width="24" height="2" rx="1"/></svg></button><nav id=mobile-nav class="absolute top-full z-20 left-0 w-full px-4 sm:px-6 overflow-hidden transition-all duration-300 ease-in-out" x-ref=mobileNav :style="expanded ? 'max-height: ' + $refs.mobileNav.scrollHeight + 'px; opacity: 1' : 'max-height: 0; opacity: .8'" @click.away="expanded = false" @keydown.escape.window="expanded = false" x-cloak><ul class="bg-gray-800 px-4 py-2"><li><a href=/#books class="flex text-gray-300 hover:text-gray-200 py-2">Books</a></li><li><a href=/reviews class="flex text-gray-300 hover:text-gray-200 py-2">Reviews</a></li><li><a href=/tutorials class="flex text-gray-300 hover:text-gray-200 py-2">Tutorials</a></li><li><a href=/blogs class="flex text-gray-300 hover:text-gray-200 py-2">Blog</a></li><li><a href=/#courses class="flex text-gray-300 hover:text-gray-200 py-2">Courses</a></li><li><a href=/static/resume/HintonCV.pdf class="flex text-gray-300 hover:text-gray-200 py-2">CV</a></li></ul></nav></div></div></div></header><div class=flex-grow><div id=post-container class="content content-wider blog-post relative"><div class="section-header blog"><h1 class=title>An Introduction to Gaussian Processes</h1><p>6th September 2020</p><p>Welcome to the wonderful world of non-parametric models and kernel functions.</p></div><div><ul class="grid gap-6 w-full md:grid-cols-2" style=list-style:none;padding-left:0><li><input type=radio id=show-code name=code-toggle value=show-code class="hidden peer" onchange=clickCheckbox(this) required checked>
<label for=show-code class="inline-flex justify-between items-center p-5 w-full text-gray-500 bg-gray-800 rounded-lg border border-gray-200 cursor-pointer peer-checked:border-2 peer-checked:border-main-300 peer-checked:text-white peer-checked:bg-gray-700 hover:text-gray-200 hover:bg-gray-700"><div class="block w-full"><div class="w-full text-center text-lg font-semibold">Show me everything!</div><div class="w-full text-center text-sm">Oh yeah, coding time.</div></div></label></li><li><input type=radio id=hide-code name=code-toggle value=hide-code class="hidden peer" onchange=clickCheckbox(this)>
<label for=hide-code class="inline-flex justify-between items-center p-5 w-full text-gray-500 bg-gray-800 rounded-lg border border-gray-200 cursor-pointer peer-checked:border-2 peer-checked:border-main-300 peer-checked:text-white peer-checked:bg-gray-700 hover:text-gray-200 hover:bg-gray-700"><div class="block w-full"><div class="w-full text-center text-lg font-semibold">Just the plots</div><div class="w-full text-center text-sm">Code is nasty.</div></div></label></li></ul></div><script>function clickCheckbox(e){e.id=="show-code"?document.getElementById("post-container").classList.remove("hide-code"):document.getElementById("post-container").classList.add("hide-code")}</script><p>In this little write up, we&rsquo;ll explore, construct and utilise Gaussian Processes for some simple interpolation models. The goal is - at the end - to know how they work under the hood, how they are trained, and how you can use them in weird and wonderful ways.</p><p>I&rsquo;ll go through some basic interpolation, covariance and correlation concepts first. If they&rsquo;re all familiar to you, scroll down the Gaussian Process section.</p><h1 id=introduction>Introduction</h1><p>To get the basics, we&rsquo;ll cover a) generating some data to play with, b) constructing a covariance matrix, and c) how drawing random numbers using said covariance matrix enumlates a smooth process.</p><h2 id=generating-data>Generating data</h2><p>Let&rsquo;s do a tiny bit of leg work before jumping into Gaussian Processes in full. Let&rsquo;s get some data to use first before we complicate things! What we&rsquo;ll do now is define some <em>funky</em> function, and then visualise what it looks like. On top of that, we&rsquo;ll pick a few points on the function that we can treat like data samples.</p><div class="expanded-code width-86" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>import</span> numpy <span style=color:#ff6ac1>as</span> np
</span></span><span style=display:flex><span><span style=color:#ff6ac1>import</span> matplotlib.pyplot <span style=color:#ff6ac1>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Our funky function here</span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>fn</span>(xs):
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> np<span style=color:#ff6ac1>.</span>exp((xs <span style=color:#ff6ac1>+</span> <span style=color:#ff9f43>10.5</span>)<span style=color:#ff6ac1>**</span><span style=color:#ff9f43>0.1</span>) <span style=color:#ff6ac1>+</span> np<span style=color:#ff6ac1>.</span>sin(xs) <span style=color:#ff6ac1>/</span> (xs <span style=color:#ff6ac1>+</span> <span style=color:#ff9f43>1</span>) <span style=color:#ff6ac1>+</span> np<span style=color:#ff6ac1>.</span>cos(<span style=color:#ff9f43>2.5</span> <span style=color:#ff6ac1>*</span> xs<span style=color:#ff6ac1>**</span><span style=color:#ff9f43>0.5</span>)<span style=color:#ff6ac1>**</span><span style=color:#ff9f43>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Lets generate both the good visualisation of the function and some samples</span>
</span></span><span style=display:flex><span>xs <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>array([<span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>2</span>, <span style=color:#ff9f43>4</span>, <span style=color:#ff9f43>5</span>, <span style=color:#ff9f43>7</span>, <span style=color:#ff9f43>9</span>, <span style=color:#ff9f43>10</span>])
</span></span><span style=display:flex><span>x_fine <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>linspace(<span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>10</span>, <span style=color:#ff9f43>200</span>)
</span></span><span style=display:flex><span>ys, y_fine <span style=color:#ff6ac1>=</span> fn(xs), fn(x_fine)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># And plot it out</span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(x_fine, y_fine, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;#ccc&#34;</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Function&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>scatter(xs, ys, s<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>30</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Samples&#34;</span>, zorder<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>20</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend();
</span></span></code></pre></div></div><p><div><figure class=rounded><picture><source srcset=/tutorials/gaussian_processes/2020-09-10-Gaussian_Processes_files/2020-09-10-Gaussian_Processes_1_0_hub0f2aa864fb4540dcab1bfdc4f88992d_103988_1920x0_resize_q90_h2_box_3.webp width=1920 height=997 type=image/webp><source srcset=/tutorials/gaussian_processes/2020-09-10-Gaussian_Processes_files/2020-09-10-Gaussian_Processes_1_0.png width=2693 height=1398 type=image/png><img width=2693 height=1398 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>Okay, so the idea here is that we are going to try and find some way of interpolating between the points we have (the samples), such that we can try and recover the underlying function in some way at least, but to do it in a way that <strong>has</strong> uncertainty on it.</p><h2 id=traditional-interpolation>Traditional interpolation</h2><p>Let&rsquo;s see what traditional methods look like, by using <code>scipy</code> to do basic interpolation:</p><div class="reduced-code width-53" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>from</span> scipy.interpolate <span style=color:#ff6ac1>import</span> interp1d
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>y_linear <span style=color:#ff6ac1>=</span> interp1d(xs, ys, kind<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;linear&#34;</span>)(x_fine)
</span></span><span style=display:flex><span>y_quad <span style=color:#ff6ac1>=</span> interp1d(xs, ys, kind<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;quadratic&#34;</span>)(x_fine)
</span></span><span style=display:flex><span>y_cubic <span style=color:#ff6ac1>=</span> interp1d(xs, ys, kind<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;cubic&#34;</span>)(x_fine)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>scatter(xs, ys, s<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>30</span>, zorder<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>20</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Samples&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(x_fine, y_fine, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Function&#34;</span>, color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;k&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(x_fine, y_linear, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Linear&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(x_fine, y_quad, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Quadratic&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(x_fine, y_cubic, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Cubic&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend(ncol<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>3</span>);
</span></span></code></pre></div></div><p><div><figure class=rounded><picture><source srcset=/tutorials/gaussian_processes/2020-09-10-Gaussian_Processes_files/2020-09-10-Gaussian_Processes_3_0_hu4a9e39de8a051ab8808d7b29d7b2207e_197694_1920x0_resize_q90_h2_box_3.webp width=1920 height=998 type=image/webp><source srcset=/tutorials/gaussian_processes/2020-09-10-Gaussian_Processes_files/2020-09-10-Gaussian_Processes_3_0.png width=2731 height=1420 type=image/png><img width=2731 height=1420 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>Well, obviously linear isn&rsquo;t a good fit. Quadratic and cubic dont look too bad, but there&rsquo;s no uncertainty on our interpolated values, so its hard for us to know how good (accurate) our interpolation is at any point along the x-axis.</p><h2 id=covariance-matrices>Covariance matrices?</h2><p>Let&rsquo;s now completely change topics. You&rsquo;ll see why soon. Let&rsquo;s create a highly correlated covariance matrix. If this is new terminology for you, a covariance matrix describes the uncertainty on your data, and how the uncertainty on one point is related to the uncertainty on another point. If you have data points which are completely independent, the matrix is just a diagonal line (off-diagonal elements are zero). But if two data points right next to each other are related, then the cell corresponding to that pair won&rsquo;t be zero.</p><p>I digress, here&rsquo;s a covariance matrix which has values that decay as you move away from the diagonal:</p><div class=width-61 markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>import</span> seaborn <span style=color:#ff6ac1>as</span> sb
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>get_cov</span>(size<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>20</span>, length<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>50</span>):    
</span></span><span style=display:flex><span>    x <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>arange(size)
</span></span><span style=display:flex><span>    cov <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>exp(<span style=color:#ff6ac1>-</span>(<span style=color:#ff9f43>1</span> <span style=color:#ff6ac1>/</span> length) <span style=color:#ff6ac1>*</span> (x <span style=color:#ff6ac1>-</span> np<span style=color:#ff6ac1>.</span>atleast_2d(x)<span style=color:#ff6ac1>.</span>T)<span style=color:#ff6ac1>**</span><span style=color:#ff9f43>2</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> cov
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cov <span style=color:#ff6ac1>=</span> get_cov()
</span></span><span style=display:flex><span>cov2 <span style=color:#ff6ac1>=</span> get_cov(length<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>5</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fig, axes <span style=color:#ff6ac1>=</span> plt<span style=color:#ff6ac1>.</span>subplots(ncols<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>, figsize<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>8</span>, <span style=color:#ff9f43>3</span>))
</span></span><span style=display:flex><span>axes[<span style=color:#ff9f43>0</span>]<span style=color:#ff6ac1>.</span>set_title(<span style=color:#5af78e>&#34;Small correlation length&#34;</span>, fontsize<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>14</span>)
</span></span><span style=display:flex><span>axes[<span style=color:#ff9f43>1</span>]<span style=color:#ff6ac1>.</span>set_title(<span style=color:#5af78e>&#34;Large correlation length&#34;</span>, fontsize<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>14</span>)
</span></span><span style=display:flex><span>sb<span style=color:#ff6ac1>.</span>heatmap(cov2, ax<span style=color:#ff6ac1>=</span>axes[<span style=color:#ff9f43>0</span>], square<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span>sb<span style=color:#ff6ac1>.</span>heatmap(cov, ax<span style=color:#ff6ac1>=</span>axes[<span style=color:#ff9f43>1</span>], square<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>);
</span></span></code></pre></div></div><p><div><figure class=rounded><picture><source srcset=/tutorials/gaussian_processes/2020-09-10-Gaussian_Processes_files/2020-09-10-Gaussian_Processes_5_0_hu428950d8d52865deea7f5363a38e5e35_81965_1920x0_resize_q90_h2_box_3.webp width=1920 height=848 type=image/webp><source srcset=/tutorials/gaussian_processes/2020-09-10-Gaussian_Processes_files/2020-09-10-Gaussian_Processes_5_0.png width=2675 height=1182 type=image/png><img width=2675 height=1182 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>To try and illustrate what covariance does, we can actually use <code>scipy</code> to generate data <em>according</em> to that covariance matrix. By tweaking the <code>length</code> argument, you can see that the longer the correlation length, the smoother the generated data trend becomes:</p><div class="reduced-code width-49" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>from</span> scipy.stats <span style=color:#ff6ac1>import</span> multivariate_normal <span style=color:#ff6ac1>as</span> mn
</span></span><span style=display:flex><span>x <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>arange(<span style=color:#ff9f43>20</span>)
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> l <span style=color:#ff6ac1>in</span> [<span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>10</span>, <span style=color:#ff9f43>50</span>, <span style=color:#ff9f43>200</span>]:
</span></span><span style=display:flex><span>    cov <span style=color:#ff6ac1>=</span> get_cov(length<span style=color:#ff6ac1>=</span>l)
</span></span><span style=display:flex><span>    rvs <span style=color:#ff6ac1>=</span> mn<span style=color:#ff6ac1>.</span>rvs(cov<span style=color:#ff6ac1>=</span>cov)
</span></span><span style=display:flex><span>    plt<span style=color:#ff6ac1>.</span>plot(x, rvs, <span style=color:#5af78e>&#39;o-&#39;</span>, ms<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>5</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>f</span><span style=color:#5af78e>&#34;L=</span><span style=color:#5af78e>{</span>l<span style=color:#5af78e>}</span><span style=color:#5af78e>&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend(ncol<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>);
</span></span></code></pre></div></div><p><div><figure class=rounded><picture><source srcset=/tutorials/gaussian_processes/2020-09-10-Gaussian_Processes_files/2020-09-10-Gaussian_Processes_7_0_hu73bb640c27739a54494c7d0b81deb373_166458_1920x0_resize_q90_h2_box_3.webp width=1920 height=1001 type=image/webp><source srcset=/tutorials/gaussian_processes/2020-09-10-Gaussian_Processes_files/2020-09-10-Gaussian_Processes_7_0.png width=2675 height=1394 type=image/png><img width=2675 height=1394 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>The take away from this is simply that increasing correlation length means you can generate smooth transitions rather than jagged, uncorrelated data. This correlation length is one of the pieces of the puzzle when creating a Gaussian process.</p><p>Let&rsquo;s say that you have a fixed start and end point you want to interpolate between. Linear interpolation will just draw a straight line. But we can use the magic of drawing correlated data to produce some uncertainty on our interpolation. (The unspoken piece here is that we have to determine the right correlation value when training a Gaussian process. We&rsquo;ll do that in a tick.)</p><div class=width-70 markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#78787e># In the previous code, we made one set of points four times in a loop</span>
</span></span><span style=display:flex><span><span style=color:#78787e># Here, we generate 500 sets of our data points in one go</span>
</span></span><span style=display:flex><span>rvs <span style=color:#ff6ac1>=</span> mn<span style=color:#ff6ac1>.</span>rvs(cov<span style=color:#ff6ac1>=</span>cov, size<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>500</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Set the start and end points the same for all realisations</span>
</span></span><span style=display:flex><span>rvs <span style=color:#ff6ac1>-=</span> np<span style=color:#ff6ac1>.</span>linspace(rvs[:, <span style=color:#ff9f43>0</span>], rvs[:, <span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>], x<span style=color:#ff6ac1>.</span>size)<span style=color:#ff6ac1>.</span>T
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Get the mean and std of our 500 realisations</span>
</span></span><span style=display:flex><span>mean <span style=color:#ff6ac1>=</span> rvs<span style=color:#ff6ac1>.</span>mean(axis<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>)
</span></span><span style=display:flex><span>std <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>std(rvs, axis<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Plot the mean and std</span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(x, mean, <span style=color:#5af78e>&#34;o-&#34;</span>, ms<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>5</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;mean&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>fill_between(x, mean <span style=color:#ff6ac1>+</span> std, mean <span style=color:#ff6ac1>-</span> std, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.4</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;std&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># And for fun, lets plot 100 of the data realisations</span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> i <span style=color:#ff6ac1>in</span> <span style=color:#ff5c57>range</span>(<span style=color:#ff9f43>100</span>):
</span></span><span style=display:flex><span>    plt<span style=color:#ff6ac1>.</span>plot(x, rvs[i, :], ls<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;-&#34;</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.2</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.7</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend();
</span></span></code></pre></div></div><p><div><figure class=rounded><picture><source srcset=/tutorials/gaussian_processes/2020-09-10-Gaussian_Processes_files/2020-09-10-Gaussian_Processes_9_0_hu22467092f9c69407b7b7624f044ef7fa_895755_1920x0_resize_q90_h2_box_3.webp width=1920 height=980 type=image/webp><source srcset=/tutorials/gaussian_processes/2020-09-10-Gaussian_Processes_files/2020-09-10-Gaussian_Processes_9_0.png width=2731 height=1394 type=image/png><img width=2731 height=1394 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>What we&rsquo;ve done is effectively interpolated between these two points using some covariance matrix, and that covariance matrix has allowed us to produce not just the interpolation, but an uncertainty on it as well (shown by the mean and std above). The small black lines are showing independent &ldquo;realisations&rdquo; of the random draws, the ensemble of which determine the mean and standard deviation of our output prediction.</p><p>The keen eyed among you will have seen some form of problem - we have 20 points in the x-axis here, and our covariance matrix we generated before is a 20x20 matrix. There are infinite real numbers between 0 and 20, so how on earth will we construct an infinite matrix to do this in practise? This is in fact entirely what the &ldquo;non-parametric&rdquo; part of Gaussian Processes refers to. Not that there are no parameters, but that there are an infinite/arbitrary number of them.</p><p>To try and hammer this part home, by drawing over and over we can come up with some mean prediction and some standard deviation on it. This is dependent on our choice for covariance. The covariance we have used has only one parameter, its length scale. The larger $l$ gets the smoother and less changing the predictions become. This number that we &lsquo;pick&rsquo; is a model parameter - it&rsquo;s something we fit when creating a real GP.</p><p>And in terms of nomenclature, moving from a multivariate Gaussian to a Gaussian process means we move from having a mean to having a mean function (because we can ask for the mean at any x). Similarly the covariance matrix depends on the input data points (in our previous example we didnt have any, just hack fixed the first and last point) and the points we want to evaluate at, so we have a covariance function. The shape of the covariance (ie we had squared distance if you look at the <code>get_cov</code> function) is called the kernel (you can have different functions, squared distance is just a useful one).</p><h1 id=gaussian-processes>Gaussian Processes</h1><p>Hopefully the above is enough of an introduction to covariance and correlated draws. Gaussian processes work by training a model, which is fitting the parameters of the specific kernel that you provide. The difficulty is in knowing what kernel to construct and then let the model train. This kernel essentially relates how every data point affects regions in parameter space.</p><p>All this kernel says is that if you are interpolating at point x, which lies between point A and point B, you determine how much A contributes and then do the same for B (plus all other points too). This is another important concept - in normal interpolation, only A and B would affect your result if you ask for a point between A and B. In a Gaussian Process, every point affects you!</p><p>So lets start with a super simple kernel function - the exponential falloff with distance we did ourselves before. This is implemented in <code>scikit-learn</code> as RBF (Radial Basis Function). The training part of this is to find the best value of the length scale in the kernel that gives the best results.</p><div class="reduced-code width-53" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#78787e># To refresh your mind on our data</span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>scatter(xs, ys, s<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>20</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Samples&#34;</span>, zorder<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>30</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(x_fine, y_fine, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Function&#34;</span>, color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend();
</span></span></code></pre></div></div><p><div><figure class=rounded><picture><source srcset=/tutorials/gaussian_processes/2020-09-10-Gaussian_Processes_files/2020-09-10-Gaussian_Processes_12_0_hu96c47b2e6c2881088fce6e1b10c9c5b8_95969_1920x0_resize_q90_h2_box_3.webp width=1920 height=997 type=image/webp><source srcset=/tutorials/gaussian_processes/2020-09-10-Gaussian_Processes_files/2020-09-10-Gaussian_Processes_12_0.png width=2693 height=1398 type=image/png><img width=2693 height=1398 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>And now to utilise the Gaussian process from sklearn. Let&rsquo;s make 3 different models.</p><div class="expanded-code width-81" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>from</span> sklearn.gaussian_process <span style=color:#ff6ac1>import</span> GaussianProcessRegressor
</span></span><span style=display:flex><span><span style=color:#ff6ac1>import</span> sklearn.gaussian_process.kernels <span style=color:#ff6ac1>as</span> k
</span></span><span style=display:flex><span><span style=color:#78787e># Note I am NOT training these models. You can see in the bounds</span>
</span></span><span style=display:flex><span><span style=color:#78787e># I am forcing them to have specific lengths. Normally, they would </span>
</span></span><span style=display:flex><span><span style=color:#78787e># fit to your data</span>
</span></span><span style=display:flex><span>kernels <span style=color:#ff6ac1>=</span> [
</span></span><span style=display:flex><span>    k<span style=color:#ff6ac1>.</span>RBF(length_scale<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1.0</span>, length_scale_bounds<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>1.0</span>, <span style=color:#ff9f43>1.01</span>)), 
</span></span><span style=display:flex><span>    k<span style=color:#ff6ac1>.</span>RBF(length_scale<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2.0</span>, length_scale_bounds<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>2.0</span>, <span style=color:#ff9f43>2.01</span>)),
</span></span><span style=display:flex><span>    k<span style=color:#ff6ac1>.</span>RBF(length_scale<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>20.0</span>, length_scale_bounds<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>20.0</span>, <span style=color:#ff9f43>20.01</span>)),
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Make some axes please</span>
</span></span><span style=display:flex><span>fig, axes <span style=color:#ff6ac1>=</span> plt<span style=color:#ff6ac1>.</span>subplots(figsize<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>10</span>, <span style=color:#ff9f43>4</span>), ncols<span style=color:#ff6ac1>=</span><span style=color:#ff5c57>len</span>(kernels), sharey<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span>fig<span style=color:#ff6ac1>.</span>subplots_adjust(wspace<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>, hspace<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> kernel, ax <span style=color:#ff6ac1>in</span> <span style=color:#ff5c57>zip</span>(kernels, axes):
</span></span><span style=display:flex><span>    <span style=color:#78787e># Fit the GP, which doesnt do much as we fixed the length_scale</span>
</span></span><span style=display:flex><span>    gp <span style=color:#ff6ac1>=</span> GaussianProcessRegressor(kernel<span style=color:#ff6ac1>=</span>kernel)
</span></span><span style=display:flex><span>    gp<span style=color:#ff6ac1>.</span>fit(np<span style=color:#ff6ac1>.</span>atleast_2d(xs)<span style=color:#ff6ac1>.</span>T, ys)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#78787e># The 2D and [:, None] stuff is because the object expects 2D data in, not 1D</span>
</span></span><span style=display:flex><span>    y_mean, y_std <span style=color:#ff6ac1>=</span> gp<span style=color:#ff6ac1>.</span>predict(x_fine[:, <span style=color:#ff6ac1>None</span>], return_std<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>scatter(xs, ys, s<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>30</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Samples&#34;</span>, zorder<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>20</span>)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>plot(x_fine, y_fine, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Function&#34;</span>, color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.5</span>)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>plot(x_fine, y_mean)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>fill_between(x_fine, y_mean <span style=color:#ff6ac1>+</span> <span style=color:#ff9f43>2</span> <span style=color:#ff6ac1>*</span> y_std, y_mean <span style=color:#ff6ac1>-</span> <span style=color:#ff9f43>2</span> <span style=color:#ff6ac1>*</span> y_std, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.3</span>)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>set_title(<span style=color:#5af78e>f</span><span style=color:#5af78e>&#34;</span><span style=color:#5af78e>{</span>kernel<span style=color:#5af78e>}</span><span style=color:#5af78e>\n</span><span style=color:#5af78e>{</span>y_std<span style=color:#ff6ac1>.</span>max()<span style=color:#5af78e>:</span><span style=color:#5af78e>0.4f</span><span style=color:#5af78e>}</span><span style=color:#5af78e>&#34;</span>)
</span></span></code></pre></div></div><p><div><figure class=rounded><picture><source srcset=/tutorials/gaussian_processes/2020-09-10-Gaussian_Processes_files/2020-09-10-Gaussian_Processes_14_0_hu05752bdb4226408f0e457ad4f185d4d2_249182_1920x0_resize_q90_h2_box_3.webp width=1920 height=926 type=image/webp><source srcset=/tutorials/gaussian_processes/2020-09-10-Gaussian_Processes_files/2020-09-10-Gaussian_Processes_14_0.png width=3331 height=1606 type=image/png><img width=3331 height=1606 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>You can see on the left we have a model where the length scale is too small - our uncertainty is huge between points. On the right the length scale is so large its pulling even the known data points away from where they belond (as the length scale gets larger, the line would start to flatten out and look like the mean of all data points). But the middle looks pretty good.</p><p>If we want to fit the hyperparameters, what we do is maximise the marginal likelihood, conditioned on the hyperparameters. I won&rsquo;t go into the math, but the marginal likelihood has a term for how well it fits to data (so the right hand plot would have a horrible score), a normalisation term, and a penalty for complexity (so the left hand model would be too complex because its scale is too small and allows for too much freedom).</p><div class="expanded-code width-86" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#78787e># Start at 2.0, fit a value between 1 and 10</span>
</span></span><span style=display:flex><span>kernel <span style=color:#ff6ac1>=</span> k<span style=color:#ff6ac1>.</span>RBF(length_scale<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2.0</span>, length_scale_bounds<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>1.0</span>, <span style=color:#ff9f43>10.0</span>))
</span></span><span style=display:flex><span>gp <span style=color:#ff6ac1>=</span> GaussianProcessRegressor(kernel<span style=color:#ff6ac1>=</span>kernel)
</span></span><span style=display:flex><span>gp<span style=color:#ff6ac1>.</span>fit(np<span style=color:#ff6ac1>.</span>atleast_2d(xs)<span style=color:#ff6ac1>.</span>T, ys)
</span></span><span style=display:flex><span>y_mean, y_std <span style=color:#ff6ac1>=</span> gp<span style=color:#ff6ac1>.</span>predict(x_fine[:, <span style=color:#ff6ac1>None</span>], return_std<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Plot it all, yay</span>
</span></span><span style=display:flex><span>fig, ax <span style=color:#ff6ac1>=</span> plt<span style=color:#ff6ac1>.</span>subplots(figsize<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>10</span>, <span style=color:#ff9f43>4</span>))
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>scatter(xs, ys, s<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>30</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Samples&#34;</span>, zorder<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>20</span>)
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>plot(x_fine, y_fine, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Function&#34;</span>, color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>plot(x_fine, y_mean)
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>fill_between(x_fine, y_mean <span style=color:#ff6ac1>+</span> <span style=color:#ff9f43>2</span> <span style=color:#ff6ac1>*</span> y_std, y_mean <span style=color:#ff6ac1>-</span> <span style=color:#ff9f43>2</span> <span style=color:#ff6ac1>*</span> y_std, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.3</span>)
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>set_title(<span style=color:#5af78e>f</span><span style=color:#5af78e>&#34;RBF, length_scale=</span><span style=color:#5af78e>{</span>gp<span style=color:#ff6ac1>.</span>kernel_<span style=color:#ff6ac1>.</span>length_scale<span style=color:#5af78e>:</span><span style=color:#5af78e>0.3f</span><span style=color:#5af78e>}</span><span style=color:#5af78e>&#34;</span>);
</span></span><span style=display:flex><span><span style=color:#78787e># Note that the fitted kernel is inside the GP, the original kernel won&#39;t be modified.</span>
</span></span></code></pre></div></div><p><div><figure class=rounded><picture><source srcset=/tutorials/gaussian_processes/2020-09-10-Gaussian_Processes_files/2020-09-10-Gaussian_Processes_16_0_hu53d689b1114e2ea91aead105c5574a47_198587_1920x0_resize_q90_h2_box_3.webp width=1920 height=869 type=image/webp><source srcset=/tutorials/gaussian_processes/2020-09-10-Gaussian_Processes_files/2020-09-10-Gaussian_Processes_16_0.png width=3313 height=1499 type=image/png><img width=3313 height=1499 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>But this is only talking about how to train model parameters. There are a ton of different kernels you can pick from, and thats the actual hard task. If you use deep gaussian processes they can do that for you too, but thats well beyond this write up.</p><p>Because kernels are really just functions, you can do multiple them, add them, combine them in fun ways. For example:</p><div class=width-78 markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>kernels <span style=color:#ff6ac1>=</span> [
</span></span><span style=display:flex><span>    k<span style=color:#ff6ac1>.</span>ConstantKernel() <span style=color:#ff6ac1>*</span> k<span style=color:#ff6ac1>.</span>RBF(), 
</span></span><span style=display:flex><span>    k<span style=color:#ff6ac1>.</span>ExpSineSquared(periodicity<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>10</span>),
</span></span><span style=display:flex><span>    k<span style=color:#ff6ac1>.</span>ConstantKernel() <span style=color:#ff6ac1>*</span> k<span style=color:#ff6ac1>.</span>RationalQuadratic() <span style=color:#ff6ac1>+</span> k<span style=color:#ff6ac1>.</span>RBF()
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fig, axes <span style=color:#ff6ac1>=</span> plt<span style=color:#ff6ac1>.</span>subplots(figsize<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>10</span>, <span style=color:#ff9f43>4</span>), ncols<span style=color:#ff6ac1>=</span><span style=color:#ff5c57>len</span>(kernels), sharey<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span>fig<span style=color:#ff6ac1>.</span>subplots_adjust(wspace<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>, hspace<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> kernel, ax <span style=color:#ff6ac1>in</span> <span style=color:#ff5c57>zip</span>(kernels, axes):
</span></span><span style=display:flex><span>    gp <span style=color:#ff6ac1>=</span> GaussianProcessRegressor(kernel<span style=color:#ff6ac1>=</span>kernel)
</span></span><span style=display:flex><span>    gp<span style=color:#ff6ac1>.</span>fit(np<span style=color:#ff6ac1>.</span>atleast_2d(xs)<span style=color:#ff6ac1>.</span>T, ys)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    y_mean, y_std <span style=color:#ff6ac1>=</span> gp<span style=color:#ff6ac1>.</span>predict(x_fine[:, <span style=color:#ff6ac1>None</span>], return_std<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>scatter(xs, ys, s<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>30</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Samples&#34;</span>, zorder<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>20</span>)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>plot(x_fine, y_fine, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Function&#34;</span>, color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.5</span>)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>plot(x_fine, y_mean)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>fill_between(x_fine, y_mean <span style=color:#ff6ac1>+</span> <span style=color:#ff9f43>2</span> <span style=color:#ff6ac1>*</span> y_std, y_mean <span style=color:#ff6ac1>-</span> <span style=color:#ff9f43>2</span> <span style=color:#ff6ac1>*</span> y_std, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.2</span>)
</span></span></code></pre></div></div><p><div><figure class=rounded><picture><source srcset=/tutorials/gaussian_processes/2020-09-10-Gaussian_Processes_files/2020-09-10-Gaussian_Processes_18_0_hue7b7cf3297487d2538c39093863813f7_292257_1920x0_resize_q90_h2_box_3.webp width=1920 height=798 type=image/webp><source srcset=/tutorials/gaussian_processes/2020-09-10-Gaussian_Processes_files/2020-09-10-Gaussian_Processes_18_0.png width=3369 height=1401 type=image/png><img width=3369 height=1401 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>As you can see, picking the different kernels will have a huge impact on your fits. Its important to use some domain knowledge to help you pick. Looking at weather patterns over multiple years? Maybe a periodic kernel with a yearly frequency would be good to include, with a Constant * RBF added on to add some flexibility on short timescales? We now enter the domain of art.</p><p>But if you need something that works pretty well in general, a constant kernel and RBF can be combined easily:</p><div class=width-73 markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>from</span> sklearn.gaussian_process <span style=color:#ff6ac1>import</span> GaussianProcessRegressor
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> sklearn.gaussian_process.kernels <span style=color:#ff6ac1>import</span> RBF, ConstantKernel <span style=color:#ff6ac1>as</span> C
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>gp <span style=color:#ff6ac1>=</span> GaussianProcessRegressor(kernel<span style=color:#ff6ac1>=</span>C() <span style=color:#ff6ac1>*</span> RBF())
</span></span><span style=display:flex><span>gp<span style=color:#ff6ac1>.</span>fit(np<span style=color:#ff6ac1>.</span>atleast_2d(xs)<span style=color:#ff6ac1>.</span>T, ys)
</span></span><span style=display:flex><span>y_pred, sigma <span style=color:#ff6ac1>=</span> gp<span style=color:#ff6ac1>.</span>predict(np<span style=color:#ff6ac1>.</span>atleast_2d(x_fine)<span style=color:#ff6ac1>.</span>T, return_std<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>upper, lower <span style=color:#ff6ac1>=</span> y_pred <span style=color:#ff6ac1>+</span> <span style=color:#ff9f43>1.96</span> <span style=color:#ff6ac1>*</span> sigma, y_pred <span style=color:#ff6ac1>-</span> <span style=color:#ff9f43>1.96</span> <span style=color:#ff6ac1>*</span> sigma
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>scatter(xs, ys, s<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>30</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Samples&#34;</span>, zorder<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>20</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(x_fine, y_fine, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Function&#34;</span>, color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.5</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(x_fine, y_pred, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;GP&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>fill_between(x_fine, upper, lower, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.2</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;95</span><span style=color:#5af78e>% c</span><span style=color:#5af78e>onfidence&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend(ncol<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>4</span>, fontsize<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>12</span>);
</span></span></code></pre></div></div><p><div><figure class="img-main rounded"><picture><source srcset=/tutorials/gaussian_processes/cover_huffe4f6deaef61d896d55cf32ae2c2b31_177885_1920x0_resize_q90_h2_box_3.webp width=1920 height=998 type=image/webp><source srcset=/tutorials/gaussian_processes/cover.png width=2731 height=1420 type=image/png><img width=2731 height=1420 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><h2 id=gaussian-processes-with-uncertainty>Gaussian Processes with uncertainty</h2><p>Conceptually, what if our data points have uncertainty. Under the hood, what we&rsquo;re doing is adding in an extra covariance matrix into the mix, where for our independent uncertainty will be a diagonal matrix. Formally, this is called Tikhonov regularization, but lets not get into that.</p><p>Let&rsquo;s generate some data points with different uncertainty on each point:</p><div class=width-79 markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>xs2 <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>array([<span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>1.5</span>, <span style=color:#ff9f43>2</span>, <span style=color:#ff9f43>3</span>, <span style=color:#ff9f43>4</span>, <span style=color:#ff9f43>4.25</span>, <span style=color:#ff9f43>4.5</span>, <span style=color:#ff9f43>5</span>, <span style=color:#ff9f43>7</span>, <span style=color:#ff9f43>9</span>, <span style=color:#ff9f43>10</span>])
</span></span><span style=display:flex><span>ys2 <span style=color:#ff6ac1>=</span> fn(xs2)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>err_scale <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>uniform(low<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.1</span>, high<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.2</span>, size<span style=color:#ff6ac1>=</span>ys2<span style=color:#ff6ac1>.</span>shape)
</span></span><span style=display:flex><span>err <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>normal(loc<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>, scale<span style=color:#ff6ac1>=</span>err_scale, size<span style=color:#ff6ac1>=</span>ys2<span style=color:#ff6ac1>.</span>shape)
</span></span><span style=display:flex><span>ys_err <span style=color:#ff6ac1>=</span> ys2 <span style=color:#ff6ac1>+</span> err
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>errorbar(xs2, ys_err, yerr<span style=color:#ff6ac1>=</span>err, fmt<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;o&#34;</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Samples&#34;</span>, markersize<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>5</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(x_fine, y_fine, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Function&#34;</span>, color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.5</span>, linestyle<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;-&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend(ncol<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>4</span>, fontsize<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>12</span>);
</span></span></code></pre></div></div><p><div><figure class=rounded><picture><source srcset=/tutorials/gaussian_processes/2020-09-10-Gaussian_Processes_files/2020-09-10-Gaussian_Processes_23_0_hu245e6e5d62dae4542a5c7efd3fee2333_99324_1920x0_resize_q90_h2_box_3.webp width=1920 height=997 type=image/webp><source srcset=/tutorials/gaussian_processes/2020-09-10-Gaussian_Processes_files/2020-09-10-Gaussian_Processes_23_0.png width=2731 height=1418 type=image/png><img width=2731 height=1418 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>We can add this uncertainty on the y-axis in incredibly easily - we just pass it in when we create the Gaussian Process.</p><div class="expanded-code width-90" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>gp <span style=color:#ff6ac1>=</span> GaussianProcessRegressor(kernel<span style=color:#ff6ac1>=</span>C() <span style=color:#ff6ac1>*</span> RBF(), alpha<span style=color:#ff6ac1>=</span>err<span style=color:#ff6ac1>**</span><span style=color:#ff9f43>2</span>)
</span></span><span style=display:flex><span>gp<span style=color:#ff6ac1>.</span>fit(np<span style=color:#ff6ac1>.</span>atleast_2d(xs2)<span style=color:#ff6ac1>.</span>T, ys_err)
</span></span><span style=display:flex><span>y_pred, sigma <span style=color:#ff6ac1>=</span> gp<span style=color:#ff6ac1>.</span>predict(np<span style=color:#ff6ac1>.</span>atleast_2d(x_fine)<span style=color:#ff6ac1>.</span>T, return_std<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># 1.96sigma = 95% confidence interval for a normal distribution</span>
</span></span><span style=display:flex><span>upper, lower <span style=color:#ff6ac1>=</span> y_pred <span style=color:#ff6ac1>+</span> <span style=color:#ff9f43>1.96</span> <span style=color:#ff6ac1>*</span> sigma, y_pred <span style=color:#ff6ac1>-</span> <span style=color:#ff9f43>1.96</span> <span style=color:#ff6ac1>*</span> sigma
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>errorbar(xs2, ys_err, yerr<span style=color:#ff6ac1>=</span>err, fmt<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;o&#34;</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Samples&#34;</span>, markersize<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>5</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(x_fine, y_fine, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Function&#34;</span>, color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(x_fine, y_pred, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;GP&#34;</span>, ls<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;-&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>fill_between(x_fine, upper, lower, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.2</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;95</span><span style=color:#5af78e>% c</span><span style=color:#5af78e>onfidence&#34;</span>, color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;#2698eb&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>ylim(<span style=color:#ff9f43>3.35</span>, <span style=color:#ff9f43>5.5</span>), plt<span style=color:#ff6ac1>.</span>legend(ncol<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>4</span>, fontsize<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>12</span>);
</span></span></code></pre></div></div><p><div><figure class=rounded><picture><source srcset=/tutorials/gaussian_processes/2020-09-10-Gaussian_Processes_files/2020-09-10-Gaussian_Processes_25_0_huf8850c388f8be52846d1106886616041_243900_1920x0_resize_q90_h2_box_3.webp width=1920 height=998 type=image/webp><source srcset=/tutorials/gaussian_processes/2020-09-10-Gaussian_Processes_files/2020-09-10-Gaussian_Processes_25_0.png width=2731 height=1420 type=image/png><img width=2731 height=1420 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><h1 id=summary>Summary</h1><p>If you&rsquo;ve reach this point and actually gone through all the code and writing, congratulations! Probably my longest write up yet, but if you&rsquo;ve made it here, hopefully Gaussian Processes, kernels and covariance are all more comfortable terms than when you started.</p><p>Whilst they might be easy to plug-and-play using scikit-learn, Gaussian proccesses require subtlety in training, especially picking the right kernel. Its a very deep and rich field that you can easily spend an entire PhD investigating.</p><hr><p>For your convenience, here&rsquo;s the code in one block:</p><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>import</span> numpy <span style=color:#ff6ac1>as</span> np
</span></span><span style=display:flex><span><span style=color:#ff6ac1>import</span> matplotlib.pyplot <span style=color:#ff6ac1>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Our funky function here</span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>fn</span>(xs):
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> np<span style=color:#ff6ac1>.</span>exp((xs <span style=color:#ff6ac1>+</span> <span style=color:#ff9f43>10.5</span>)<span style=color:#ff6ac1>**</span><span style=color:#ff9f43>0.1</span>) <span style=color:#ff6ac1>+</span> np<span style=color:#ff6ac1>.</span>sin(xs) <span style=color:#ff6ac1>/</span> (xs <span style=color:#ff6ac1>+</span> <span style=color:#ff9f43>1</span>) <span style=color:#ff6ac1>+</span> np<span style=color:#ff6ac1>.</span>cos(<span style=color:#ff9f43>2.5</span> <span style=color:#ff6ac1>*</span> xs<span style=color:#ff6ac1>**</span><span style=color:#ff9f43>0.5</span>)<span style=color:#ff6ac1>**</span><span style=color:#ff9f43>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Lets generate both the good visualisation of the function and some samples</span>
</span></span><span style=display:flex><span>xs <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>array([<span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>2</span>, <span style=color:#ff9f43>4</span>, <span style=color:#ff9f43>5</span>, <span style=color:#ff9f43>7</span>, <span style=color:#ff9f43>9</span>, <span style=color:#ff9f43>10</span>])
</span></span><span style=display:flex><span>x_fine <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>linspace(<span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>10</span>, <span style=color:#ff9f43>200</span>)
</span></span><span style=display:flex><span>ys, y_fine <span style=color:#ff6ac1>=</span> fn(xs), fn(x_fine)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># And plot it out</span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(x_fine, y_fine, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;#ccc&#34;</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Function&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>scatter(xs, ys, s<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>30</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Samples&#34;</span>, zorder<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>20</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend();
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> scipy.interpolate <span style=color:#ff6ac1>import</span> interp1d
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>y_linear <span style=color:#ff6ac1>=</span> interp1d(xs, ys, kind<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;linear&#34;</span>)(x_fine)
</span></span><span style=display:flex><span>y_quad <span style=color:#ff6ac1>=</span> interp1d(xs, ys, kind<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;quadratic&#34;</span>)(x_fine)
</span></span><span style=display:flex><span>y_cubic <span style=color:#ff6ac1>=</span> interp1d(xs, ys, kind<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;cubic&#34;</span>)(x_fine)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>scatter(xs, ys, s<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>30</span>, zorder<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>20</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Samples&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(x_fine, y_fine, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Function&#34;</span>, color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;k&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(x_fine, y_linear, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Linear&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(x_fine, y_quad, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Quadratic&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(x_fine, y_cubic, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Cubic&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend(ncol<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>3</span>);
</span></span><span style=display:flex><span><span style=color:#ff6ac1>import</span> seaborn <span style=color:#ff6ac1>as</span> sb
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>get_cov</span>(size<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>20</span>, length<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>50</span>):    
</span></span><span style=display:flex><span>    x <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>arange(size)
</span></span><span style=display:flex><span>    cov <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>exp(<span style=color:#ff6ac1>-</span>(<span style=color:#ff9f43>1</span> <span style=color:#ff6ac1>/</span> length) <span style=color:#ff6ac1>*</span> (x <span style=color:#ff6ac1>-</span> np<span style=color:#ff6ac1>.</span>atleast_2d(x)<span style=color:#ff6ac1>.</span>T)<span style=color:#ff6ac1>**</span><span style=color:#ff9f43>2</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> cov
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cov <span style=color:#ff6ac1>=</span> get_cov()
</span></span><span style=display:flex><span>cov2 <span style=color:#ff6ac1>=</span> get_cov(length<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>5</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fig, axes <span style=color:#ff6ac1>=</span> plt<span style=color:#ff6ac1>.</span>subplots(ncols<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>, figsize<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>8</span>, <span style=color:#ff9f43>3</span>))
</span></span><span style=display:flex><span>axes[<span style=color:#ff9f43>0</span>]<span style=color:#ff6ac1>.</span>set_title(<span style=color:#5af78e>&#34;Small correlation length&#34;</span>, fontsize<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>14</span>)
</span></span><span style=display:flex><span>axes[<span style=color:#ff9f43>1</span>]<span style=color:#ff6ac1>.</span>set_title(<span style=color:#5af78e>&#34;Large correlation length&#34;</span>, fontsize<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>14</span>)
</span></span><span style=display:flex><span>sb<span style=color:#ff6ac1>.</span>heatmap(cov2, ax<span style=color:#ff6ac1>=</span>axes[<span style=color:#ff9f43>0</span>], square<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span>sb<span style=color:#ff6ac1>.</span>heatmap(cov, ax<span style=color:#ff6ac1>=</span>axes[<span style=color:#ff9f43>1</span>], square<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>);
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> scipy.stats <span style=color:#ff6ac1>import</span> multivariate_normal <span style=color:#ff6ac1>as</span> mn
</span></span><span style=display:flex><span>x <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>arange(<span style=color:#ff9f43>20</span>)
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> l <span style=color:#ff6ac1>in</span> [<span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>10</span>, <span style=color:#ff9f43>50</span>, <span style=color:#ff9f43>200</span>]:
</span></span><span style=display:flex><span>    cov <span style=color:#ff6ac1>=</span> get_cov(length<span style=color:#ff6ac1>=</span>l)
</span></span><span style=display:flex><span>    rvs <span style=color:#ff6ac1>=</span> mn<span style=color:#ff6ac1>.</span>rvs(cov<span style=color:#ff6ac1>=</span>cov)
</span></span><span style=display:flex><span>    plt<span style=color:#ff6ac1>.</span>plot(x, rvs, <span style=color:#5af78e>&#39;o-&#39;</span>, ms<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>5</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>f</span><span style=color:#5af78e>&#34;L=</span><span style=color:#5af78e>{</span>l<span style=color:#5af78e>}</span><span style=color:#5af78e>&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend(ncol<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>);
</span></span><span style=display:flex><span><span style=color:#78787e># In the previous code, we made one set of points four times in a loop</span>
</span></span><span style=display:flex><span><span style=color:#78787e># Here, we generate 500 sets of our data points in one go</span>
</span></span><span style=display:flex><span>rvs <span style=color:#ff6ac1>=</span> mn<span style=color:#ff6ac1>.</span>rvs(cov<span style=color:#ff6ac1>=</span>cov, size<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>500</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Set the start and end points the same for all realisations</span>
</span></span><span style=display:flex><span>rvs <span style=color:#ff6ac1>-=</span> np<span style=color:#ff6ac1>.</span>linspace(rvs[:, <span style=color:#ff9f43>0</span>], rvs[:, <span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>], x<span style=color:#ff6ac1>.</span>size)<span style=color:#ff6ac1>.</span>T
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Get the mean and std of our 500 realisations</span>
</span></span><span style=display:flex><span>mean <span style=color:#ff6ac1>=</span> rvs<span style=color:#ff6ac1>.</span>mean(axis<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>)
</span></span><span style=display:flex><span>std <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>std(rvs, axis<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Plot the mean and std</span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(x, mean, <span style=color:#5af78e>&#34;o-&#34;</span>, ms<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>5</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;mean&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>fill_between(x, mean <span style=color:#ff6ac1>+</span> std, mean <span style=color:#ff6ac1>-</span> std, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.4</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;std&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># And for fun, lets plot 100 of the data realisations</span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> i <span style=color:#ff6ac1>in</span> <span style=color:#ff5c57>range</span>(<span style=color:#ff9f43>100</span>):
</span></span><span style=display:flex><span>    plt<span style=color:#ff6ac1>.</span>plot(x, rvs[i, :], ls<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;-&#34;</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.2</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.7</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend();
</span></span><span style=display:flex><span><span style=color:#78787e># To refresh your mind on our data</span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>scatter(xs, ys, s<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>20</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Samples&#34;</span>, zorder<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>30</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(x_fine, y_fine, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Function&#34;</span>, color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend();
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> sklearn.gaussian_process <span style=color:#ff6ac1>import</span> GaussianProcessRegressor
</span></span><span style=display:flex><span><span style=color:#ff6ac1>import</span> sklearn.gaussian_process.kernels <span style=color:#ff6ac1>as</span> k
</span></span><span style=display:flex><span><span style=color:#78787e># Note I am NOT training these models. You can see in the bounds</span>
</span></span><span style=display:flex><span><span style=color:#78787e># I am forcing them to have specific lengths. Normally, they would </span>
</span></span><span style=display:flex><span><span style=color:#78787e># fit to your data</span>
</span></span><span style=display:flex><span>kernels <span style=color:#ff6ac1>=</span> [
</span></span><span style=display:flex><span>    k<span style=color:#ff6ac1>.</span>RBF(length_scale<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1.0</span>, length_scale_bounds<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>1.0</span>, <span style=color:#ff9f43>1.01</span>)), 
</span></span><span style=display:flex><span>    k<span style=color:#ff6ac1>.</span>RBF(length_scale<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2.0</span>, length_scale_bounds<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>2.0</span>, <span style=color:#ff9f43>2.01</span>)),
</span></span><span style=display:flex><span>    k<span style=color:#ff6ac1>.</span>RBF(length_scale<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>20.0</span>, length_scale_bounds<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>20.0</span>, <span style=color:#ff9f43>20.01</span>)),
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Make some axes please</span>
</span></span><span style=display:flex><span>fig, axes <span style=color:#ff6ac1>=</span> plt<span style=color:#ff6ac1>.</span>subplots(figsize<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>10</span>, <span style=color:#ff9f43>4</span>), ncols<span style=color:#ff6ac1>=</span><span style=color:#ff5c57>len</span>(kernels), sharey<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span>fig<span style=color:#ff6ac1>.</span>subplots_adjust(wspace<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>, hspace<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> kernel, ax <span style=color:#ff6ac1>in</span> <span style=color:#ff5c57>zip</span>(kernels, axes):
</span></span><span style=display:flex><span>    <span style=color:#78787e># Fit the GP, which doesnt do much as we fixed the length_scale</span>
</span></span><span style=display:flex><span>    gp <span style=color:#ff6ac1>=</span> GaussianProcessRegressor(kernel<span style=color:#ff6ac1>=</span>kernel)
</span></span><span style=display:flex><span>    gp<span style=color:#ff6ac1>.</span>fit(np<span style=color:#ff6ac1>.</span>atleast_2d(xs)<span style=color:#ff6ac1>.</span>T, ys)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#78787e># The 2D and [:, None] stuff is because the object expects 2D data in, not 1D</span>
</span></span><span style=display:flex><span>    y_mean, y_std <span style=color:#ff6ac1>=</span> gp<span style=color:#ff6ac1>.</span>predict(x_fine[:, <span style=color:#ff6ac1>None</span>], return_std<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>scatter(xs, ys, s<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>30</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Samples&#34;</span>, zorder<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>20</span>)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>plot(x_fine, y_fine, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Function&#34;</span>, color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.5</span>)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>plot(x_fine, y_mean)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>fill_between(x_fine, y_mean <span style=color:#ff6ac1>+</span> <span style=color:#ff9f43>2</span> <span style=color:#ff6ac1>*</span> y_std, y_mean <span style=color:#ff6ac1>-</span> <span style=color:#ff9f43>2</span> <span style=color:#ff6ac1>*</span> y_std, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.3</span>)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>set_title(<span style=color:#5af78e>f</span><span style=color:#5af78e>&#34;</span><span style=color:#5af78e>{</span>kernel<span style=color:#5af78e>}</span><span style=color:#5af78e>\n</span><span style=color:#5af78e>{</span>y_std<span style=color:#ff6ac1>.</span>max()<span style=color:#5af78e>:</span><span style=color:#5af78e>0.4f</span><span style=color:#5af78e>}</span><span style=color:#5af78e>&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#78787e># Start at 2.0, fit a value between 1 and 10</span>
</span></span><span style=display:flex><span>kernel <span style=color:#ff6ac1>=</span> k<span style=color:#ff6ac1>.</span>RBF(length_scale<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2.0</span>, length_scale_bounds<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>1.0</span>, <span style=color:#ff9f43>10.0</span>))
</span></span><span style=display:flex><span>gp <span style=color:#ff6ac1>=</span> GaussianProcessRegressor(kernel<span style=color:#ff6ac1>=</span>kernel)
</span></span><span style=display:flex><span>gp<span style=color:#ff6ac1>.</span>fit(np<span style=color:#ff6ac1>.</span>atleast_2d(xs)<span style=color:#ff6ac1>.</span>T, ys)
</span></span><span style=display:flex><span>y_mean, y_std <span style=color:#ff6ac1>=</span> gp<span style=color:#ff6ac1>.</span>predict(x_fine[:, <span style=color:#ff6ac1>None</span>], return_std<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Plot it all, yay</span>
</span></span><span style=display:flex><span>fig, ax <span style=color:#ff6ac1>=</span> plt<span style=color:#ff6ac1>.</span>subplots(figsize<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>10</span>, <span style=color:#ff9f43>4</span>))
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>scatter(xs, ys, s<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>30</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Samples&#34;</span>, zorder<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>20</span>)
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>plot(x_fine, y_fine, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Function&#34;</span>, color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>plot(x_fine, y_mean)
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>fill_between(x_fine, y_mean <span style=color:#ff6ac1>+</span> <span style=color:#ff9f43>2</span> <span style=color:#ff6ac1>*</span> y_std, y_mean <span style=color:#ff6ac1>-</span> <span style=color:#ff9f43>2</span> <span style=color:#ff6ac1>*</span> y_std, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.3</span>)
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>set_title(<span style=color:#5af78e>f</span><span style=color:#5af78e>&#34;RBF, length_scale=</span><span style=color:#5af78e>{</span>gp<span style=color:#ff6ac1>.</span>kernel_<span style=color:#ff6ac1>.</span>length_scale<span style=color:#5af78e>:</span><span style=color:#5af78e>0.3f</span><span style=color:#5af78e>}</span><span style=color:#5af78e>&#34;</span>);
</span></span><span style=display:flex><span><span style=color:#78787e># Note that the fitted kernel is inside the GP, the original kernel won&#39;t be modified.</span>
</span></span><span style=display:flex><span>kernels <span style=color:#ff6ac1>=</span> [
</span></span><span style=display:flex><span>    k<span style=color:#ff6ac1>.</span>ConstantKernel() <span style=color:#ff6ac1>*</span> k<span style=color:#ff6ac1>.</span>RBF(), 
</span></span><span style=display:flex><span>    k<span style=color:#ff6ac1>.</span>ExpSineSquared(periodicity<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>10</span>),
</span></span><span style=display:flex><span>    k<span style=color:#ff6ac1>.</span>ConstantKernel() <span style=color:#ff6ac1>*</span> k<span style=color:#ff6ac1>.</span>RationalQuadratic() <span style=color:#ff6ac1>+</span> k<span style=color:#ff6ac1>.</span>RBF()
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fig, axes <span style=color:#ff6ac1>=</span> plt<span style=color:#ff6ac1>.</span>subplots(figsize<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>10</span>, <span style=color:#ff9f43>4</span>), ncols<span style=color:#ff6ac1>=</span><span style=color:#ff5c57>len</span>(kernels), sharey<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span>fig<span style=color:#ff6ac1>.</span>subplots_adjust(wspace<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>, hspace<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> kernel, ax <span style=color:#ff6ac1>in</span> <span style=color:#ff5c57>zip</span>(kernels, axes):
</span></span><span style=display:flex><span>    gp <span style=color:#ff6ac1>=</span> GaussianProcessRegressor(kernel<span style=color:#ff6ac1>=</span>kernel)
</span></span><span style=display:flex><span>    gp<span style=color:#ff6ac1>.</span>fit(np<span style=color:#ff6ac1>.</span>atleast_2d(xs)<span style=color:#ff6ac1>.</span>T, ys)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    y_mean, y_std <span style=color:#ff6ac1>=</span> gp<span style=color:#ff6ac1>.</span>predict(x_fine[:, <span style=color:#ff6ac1>None</span>], return_std<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>scatter(xs, ys, s<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>30</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Samples&#34;</span>, zorder<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>20</span>)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>plot(x_fine, y_fine, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Function&#34;</span>, color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.5</span>)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>plot(x_fine, y_mean)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>fill_between(x_fine, y_mean <span style=color:#ff6ac1>+</span> <span style=color:#ff9f43>2</span> <span style=color:#ff6ac1>*</span> y_std, y_mean <span style=color:#ff6ac1>-</span> <span style=color:#ff9f43>2</span> <span style=color:#ff6ac1>*</span> y_std, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.2</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> sklearn.gaussian_process <span style=color:#ff6ac1>import</span> GaussianProcessRegressor
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> sklearn.gaussian_process.kernels <span style=color:#ff6ac1>import</span> RBF, ConstantKernel <span style=color:#ff6ac1>as</span> C
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>gp <span style=color:#ff6ac1>=</span> GaussianProcessRegressor(kernel<span style=color:#ff6ac1>=</span>C() <span style=color:#ff6ac1>*</span> RBF())
</span></span><span style=display:flex><span>gp<span style=color:#ff6ac1>.</span>fit(np<span style=color:#ff6ac1>.</span>atleast_2d(xs)<span style=color:#ff6ac1>.</span>T, ys)
</span></span><span style=display:flex><span>y_pred, sigma <span style=color:#ff6ac1>=</span> gp<span style=color:#ff6ac1>.</span>predict(np<span style=color:#ff6ac1>.</span>atleast_2d(x_fine)<span style=color:#ff6ac1>.</span>T, return_std<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>upper, lower <span style=color:#ff6ac1>=</span> y_pred <span style=color:#ff6ac1>+</span> <span style=color:#ff9f43>1.96</span> <span style=color:#ff6ac1>*</span> sigma, y_pred <span style=color:#ff6ac1>-</span> <span style=color:#ff9f43>1.96</span> <span style=color:#ff6ac1>*</span> sigma
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>scatter(xs, ys, s<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>30</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Samples&#34;</span>, zorder<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>20</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(x_fine, y_fine, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Function&#34;</span>, color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.5</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(x_fine, y_pred, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;GP&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>fill_between(x_fine, upper, lower, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.2</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;95</span><span style=color:#5af78e>% c</span><span style=color:#5af78e>onfidence&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend(ncol<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>4</span>, fontsize<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>12</span>);
</span></span><span style=display:flex><span>xs2 <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>array([<span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>1.5</span>, <span style=color:#ff9f43>2</span>, <span style=color:#ff9f43>3</span>, <span style=color:#ff9f43>4</span>, <span style=color:#ff9f43>4.25</span>, <span style=color:#ff9f43>4.5</span>, <span style=color:#ff9f43>5</span>, <span style=color:#ff9f43>7</span>, <span style=color:#ff9f43>9</span>, <span style=color:#ff9f43>10</span>])
</span></span><span style=display:flex><span>ys2 <span style=color:#ff6ac1>=</span> fn(xs2)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>err_scale <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>uniform(low<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.1</span>, high<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.2</span>, size<span style=color:#ff6ac1>=</span>ys2<span style=color:#ff6ac1>.</span>shape)
</span></span><span style=display:flex><span>err <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>normal(loc<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>, scale<span style=color:#ff6ac1>=</span>err_scale, size<span style=color:#ff6ac1>=</span>ys2<span style=color:#ff6ac1>.</span>shape)
</span></span><span style=display:flex><span>ys_err <span style=color:#ff6ac1>=</span> ys2 <span style=color:#ff6ac1>+</span> err
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>errorbar(xs2, ys_err, yerr<span style=color:#ff6ac1>=</span>err, fmt<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;o&#34;</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Samples&#34;</span>, markersize<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>5</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(x_fine, y_fine, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Function&#34;</span>, color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.5</span>, linestyle<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;-&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend(ncol<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>4</span>, fontsize<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>12</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>gp <span style=color:#ff6ac1>=</span> GaussianProcessRegressor(kernel<span style=color:#ff6ac1>=</span>C() <span style=color:#ff6ac1>*</span> RBF(), alpha<span style=color:#ff6ac1>=</span>err<span style=color:#ff6ac1>**</span><span style=color:#ff9f43>2</span>)
</span></span><span style=display:flex><span>gp<span style=color:#ff6ac1>.</span>fit(np<span style=color:#ff6ac1>.</span>atleast_2d(xs2)<span style=color:#ff6ac1>.</span>T, ys_err)
</span></span><span style=display:flex><span>y_pred, sigma <span style=color:#ff6ac1>=</span> gp<span style=color:#ff6ac1>.</span>predict(np<span style=color:#ff6ac1>.</span>atleast_2d(x_fine)<span style=color:#ff6ac1>.</span>T, return_std<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># 1.96sigma = 95% confidence interval for a normal distribution</span>
</span></span><span style=display:flex><span>upper, lower <span style=color:#ff6ac1>=</span> y_pred <span style=color:#ff6ac1>+</span> <span style=color:#ff9f43>1.96</span> <span style=color:#ff6ac1>*</span> sigma, y_pred <span style=color:#ff6ac1>-</span> <span style=color:#ff9f43>1.96</span> <span style=color:#ff6ac1>*</span> sigma
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>errorbar(xs2, ys_err, yerr<span style=color:#ff6ac1>=</span>err, fmt<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;o&#34;</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Samples&#34;</span>, markersize<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>5</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(x_fine, y_fine, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Function&#34;</span>, color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(x_fine, y_pred, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;GP&#34;</span>, ls<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;-&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>fill_between(x_fine, upper, lower, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.2</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;95</span><span style=color:#5af78e>% c</span><span style=color:#5af78e>onfidence&#34;</span>, color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;#2698eb&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>ylim(<span style=color:#ff9f43>3.35</span>, <span style=color:#ff9f43>5.5</span>), plt<span style=color:#ff6ac1>.</span>legend(ncol<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>4</span>, fontsize<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>12</span>);
</span></span></code></pre></div><div class="relative max-w-xl mx-auto my-20"><div class="fancy_card horizontal"><div class=card_translator><div class="card_rotator tiny_rot card_layer"><div class="card_layer newsletter p-2"><div class="newsletter-inner h-full"><div class="relative h-full sib-form-container" id=sib-form-container><div class="mb-6 lg:mb-12 text-center"><h3 class="text-main-200 mb-2 lg:mb-8">Stay in the loop!</h3><p class="text-main-100 text-lg text-opacity-70">For new releases, exclusive giveaways, and reviews.</p></div><form action="https://Cosmiccoding.us5.list-manage.com/subscribe/post?u=aebe5de1d2e40dccb3aeca491&amp;id=3987dd4a1f" method=post id=mc-embedded-subscribe-form name=mc-embedded-subscribe-form class="validate w-full" target=_blank novalidate><div><input type=email class="w-full appearance-none bg-main-600 mb-4 border border-main-600 bg-opacity-5 focus:border-main-300 rounded-sm px-4 py-3 text-main-100 placeholder-main-100 required" placeholder="Your best email" aria-label="Your best email" name=EMAIL required data-required=true id=EMAIL><div style=position:absolute;left:-5000px aria-hidden=true><input type=text name=b_aebe5de1d2e40dccb3aeca491_3987dd4a1f tabindex=-1></div><input type=submit value=Subscribe name=subscribe id=mc-embedded-subscribe class="button btn bg-main-600 hover:bg-main-400 shadow w-full"></div><p id=mce-error-response style=display:none class="text-center mt-2 opacity-50 text-main-200">There was a problem, please try again.</p><p id=mce-success-response style=display:none class="text-center mt-2 opacity-50 text-main-200">Thanks mate, won't let ya down.</p></form></div></div></div><div class="card_layer card_effect card_soft_glare" style=pointer-events:none></div></div></div></div></div></div><script type=text/x-mathjax-config>
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script></div><script language=javascript type=text/javascript src=https://cosmiccoding.com.au/js/main.min.11673d10a962fb5205229607e62ea1d23424d35dad33db7bfe2a09a63d5409fa.js></script>
<script async defer src="https://www.googletagmanager.com/gtag/js?id=G-GRX6QE03YR"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-GRX6QE03YR")</script></div></body></html>