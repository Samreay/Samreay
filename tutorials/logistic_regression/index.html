<!doctype html><html lang=en-gb><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>An Introduction to Logistic Regression - Samuel Hinton</title><link rel=stylesheet href="https://cosmiccoding.com.au/css/main.min.b842bc4f8d358708f7b5666fe7108ed6474cd18dad036996cd6f85d3bb7b6a42.css" integrity="sha256-uEK8T401hwj3tWZv5xCO1kdM0Y2tA2mWzW+F07t7akI="><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel="shortcut icon" href=https://cosmiccoding.com.au/img/favicon.png type=image/x-icon></head><meta name=description content="A beginners introduction to logistic regression in python."><meta name=robots content="noodp"><link rel=canonical href=https://cosmiccoding.com.au/tutorials/logistic_regression/><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://cosmiccoding.com.au/tutorials/logistic_regression/cover.png"><meta name=twitter:title content="An Introduction to Logistic Regression"><meta name=twitter:description content="A beginners introduction to logistic regression in python."><meta property="og:title" content="An Introduction to Logistic Regression"><meta property="og:description" content="A beginners introduction to logistic regression in python."><meta property="og:type" content="article"><meta property="og:url" content="https://cosmiccoding.com.au/tutorials/logistic_regression/"><meta property="og:image" content="https://cosmiccoding.com.au/tutorials/logistic_regression/cover.png"><meta property="article:section" content="tutorials"><meta property="article:published_time" content="2020-07-11T00:00:00+00:00"><meta property="article:modified_time" content="2020-07-11T00:00:00+00:00"><meta property="article:section" content="tutorial"><meta property="article:published_time" content="2020-07-11 00:00:00 +0000 UTC"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"An Introduction to Logistic Regression","genre":"tutorial","url":"https:\/\/cosmiccoding.com.au\/tutorials\/logistic_regression\/","datePublished":"2020-07-11 00:00:00 \u002b0000 UTC","description":"A beginners introduction to logistic regression in python.","author":{"@type":"Person","name":""}}</script><body><div class="flex flex-col min-h-screen overflow-hidden"><header class="absolute w-full z-30"><div class="max-w-6xl mx-auto px-4 sm:px-6"><div class="flex items-center justify-between h-20"><div class="flex-shrink-0 mr-4"><a class=block href=/ aria-label="Samuel Hinton"><h2 class=logo>SRH</h2></a></div><nav class="hidden md:flex md:flex-grow"><ul class="flex flex-grow justify-end flex-wrap items-center"><li><a href=/#books class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Books</a></li><li><a href=/reviews class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Reviews</a></li><li><a href=/tutorials class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Tutorials</a></li><li><a href=/blogs class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Blog</a></li><li><a href=/#courses class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Courses</a></li><li><a href=/static/resume/HintonCV.pdf class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">CV</a></li></ul></nav><div class=md:hidden x-data="{ expanded: false }"><button class=hamburger :class="{ 'active': expanded }" @click.stop="expanded = !expanded" aria-controls=mobile-nav :aria-expanded=expanded>
<span class=sr-only>Menu</span><svg class="w-6 h-6 fill-current text-gray-300 hover:text-gray-200 transition duration-150 ease-in-out" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><rect y="4" width="24" height="2" rx="1"/><rect y="11" width="24" height="2" rx="1"/><rect y="18" width="24" height="2" rx="1"/></svg></button><nav id=mobile-nav class="absolute top-full z-20 left-0 w-full px-4 sm:px-6 overflow-hidden transition-all duration-300 ease-in-out" x-ref=mobileNav :style="expanded ? 'max-height: ' + $refs.mobileNav.scrollHeight + 'px; opacity: 1' : 'max-height: 0; opacity: .8'" @click.away="expanded = false" @keydown.escape.window="expanded = false" x-cloak><ul class="bg-gray-800 px-4 py-2"><li><a href=/#books class="flex text-gray-300 hover:text-gray-200 py-2">Books</a></li><li><a href=/reviews class="flex text-gray-300 hover:text-gray-200 py-2">Reviews</a></li><li><a href=/tutorials class="flex text-gray-300 hover:text-gray-200 py-2">Tutorials</a></li><li><a href=/blogs class="flex text-gray-300 hover:text-gray-200 py-2">Blog</a></li><li><a href=/#courses class="flex text-gray-300 hover:text-gray-200 py-2">Courses</a></li><li><a href=/static/resume/HintonCV.pdf class="flex text-gray-300 hover:text-gray-200 py-2">CV</a></li></ul></nav></div></div></div></header><div class=flex-grow><div id=post-container class="content content-wider blog-post relative"><div class="section-header blog"><h1 class=title>An Introduction to Logistic Regression</h1><p>6th July 2020</p><p>A beginners introduction to logistic regression in python.</p></div><div><ul class="grid gap-6 w-full md:grid-cols-2" style=list-style:none;padding-left:0><li><input type=radio id=show-code name=code-toggle value=show-code class="hidden peer" onchange=clickCheckbox(this) required checked>
<label for=show-code class="inline-flex justify-between items-center p-5 w-full text-gray-500 bg-gray-800 rounded-lg border border-gray-200 cursor-pointer peer-checked:border-2 peer-checked:border-main-300 peer-checked:text-white peer-checked:bg-gray-700 hover:text-gray-200 hover:bg-gray-700"><div class="block w-full"><div class="w-full text-center text-lg font-semibold">Show me everything!</div><div class="w-full text-center text-sm">Oh yeah, coding time.</div></div></label></li><li><input type=radio id=hide-code name=code-toggle value=hide-code class="hidden peer" onchange=clickCheckbox(this)>
<label for=hide-code class="inline-flex justify-between items-center p-5 w-full text-gray-500 bg-gray-800 rounded-lg border border-gray-200 cursor-pointer peer-checked:border-2 peer-checked:border-main-300 peer-checked:text-white peer-checked:bg-gray-700 hover:text-gray-200 hover:bg-gray-700"><div class="block w-full"><div class="w-full text-center text-lg font-semibold">Just the plots</div><div class="w-full text-center text-sm">Code is nasty.</div></div></label></li></ul></div><script>function clickCheckbox(e){e.id=="show-code"?document.getElementById("post-container").classList.remove("hide-code"):document.getElementById("post-container").classList.add("hide-code")}</script><p>In this small write up, we&rsquo;ll cover logistic functions, probabilities vs odds, logit functions, and how to perform logistic regression in Python.</p><p>Logistic regression is a method of calculating the probability that an event will pass or fail. That is, we utilise it for dichotomous results - 0 and 1, pass or fail. Is this patient going to survive or not? Is this email spam or not? This is specifically called <strong>binary logistic regression</strong>, and is important to note because we can do logistic regression in other contexts.</p><h2 id=the-background-math>The background math</h2><p>As the name implies, it is based off the logistic function. Often when someone says a sigmoid function, they are referring to a logistic sigmoid, which is also what we are referring to. A logistic sigmoid function has the following form:</p><p>$$ f(x) = \frac{1}{1 + e^{-x}} $$</p><p>and looks like this:</p><p><div><figure class=rounded><picture><source srcset=/tutorials/logistic_regression/2020-07-11-Logistic_Regression_files/2020-07-11-Logistic_Regression_1_0_huce8b102987ab2ecf7371af6b77fd4843_79294_1920x0_resize_q90_h2_box_3.webp width=1920 height=1099 type=image/webp><source srcset=/tutorials/logistic_regression/2020-07-11-Logistic_Regression_files/2020-07-11-Logistic_Regression_1_0.png width=2794 height=1600 type=image/png><img width=2794 height=1600 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>So this raises the question - now that we have some function which goes from 0 to 1&mldr; how do we actually use it?</p><p>Let&rsquo;s create a simple example with some data, something super easy to understand. How about the probability that an egg breaks when dropped from some distance. Our binary variable is whether the egg broke, and the single input is the height it was dropped. We can, of course, have multiple inputs.</p><div class=width-66 markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#78787e># Using a sigmiod to generate data for a sigmoid example</span>
</span></span><span style=display:flex><span><span style=color:#78787e># How uninspired!</span>
</span></span><span style=display:flex><span>height <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>random(<span style=color:#ff9f43>100</span>)
</span></span><span style=display:flex><span>broke <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>1</span><span style=color:#ff6ac1>/</span>(<span style=color:#ff9f43>1</span><span style=color:#ff6ac1>+</span>np<span style=color:#ff6ac1>.</span>exp(<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>20</span><span style=color:#ff6ac1>*</span>height<span style=color:#ff6ac1>+</span><span style=color:#ff9f43>5</span>)) <span style=color:#ff6ac1>&gt;</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>random(<span style=color:#ff9f43>100</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>scatter(height, broke, c<span style=color:#ff6ac1>=</span>broke)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>xlabel(<span style=color:#5af78e>&#34;Height&#34;</span>), plt<span style=color:#ff6ac1>.</span>ylabel(<span style=color:#5af78e>&#34;Did it break?!?&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>annotate(<span style=color:#5af78e>&#34;Yep&#34;</span>, (<span style=color:#ff9f43>0.55</span>, <span style=color:#ff9f43>0.85</span>)), plt<span style=color:#ff6ac1>.</span>annotate(<span style=color:#5af78e>&#34;Nah&#34;</span>, (<span style=color:#ff9f43>0.1</span>, <span style=color:#ff9f43>0.1</span>))
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>title(<span style=color:#5af78e>&#34;Dropping eggs... will they break??&#34;</span>)
</span></span></code></pre></div></div><p><div><figure class=rounded><picture><source srcset=/tutorials/logistic_regression/2020-07-11-Logistic_Regression_files/2020-07-11-Logistic_Regression_3_0_hu8f3c5d8fc7a7f8ac7049ea1de680e657_102761_1920x0_resize_q90_h2_box_3.webp width=1920 height=1099 type=image/webp><source srcset=/tutorials/logistic_regression/2020-07-11-Logistic_Regression_files/2020-07-11-Logistic_Regression_3_0.png width=2794 height=1600 type=image/png><img width=2794 height=1600 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>What logistic regression is going to do, is get us $P(\text{egg broke}\ |\ \text{height it was dropped})$. The separator there is read in English as &ldquo;given that&rdquo;, if the syntax is new. The nomenclature generally denotes the output at $Y$ and the input as $X$, so this would be $P(Y=1|X)$. We call these sort of models that give the output condition on the input <strong>&ldquo;discriminative models&rdquo;</strong>.</p><p>There is a small subtlety here. The input $X$ is not just the height - we want logistic regression to handle multiple inputs combined in different ways (and a bias), which means we define the input $X$ as
$$ X = \beta_0 + \beta_1 X_1 + \beta_2 X_2 &mldr; $$</p><p>where we fit for the $\beta_i$ parameters, and $X_1$ is our first feature (aka variable/column, for us this is height), and $X_2$ the second feature (which we dont have in our example).</p><div class=aside markdown=1><p>Before going further, I should pause here and clarify the difference between probability and a probability ratio. If you open up random pages on logistic regression, sometimes you will see:</p><p>$$ f(x) = \frac{1}{1 + e^{-X}} \quad \text{or}\quad f(x) = \frac{e^{-X}}{1 + e^{-X}} $$</p><p>The former is the probability (and is a <em>logistic</em> function), the latter is the probability ratio (better known as the odds, or odds ratio, and this one is called a <em>logit transformation</em>), and you can derive the ratio by simply rearranging</p><p>$$ \frac{P(Y=1|X)}{1 - P(Y=1|X)}. $$</p><p>So if we have $P(Y=1|X)=0.9$, thats an odds ratio of $0.9/0.1 = 9$. As the probability goes from $0$ to $1$, the odds will go from $0$ to $\infty$, which means the <strong>log odds</strong> will go from $-\infty$ to $\infty$. <strong>For logistic regression, we normally optimise the log odds.</strong></p><p>Why? A few reasons. Firstly, numerically its easier to not work with bounded functions, and having infinite range is great. More importantly, working in log odds allows us to better understand the impact of any specific $X_i$ (column") in our model. For example, if your model is predicting $p=0.5$ and you perturb $X_1$ slightly, you might get $p=0.509$. This is a negligible change. But if you your model is giving you $p=0.99$ and you perturb $X_1$ and get $p=0.999$, thats not negligible, your model is 10 times as confident! Working in odds helps us get around this potential confounder.</p><p>So what we normally do is <strong>optimise using logit transformation</strong>, and <strong>report probabilities based on the logistic function</strong>.</p></div><p>Back on track, lets see what an abitrary fit to a logistic function would look like:</p><div class=width-76 markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>logistic</span>(x_1, beta_0, beta_1):
</span></span><span style=display:flex><span>    X <span style=color:#ff6ac1>=</span> beta_0 <span style=color:#ff6ac1>+</span> beta_1 <span style=color:#ff6ac1>*</span> x_1
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> <span style=color:#ff9f43>1</span> <span style=color:#ff6ac1>/</span> (<span style=color:#ff9f43>1</span> <span style=color:#ff6ac1>+</span> np<span style=color:#ff6ac1>.</span>exp(<span style=color:#ff6ac1>-</span>X))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>b0, b1 <span style=color:#ff6ac1>=</span> <span style=color:#ff6ac1>-</span><span style=color:#ff9f43>7</span>, <span style=color:#ff9f43>15</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Get some probabilities for arbitrary params</span>
</span></span><span style=display:flex><span>probs <span style=color:#ff6ac1>=</span> logistic(height, b0, b1)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>scatter(height, probs, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>f</span><span style=color:#5af78e>&#34;Prob | $</span><span style=color:#5af78e>\\</span><span style=color:#5af78e>beta_0=</span><span style=color:#5af78e>{</span>b0<span style=color:#5af78e>}</span><span style=color:#5af78e>,</span><span style=color:#5af78e>\\</span><span style=color:#5af78e> </span><span style=color:#5af78e>\\</span><span style=color:#5af78e>beta_1=</span><span style=color:#5af78e>{</span>b1<span style=color:#5af78e>}</span><span style=color:#5af78e>$&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>scatter(height, broke, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Data&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend(loc<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>), plt<span style=color:#ff6ac1>.</span>xlabel(<span style=color:#5af78e>&#34;$X_1$&#34;</span>), plt<span style=color:#ff6ac1>.</span>ylabel(<span style=color:#5af78e>r</span><span style=color:#5af78e>&#34;$P(broke)$&#34;</span>)
</span></span></code></pre></div></div><p><div><figure class=rounded><picture><source srcset=/tutorials/logistic_regression/2020-07-11-Logistic_Regression_files/2020-07-11-Logistic_Regression_5_0_hu7d08154c32e99272cea2ac6cad4cbee6_93587_1920x0_resize_q90_h2_box_3.webp width=1920 height=1027 type=image/webp><source srcset=/tutorials/logistic_regression/2020-07-11-Logistic_Regression_files/2020-07-11-Logistic_Regression_5_0.png width=2794 height=1495 type=image/png><img width=2794 height=1495 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>Notice that we are <em>comparing probabilities to binary outcomes here</em>. If we wanted outcomes, we&rsquo;d add some threshold (like 0.5) that we would cut on. Over 0.5 and its a success, under 0.5 and its a failure. We&rsquo;ll see this down below.</p><h2 id=using-sklearn>Using sklearn</h2><p>At this point, we now have - like any other form of regression - predictions vs data, and we could optimise the parameters ($\beta_i$) such that we fit the logistic as well as we can. Here is how you would do that using <code>sklearn</code>:</p><div class="expanded-code width-82" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>from</span> sklearn.linear_model <span style=color:#ff6ac1>import</span> LogisticRegression
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#ff6ac1>=</span> LogisticRegression(random_state<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Fit by passing in all features, and the outcome variable</span>
</span></span><span style=display:flex><span><span style=color:#78787e># The [:, None] just makes it a 2D array, not 1D</span>
</span></span><span style=display:flex><span>model<span style=color:#ff6ac1>.</span>fit(height[:, <span style=color:#ff6ac1>None</span>], broke)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Predict 0 or 1 on our inputs to check</span>
</span></span><span style=display:flex><span>pred_binary <span style=color:#ff6ac1>=</span> model<span style=color:#ff6ac1>.</span>predict(height[:, <span style=color:#ff6ac1>None</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Get prob for a smooth range of heights</span>
</span></span><span style=display:flex><span>xs <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>linspace(<span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>100</span>)
</span></span><span style=display:flex><span>pred_prob <span style=color:#ff6ac1>=</span> model<span style=color:#ff6ac1>.</span>predict_proba(xs[:, <span style=color:#ff6ac1>None</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Plotting code</span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>scatter(height, pred_binary, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>f</span><span style=color:#5af78e>&#34;sklearn prediction&#34;</span>, s<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>30</span>, 
</span></span><span style=display:flex><span>            marker<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#39;x&#39;</span>, zorder<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;#00796B&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>scatter(height, broke, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Data&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(xs, pred_prob[:, <span style=color:#ff9f43>1</span>], label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;sklearn probability&#34;</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;#006156&#34;</span>, zorder<span style=color:#ff6ac1>=-</span><span style=color:#ff9f43>1</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend(loc<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>, mode<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;expand&#34;</span>, ncol<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>3</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>xlabel(<span style=color:#5af78e>&#34;$X_1$&#34;</span>), plt<span style=color:#ff6ac1>.</span>ylabel(<span style=color:#5af78e>r</span><span style=color:#5af78e>&#34;$P(broke)$&#34;</span>);
</span></span></code></pre></div></div><p><div><figure class=rounded><picture><source srcset=/tutorials/logistic_regression/2020-07-11-Logistic_Regression_files/2020-07-11-Logistic_Regression_7_0_huc7fc943f15b3c905620945d634d1eb3b_109300_1920x0_resize_q90_h2_box_3.webp width=1920 height=1028 type=image/webp><source srcset=/tutorials/logistic_regression/2020-07-11-Logistic_Regression_files/2020-07-11-Logistic_Regression_7_0.png width=2841 height=1521 type=image/png><img width=2841 height=1521 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>Now if you&rsquo;re looking at the probability function and thinking &ldquo;this doesnt look like a sigmoid at all&rdquo;, you&rsquo;re entirely right. Welcome to the world of <strong>regularization</strong>. For small data like we have, the default L2 regularisation is going to ensure that our $\beta$ values stay pretty low.</p><p>If we want to see what happens without the penalty, we can turn it off, but note that in general, its good practise to keep it on because it helps with the generalisation of our models.</p><div class="expanded-code width-82" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model <span style=color:#ff6ac1>=</span> LogisticRegression(random_state<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>, penalty<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;none&#34;</span>)
</span></span><span style=display:flex><span>model<span style=color:#ff6ac1>.</span>fit(height[:, <span style=color:#ff6ac1>None</span>], broke)
</span></span><span style=display:flex><span>pred_binary <span style=color:#ff6ac1>=</span> model<span style=color:#ff6ac1>.</span>predict(height[:, <span style=color:#ff6ac1>None</span>])
</span></span><span style=display:flex><span>pred_prob <span style=color:#ff6ac1>=</span> model<span style=color:#ff6ac1>.</span>predict_proba(xs[:, <span style=color:#ff6ac1>None</span>])
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>scatter(height, pred_binary, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>f</span><span style=color:#5af78e>&#34;sklearn prediction&#34;</span>,
</span></span><span style=display:flex><span>            s<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>30</span>, marker<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#39;x&#39;</span>, zorder<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;#00796B&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>scatter(height, broke, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Data&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(xs, pred_prob[:, <span style=color:#ff9f43>1</span>], label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;sklearn probability&#34;</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;#006156&#34;</span>, zorder<span style=color:#ff6ac1>=-</span><span style=color:#ff9f43>1</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend(loc<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>, mode<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;expand&#34;</span>, ncol<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>3</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>xlabel(<span style=color:#5af78e>&#34;$X_1$&#34;</span>), plt<span style=color:#ff6ac1>.</span>ylabel(<span style=color:#5af78e>r</span><span style=color:#5af78e>&#34;$P(broke)$&#34;</span>);
</span></span></code></pre></div></div><p><div><figure class="img-main rounded"><picture><source srcset=/tutorials/logistic_regression/cover_hufbcac99c2fef1334aa87ba8ab868bfd7_111820_1920x0_resize_q90_h2_box_3.webp width=1920 height=1028 type=image/webp><source srcset=/tutorials/logistic_regression/cover.png width=2841 height=1521 type=image/png><img width=2841 height=1521 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>And you can now see that its a much better fit to the data (in terms of probability, not necessarily in terms of predictions). Instead of turning it off, we can also modify the <code>C</code> value which controls the regularization strength. The larger you set <code>C</code>, the less regularization you get.</p><h2 id=assumptions-that-go-into-logistic-regression>Assumptions that go into logistic regression</h2><p>Its time to get our hands dirty and talk about assumptions. First, you can incorporate uncertainty into <code>sklearns</code> implementation of LogisticRegression by changing the sample weights of each sample (if one sample has twice as much uncertainty as another, it has half the weight), which can be passed in when you fit the model. This is a choice you make, not one the regression makes. Furthermore, the regression does not assume:</p><ul><li><strong>Linearity</strong>: You do not need to have a linear relationship between dependent and independent variables (output and input).</li><li><strong>Normality</strong>: Your uncertainty does not have to be normally distribution.</li><li><strong>Homoscedasticity</strong>: Is not required. Without the jargon: you can have different amounts of uncertainty on different features.</li></ul><p>That said, there are stil some assumptions to be aware of:</p><ul><li><strong>Observations are independent</strong>: Each egg drop is brand new.</li><li><strong>Features are directional</strong>: Logisitc regression might give odd answers if the impact of features are one unidirectional. For example, mortality and age have peaks at the very young and very old, so using age to predict mortality not give you good results because you have only one $\beta$ value for that column that will only push the result in one direction.</li><li><strong>No multicollinearity</strong>: Whilst logistic regression can handle correlated variables, keep the correlations below 0.9.</li><li><strong>You have a large sample</strong>: For good results, try to ensure your sample size isn&rsquo;t just a handful of points. A hundred points is a good (and arbitrary) starting point, and you will need more points per feature that you use.</li></ul><h2 id=summary>Summary</h2><p>If you&rsquo;ve made it down to this point, congratulations! To try and briefly summarise everything:</p><ul><li>Binary lostistic regression is used for binary problems.</li><li>The logistic function smoothly transitions from 0 to 1 and gives a probability.</li><li>The logit function is a transformation to get odds from $X$.</li><li>Regularization is good for generalisation, even if it makes things look a bit odd on low number test data.</li><li>The assumptions going into logistic regression are fairly minimal, making it applicable for a variety of problems.</li></ul><p>I hope its useful!</p><hr><p>For your convenience, here&rsquo;s the code in one block:</p><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#78787e># Using a sigmiod to generate data for a sigmoid example</span>
</span></span><span style=display:flex><span><span style=color:#78787e># How uninspired!</span>
</span></span><span style=display:flex><span>height <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>random(<span style=color:#ff9f43>100</span>)
</span></span><span style=display:flex><span>broke <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>1</span><span style=color:#ff6ac1>/</span>(<span style=color:#ff9f43>1</span><span style=color:#ff6ac1>+</span>np<span style=color:#ff6ac1>.</span>exp(<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>20</span><span style=color:#ff6ac1>*</span>height<span style=color:#ff6ac1>+</span><span style=color:#ff9f43>5</span>)) <span style=color:#ff6ac1>&gt;</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>random(<span style=color:#ff9f43>100</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>scatter(height, broke, c<span style=color:#ff6ac1>=</span>broke)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>xlabel(<span style=color:#5af78e>&#34;Height&#34;</span>), plt<span style=color:#ff6ac1>.</span>ylabel(<span style=color:#5af78e>&#34;Did it break?!?&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>annotate(<span style=color:#5af78e>&#34;Yep&#34;</span>, (<span style=color:#ff9f43>0.55</span>, <span style=color:#ff9f43>0.85</span>)), plt<span style=color:#ff6ac1>.</span>annotate(<span style=color:#5af78e>&#34;Nah&#34;</span>, (<span style=color:#ff9f43>0.1</span>, <span style=color:#ff9f43>0.1</span>))
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>title(<span style=color:#5af78e>&#34;Dropping eggs... will they break??&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>logistic</span>(x_1, beta_0, beta_1):
</span></span><span style=display:flex><span>    X <span style=color:#ff6ac1>=</span> beta_0 <span style=color:#ff6ac1>+</span> beta_1 <span style=color:#ff6ac1>*</span> x_1
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> <span style=color:#ff9f43>1</span> <span style=color:#ff6ac1>/</span> (<span style=color:#ff9f43>1</span> <span style=color:#ff6ac1>+</span> np<span style=color:#ff6ac1>.</span>exp(<span style=color:#ff6ac1>-</span>X))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>b0, b1 <span style=color:#ff6ac1>=</span> <span style=color:#ff6ac1>-</span><span style=color:#ff9f43>7</span>, <span style=color:#ff9f43>15</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Get some probabilities for arbitrary params</span>
</span></span><span style=display:flex><span>probs <span style=color:#ff6ac1>=</span> logistic(height, b0, b1)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>scatter(height, probs, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>f</span><span style=color:#5af78e>&#34;Prob | $</span><span style=color:#5af78e>\\</span><span style=color:#5af78e>beta_0=</span><span style=color:#5af78e>{</span>b0<span style=color:#5af78e>}</span><span style=color:#5af78e>,</span><span style=color:#5af78e>\\</span><span style=color:#5af78e> </span><span style=color:#5af78e>\\</span><span style=color:#5af78e>beta_1=</span><span style=color:#5af78e>{</span>b1<span style=color:#5af78e>}</span><span style=color:#5af78e>$&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>scatter(height, broke, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Data&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend(loc<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>), plt<span style=color:#ff6ac1>.</span>xlabel(<span style=color:#5af78e>&#34;$X_1$&#34;</span>), plt<span style=color:#ff6ac1>.</span>ylabel(<span style=color:#5af78e>r</span><span style=color:#5af78e>&#34;$P(broke)$&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> sklearn.linear_model <span style=color:#ff6ac1>import</span> LogisticRegression
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#ff6ac1>=</span> LogisticRegression(random_state<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Fit by passing in all features, and the outcome variable</span>
</span></span><span style=display:flex><span><span style=color:#78787e># The [:, None] just makes it a 2D array, not 1D</span>
</span></span><span style=display:flex><span>model<span style=color:#ff6ac1>.</span>fit(height[:, <span style=color:#ff6ac1>None</span>], broke)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Predict 0 or 1 on our inputs to check</span>
</span></span><span style=display:flex><span>pred_binary <span style=color:#ff6ac1>=</span> model<span style=color:#ff6ac1>.</span>predict(height[:, <span style=color:#ff6ac1>None</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Get prob for a smooth range of heights</span>
</span></span><span style=display:flex><span>xs <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>linspace(<span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>100</span>)
</span></span><span style=display:flex><span>pred_prob <span style=color:#ff6ac1>=</span> model<span style=color:#ff6ac1>.</span>predict_proba(xs[:, <span style=color:#ff6ac1>None</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Plotting code</span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>scatter(height, pred_binary, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>f</span><span style=color:#5af78e>&#34;sklearn prediction&#34;</span>, s<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>30</span>, 
</span></span><span style=display:flex><span>            marker<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#39;x&#39;</span>, zorder<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;#00796B&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>scatter(height, broke, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Data&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(xs, pred_prob[:, <span style=color:#ff9f43>1</span>], label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;sklearn probability&#34;</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;#006156&#34;</span>, zorder<span style=color:#ff6ac1>=-</span><span style=color:#ff9f43>1</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend(loc<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>, mode<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;expand&#34;</span>, ncol<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>3</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>xlabel(<span style=color:#5af78e>&#34;$X_1$&#34;</span>), plt<span style=color:#ff6ac1>.</span>ylabel(<span style=color:#5af78e>r</span><span style=color:#5af78e>&#34;$P(broke)$&#34;</span>);
</span></span><span style=display:flex><span>model <span style=color:#ff6ac1>=</span> LogisticRegression(random_state<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>, penalty<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;none&#34;</span>)
</span></span><span style=display:flex><span>model<span style=color:#ff6ac1>.</span>fit(height[:, <span style=color:#ff6ac1>None</span>], broke)
</span></span><span style=display:flex><span>pred_binary <span style=color:#ff6ac1>=</span> model<span style=color:#ff6ac1>.</span>predict(height[:, <span style=color:#ff6ac1>None</span>])
</span></span><span style=display:flex><span>pred_prob <span style=color:#ff6ac1>=</span> model<span style=color:#ff6ac1>.</span>predict_proba(xs[:, <span style=color:#ff6ac1>None</span>])
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>scatter(height, pred_binary, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>f</span><span style=color:#5af78e>&#34;sklearn prediction&#34;</span>,
</span></span><span style=display:flex><span>            s<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>30</span>, marker<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#39;x&#39;</span>, zorder<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;#00796B&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>scatter(height, broke, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Data&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(xs, pred_prob[:, <span style=color:#ff9f43>1</span>], label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;sklearn probability&#34;</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;#006156&#34;</span>, zorder<span style=color:#ff6ac1>=-</span><span style=color:#ff9f43>1</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend(loc<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>, mode<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;expand&#34;</span>, ncol<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>3</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>xlabel(<span style=color:#5af78e>&#34;$X_1$&#34;</span>), plt<span style=color:#ff6ac1>.</span>ylabel(<span style=color:#5af78e>r</span><span style=color:#5af78e>&#34;$P(broke)$&#34;</span>);
</span></span></code></pre></div><div class="relative max-w-xl mx-auto my-20"><div class="fancy_card horizontal"><div class=card_translator><div class="card_rotator tiny_rot card_layer"><div class="card_layer newsletter p-2"><div class="newsletter-inner h-full"><div class="relative h-full sib-form-container" id=sib-form-container><div class="mb-6 lg:mb-12 text-center"><h3 class="text-main-200 mb-2 lg:mb-8">Stay in the loop!</h3><p class="text-main-100 text-lg text-opacity-70">For new releases, exclusive giveaways, and reviews.</p></div><form action="https://Cosmiccoding.us5.list-manage.com/subscribe/post?u=aebe5de1d2e40dccb3aeca491&id=3987dd4a1f" method=post id=mc-embedded-subscribe-form name=mc-embedded-subscribe-form class="validate w-full" target=_blank novalidate><div><input type=email class="w-full appearance-none bg-main-600 mb-4 border border-main-600 bg-opacity-5 focus:border-main-300 rounded-sm px-4 py-3 text-main-100 placeholder-main-100 required" placeholder="Your best email…" aria-label="Your best email…" name=EMAIL required data-required=true id=EMAIL><div style=position:absolute;left:-5000px aria-hidden=true><input type=text name=b_aebe5de1d2e40dccb3aeca491_3987dd4a1f tabindex=-1></div><input type=submit value=Subscribe name=subscribe id=mc-embedded-subscribe class="button btn bg-main-600 hover:bg-main-400 shadow w-full"></div><p id=mce-error-response style=display:none class="text-center mt-2 opacity-50 text-main-200">There was a problem, please try again.</p><p id=mce-success-response style=display:none class="text-center mt-2 opacity-50 text-main-200">Thanks mate, won't let ya down.</p></form></div></div></div><div class="card_layer card_effect card_soft_glare" style=pointer-events:none></div></div></div></div></div></div><script type=text/x-mathjax-config>
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script></div><script language=javascript type=text/javascript src=https://cosmiccoding.com.au/js/main.min.11673d10a962fb5205229607e62ea1d23424d35dad33db7bfe2a09a63d5409fa.js></script>
<script async defer src="https://www.googletagmanager.com/gtag/js?id=G-GRX6QE03YR"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-GRX6QE03YR")</script></div></body></html>