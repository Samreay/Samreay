<!doctype html><html lang=en-gb><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>Introduction to Naive Bayes - Samuel Hinton</title><link rel=stylesheet href="https://cosmiccoding.com.au/css/main.min.f97531dbcfa76c58822e21d7ec7cb2612e5bb7e7785e16d1aeeef3f61bbae184.css" integrity="sha256-+XUx28+nbFiCLiHX7HyyYS5bt+d4XhbRru7z9hu64YQ="><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel="shortcut icon" href=https://cosmiccoding.com.au/img/favicon.png type=image/x-icon></head><meta name=description content="Overview, assumptions, and pitfalls."><meta name=robots content="noodp"><link rel=canonical href=https://cosmiccoding.com.au/tutorials/naivebayes/><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://cosmiccoding.com.au/tutorials/naivebayes/cover.jpg"><meta name=twitter:title content="Introduction to Naive Bayes"><meta name=twitter:description content="Overview, assumptions, and pitfalls."><meta property="og:title" content="Introduction to Naive Bayes"><meta property="og:description" content="Overview, assumptions, and pitfalls."><meta property="og:type" content="article"><meta property="og:url" content="https://cosmiccoding.com.au/tutorials/naivebayes/"><meta property="og:image" content="https://cosmiccoding.com.au/tutorials/naivebayes/cover.jpg"><meta property="article:section" content="tutorials"><meta property="article:published_time" content="2020-07-11T00:00:00+00:00"><meta property="article:modified_time" content="2020-07-11T00:00:00+00:00"><meta property="article:section" content="tutorial"><meta property="article:published_time" content="2020-07-11 00:00:00 +0000 UTC"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Introduction to Naive Bayes","genre":"tutorial","url":"https:\/\/cosmiccoding.com.au\/tutorials\/naivebayes\/","datePublished":"2020-07-11 00:00:00 \u002b0000 UTC","description":"Overview, assumptions, and pitfalls.","author":{"@type":"Person","name":""}}</script><body><div class="flex flex-col min-h-screen overflow-hidden"><header class="absolute w-full z-30"><div class="max-w-6xl mx-auto px-4 sm:px-6"><div class="flex items-center justify-between h-20"><div class="flex-shrink-0 mr-4"><a class=block href=/ aria-label="Samuel Hinton"><h2 class=logo>SRH</h2></a></div><nav class="hidden md:flex md:flex-grow"><ul class="flex flex-grow justify-end flex-wrap items-center"><li><a href=/#books class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Books</a></li><li><a href=/reviews class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Reviews</a></li><li><a href=/tutorials class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Tutorials</a></li><li><a href=/blogs class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Blog</a></li><li><a href=/#courses class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Courses</a></li><li><a href=/static/resume/HintonCV.pdf class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">CV</a></li></ul></nav><div class=md:hidden x-data="{ expanded: false }"><button class=hamburger :class="{ 'active': expanded }" @click.stop="expanded = !expanded" aria-controls=mobile-nav :aria-expanded=expanded>
<span class=sr-only>Menu</span><svg class="w-6 h-6 fill-current text-gray-300 hover:text-gray-200 transition duration-150 ease-in-out" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><rect y="4" width="24" height="2" rx="1"/><rect y="11" width="24" height="2" rx="1"/><rect y="18" width="24" height="2" rx="1"/></svg></button><nav id=mobile-nav class="absolute top-full z-20 left-0 w-full px-4 sm:px-6 overflow-hidden transition-all duration-300 ease-in-out" x-ref=mobileNav :style="expanded ? 'max-height: ' + $refs.mobileNav.scrollHeight + 'px; opacity: 1' : 'max-height: 0; opacity: .8'" @click.away="expanded = false" @keydown.escape.window="expanded = false" x-cloak><ul class="bg-gray-800 px-4 py-2"><li><a href=/#books class="flex text-gray-300 hover:text-gray-200 py-2">Books</a></li><li><a href=/reviews class="flex text-gray-300 hover:text-gray-200 py-2">Reviews</a></li><li><a href=/tutorials class="flex text-gray-300 hover:text-gray-200 py-2">Tutorials</a></li><li><a href=/blogs class="flex text-gray-300 hover:text-gray-200 py-2">Blog</a></li><li><a href=/#courses class="flex text-gray-300 hover:text-gray-200 py-2">Courses</a></li><li><a href=/static/resume/HintonCV.pdf class="flex text-gray-300 hover:text-gray-200 py-2">CV</a></li></ul></nav></div></div></div></header><div class=flex-grow><div id=post-container class="content content-wider blog-post relative"><div class="section-header blog"><h1 class=title>Introduction to Naive Bayes</h1><p>6th July 2020</p><p>Overview, assumptions, and pitfalls.</p></div><div><ul class="grid gap-6 w-full md:grid-cols-2" style=list-style:none;padding-left:0><li><input type=radio id=show-code name=code-toggle value=show-code class="hidden peer" onchange=clickCheckbox(this) required checked>
<label for=show-code class="inline-flex justify-between items-center p-5 w-full text-gray-500 bg-gray-800 rounded-lg border border-gray-200 cursor-pointer peer-checked:border-2 peer-checked:border-main-300 peer-checked:text-white peer-checked:bg-gray-700 hover:text-gray-200 hover:bg-gray-700"><div class="block w-full"><div class="w-full text-center text-lg font-semibold">Show me everything!</div><div class="w-full text-center text-sm">Oh yeah, coding time.</div></div></label></li><li><input type=radio id=hide-code name=code-toggle value=hide-code class="hidden peer" onchange=clickCheckbox(this)>
<label for=hide-code class="inline-flex justify-between items-center p-5 w-full text-gray-500 bg-gray-800 rounded-lg border border-gray-200 cursor-pointer peer-checked:border-2 peer-checked:border-main-300 peer-checked:text-white peer-checked:bg-gray-700 hover:text-gray-200 hover:bg-gray-700"><div class="block w-full"><div class="w-full text-center text-lg font-semibold">Just the plots</div><div class="w-full text-center text-sm">Code is nasty.</div></div></label></li></ul></div><script>function clickCheckbox(e){e.id=="show-code"?document.getElementById("post-container").classList.remove("hide-code"):document.getElementById("post-container").classList.add("hide-code")}</script><p>Naive Bayes is a simple, extraordinarily fast, and incredibly useful categorical classification tool. The underlying crux of Naive Bayes is that we assume feature independence, plug it into Bayes theorem, and the math that falls out is incredibly simple.</p><h2 id=the-background-math>The background math</h2><p>Let&rsquo;s start with the famous Bayes Theorem:</p><p>$$ P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)} $$</p><p>On the left hand side, we have the posterior: the probability of the output given the input. On the right, we have a few terms:</p><ul><li>$P(X|Y)$ - the <strong>likelihood</strong> - the probability of getting an input, given the output.</li><li>$P(Y)$ - the <strong>prior</strong> - the probability of getting the output in general.</li><li>$P(X)$ - the <strong>evidence</strong> - which is the probability of getting some input.</li></ul><div class=aside markdown=1><p>For those coming from a model fitting side, you&rsquo;ve probably seen this formulated using $\theta$ as the input pararameters and $x$ as the data, trying to calculate the posterior $P(\theta|x)$. The math is the same here, our usage of things like the prior and evidence just change slightly.</p></div><p>What should be jumping out is that all of these terms are very easy to calculate from the data. Before we jump into an example, let me first complicate things a bit by pointing out that $X$ is not just one feature. $X$ is all our features combined. Naive Bayes is used a lot in Natural Language Processing (NLP), so lets make a dummy example to classify spam. Let us have only two features:</p><ol><li>$X_1$ is whether or not the email contains our name</li><li>$X_2$ is whether or not the email contains the word &ldquo;win&rdquo;</li><li>$Y$ is if the email is spam.</li></ol><p>What we are really wanting to do is predict</p><p>$$P(Y|X_1, X_2) = \frac{P(X_1, X_2 | Y) P(Y)}{P(X_1, X_2)} $$</p><p>This is where the assumption of <strong>conditional independence</strong> comes into play, because if we assume the features are independent, we can split up our joint probability $P(X_1, X_2)$ into $P(X_1)P(X_2)$, and the above posterior becomes:</p><p>$$P(Y|X_1, X_2) = \frac{P(X_1| Y)\ P( X_2 | Y)\ P(Y)}{P(X_1)\ P(X_2)} $$</p><p>Lets now make up 5 emails:</p><div class="reduced-code width-59" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>import</span> pandas <span style=color:#ff6ac1>as</span> pd
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>emails <span style=color:#ff6ac1>=</span> [
</span></span><span style=display:flex><span>    <span style=color:#5af78e>&#34;Hey Sam, I hope everyone&#39;s keep up!&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#5af78e>&#34;Dear Sir/Madam. I hope this email finds you well...&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#5af78e>&#34;Hi Sam, any updates on our clients win?&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#5af78e>&#34;WHAT AN OFFER! CLICK HERE TO WIN BIG!&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#5af78e>&#34;Weekly article curation list from someone I want&#34;</span>
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>spam <span style=color:#ff6ac1>=</span> [<span style=color:#ff6ac1>False</span>, <span style=color:#ff6ac1>True</span>, <span style=color:#ff6ac1>False</span>, <span style=color:#ff6ac1>True</span>, <span style=color:#ff6ac1>False</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># And turn this into a dataset</span>
</span></span><span style=display:flex><span>df <span style=color:#ff6ac1>=</span> pd<span style=color:#ff6ac1>.</span>DataFrame(<span style=color:#ff5c57>dict</span>(content<span style=color:#ff6ac1>=</span>emails, spam<span style=color:#ff6ac1>=</span>spam))
</span></span><span style=display:flex><span>df[<span style=color:#5af78e>&#34;has_name&#34;</span>] <span style=color:#ff6ac1>=</span> df<span style=color:#ff6ac1>.</span>content<span style=color:#ff6ac1>.</span>str<span style=color:#ff6ac1>.</span>lower()<span style=color:#ff6ac1>.</span>str<span style=color:#ff6ac1>.</span>contains(<span style=color:#5af78e>&#34;sam&#34;</span>)
</span></span><span style=display:flex><span>df[<span style=color:#5af78e>&#34;has_win&#34;</span>] <span style=color:#ff6ac1>=</span> df<span style=color:#ff6ac1>.</span>content<span style=color:#ff6ac1>.</span>str<span style=color:#ff6ac1>.</span>lower()<span style=color:#ff6ac1>.</span>str<span style=color:#ff6ac1>.</span>contains(<span style=color:#5af78e>&#34;win&#34;</span>)
</span></span><span style=display:flex><span>df
</span></span></code></pre></div></div><div><table class="table-auto table dataframe"><thead><tr style=text-align:right><th></th><th>content</th><th>spam</th><th>has_name</th><th>has_win</th></tr></thead><tbody><tr><th>0</th><td>Hey Sam, I hope everyone's keep up!</td><td>False</td><td>True</td><td>False</td></tr><tr><th>1</th><td>Dear Sir/Madam. I hope this email finds you we...</td><td>True</td><td>False</td><td>False</td></tr><tr><th>2</th><td>Hi Sam, any updates on our clients win?</td><td>False</td><td>True</td><td>True</td></tr><tr><th>3</th><td>WHAT AN OFFER! CLICK HERE TO WIN BIG!</td><td>True</td><td>False</td><td>True</td></tr><tr><th>4</th><td>Weekly article curation list from someone I want</td><td>False</td><td>False</td><td>False</td></tr></tbody></table></div><p>Note this is <em>not</em> how you&rsquo;d do tokenisation, I&rsquo;m just keeping the example simple. But lets have a look at our Naive Bayes math now. To recap, we want to determine the terms in this:</p><p>$$P(Y|X_1, X_2) = \frac{P(X_1| Y)\ P( X_2 | Y)\ P(Y)}{P(X_1)\ P(X_2)} $$</p><p>Or to condense the notation down, if we have $N$ features:</p><p>$$P(Y|X) = \frac{P(Y) \prod_i^N P(X_i|Y)}{P(X)} $$</p><p>The way we&rsquo;d go through and classify the emails by hand, is for each email we&rsquo;d have $X_1$ is either 0 or 1, same for $X_2$ (depending on that email). To take the first email: <em>&ldquo;Hey Sam, I hope everyone&rsquo;s following so far!&rdquo;</em>, if we wanted to calculate $P(X|Y=1)$, we&rsquo;d have:</p><ul><li>$P(X_1=1)$ - the fraction of entries that have my name. $P(X_1=1) = 1/5$.</li><li>$P(X_2=0)$ - the fraction of entries that <strong>lack</strong> the word win: $P(X_2=0) = 3/5$.</li><li>$P(Y=1)$ - the fraction of spam in general: $2/5$.</li><li>$P(X_1=1|Y=1)$ - the fraction of spam emails which have my name: $0/2$.</li><li>$P(X_2=0|Y=1)$ - the fraction of spam which don&rsquo;t have win: $1/2$.</li></ul><p>$$P(Y=1|X) = \frac{0.4 \times 0.0 \times 0.5}{0.2 \times 0.6} = 0.0 $$</p><p>And yes, there is a slight issue here - we have a probability of zero that will turn <strong>everything</strong> to zero when we multiply it out, removing potentially useful information from other features! This is a pitfall to Naive Bayes, but is easily fixed. We generally implement something called Laplace Smoothing (a specific case of Lidstone smoothing), which is use here to ensure we don&rsquo;t get a zero probability.</p><p>The way it works is instead of just taking the fraction, we add small amount to the denominator and numerator:</p><p>$$P(X_1=1|Y=1) = \frac{0 + 1}{2 + 2} = \frac{1}{4}$$</p><p>Here I add 1 to the numerator and 2 to the denominator. The choice here is <strong>not</strong> locked in stone. It is also common to add a number much smaller than one to the numerator, and a larger number representative of the number of classes you have to the denominator. The choice is part of the regularisation of the model, just make sure you cant get probabilities greater than one. But once we do this smoothing, we ruin our probabilities as they aren&rsquo;t normalised, and so we now need to compute both spam and not spam, and compare them (instead of just checking if the probability of spam is above or below 0.5).</p><p>If we are now checking both $P(Y=1|X)$ and $P(Y=0|X)$, then we can also not bother computing the $P(X)$ terms, because they are the same for both calculations.</p><h2 id=slow-python-implementation>Slow Python Implementation</h2><p>Lets write some super basic code to try and classify our emails:</p><div class=width-72 markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>import</span> numpy <span style=color:#ff6ac1>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>frac</span>(x):
</span></span><span style=display:flex><span>    <span style=color:#78787e># Get smoothed fraction of array thats true</span>
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> (x<span style=color:#ff6ac1>.</span>sum() <span style=color:#ff6ac1>+</span> <span style=color:#ff9f43>1</span>) <span style=color:#ff6ac1>/</span> (x<span style=color:#ff6ac1>.</span>size <span style=color:#ff6ac1>+</span> <span style=color:#ff9f43>2</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>conditional</span>(x_vec, x_val, y_vec, y_val):
</span></span><span style=display:flex><span>    <span style=color:#78787e># Compute P(x_vec = x_val | y_vec = y_val)</span>
</span></span><span style=display:flex><span>     <span style=color:#ff6ac1>return</span> frac(x_vec[y_vec <span style=color:#ff6ac1>==</span> y_val] <span style=color:#ff6ac1>==</span> x_val)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>classify_outcome</span>(X, Y, y):
</span></span><span style=display:flex><span>    <span style=color:#78787e># Get unnormalised prob for outcome y using X and Y</span>
</span></span><span style=display:flex><span>    probs <span style=color:#ff6ac1>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>for</span> index, row <span style=color:#ff6ac1>in</span> X<span style=color:#ff6ac1>.</span>iterrows():
</span></span><span style=display:flex><span>        <span style=color:#78787e># P(Y=y) term</span>
</span></span><span style=display:flex><span>        prob <span style=color:#ff6ac1>=</span> (y <span style=color:#ff6ac1>==</span> Y)<span style=color:#ff6ac1>.</span>astype(<span style=color:#ff5c57>float</span>)<span style=color:#ff6ac1>.</span>mean()
</span></span><span style=display:flex><span>        <span style=color:#78787e># For everying X_i we have</span>
</span></span><span style=display:flex><span>        <span style=color:#ff6ac1>for</span> i, x <span style=color:#ff6ac1>in</span> <span style=color:#ff5c57>enumerate</span>(row):
</span></span><span style=display:flex><span>            X_vec <span style=color:#ff6ac1>=</span> X<span style=color:#ff6ac1>.</span>iloc[:, i]
</span></span><span style=display:flex><span>            <span style=color:#78787e># P(X_i = x | Y = y) term</span>
</span></span><span style=display:flex><span>            prob <span style=color:#ff6ac1>*=</span> conditional(X_vec, x, Y, y)
</span></span><span style=display:flex><span>        probs<span style=color:#ff6ac1>.</span>append(prob)
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> np<span style=color:#ff6ac1>.</span>array(probs)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>classify_emails</span>(X, Y):
</span></span><span style=display:flex><span>    <span style=color:#78787e># Inefficient row by row calculation to illustrate</span>
</span></span><span style=display:flex><span>    prob_spam <span style=color:#ff6ac1>=</span> classify_outcome(X, Y, <span style=color:#ff9f43>1</span>)
</span></span><span style=display:flex><span>    prob_not_spam <span style=color:#ff6ac1>=</span> classify_outcome(X, Y, <span style=color:#ff9f43>0</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> prob_spam <span style=color:#ff6ac1>&gt;</span> prob_not_spam
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>df[<span style=color:#5af78e>&#34;prediction&#34;</span>] <span style=color:#ff6ac1>=</span> classify_emails(df[[<span style=color:#5af78e>&#34;has_name&#34;</span>, <span style=color:#5af78e>&#34;has_win&#34;</span>]], df<span style=color:#ff6ac1>.</span>spam)
</span></span><span style=display:flex><span>df
</span></span></code></pre></div></div><div><table class="table-auto table dataframe"><thead><tr style=text-align:right><th></th><th>content</th><th>spam</th><th>has_name</th><th>has_win</th><th>prediction</th></tr></thead><tbody><tr><th>0</th><td>Hey Sam, I hope everyone's keep up!</td><td>False</td><td>True</td><td>False</td><td>False</td></tr><tr><th>1</th><td>Dear Sir/Madam. I hope this email finds you we...</td><td>True</td><td>False</td><td>False</td><td>True</td></tr><tr><th>2</th><td>Hi Sam, any updates on our clients win?</td><td>False</td><td>True</td><td>True</td><td>False</td></tr><tr><th>3</th><td>WHAT AN OFFER! CLICK HERE TO WIN BIG!</td><td>True</td><td>False</td><td>True</td><td>True</td></tr><tr><th>4</th><td>Weekly article curation list from someone I want</td><td>False</td><td>False</td><td>False</td><td>True</td></tr></tbody></table></div><p>So our predictions have found 3 spam emails, 2 of which are correct, and the one without my name in it from a mailing list has be flagged as well. We could play around with the amount of smoothing, and if you set it to zero you&rsquo;ll see that only one email gets flagged as spam.</p><p>Lets now graduate from bad explantory code, to using <code>scikit-learn</code>.</p><h2 id=scikit-learn-code>Scikit-Learn Code</h2><div class="reduced-code width-43" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>from</span> sklearn.naive_bayes <span style=color:#ff6ac1>import</span> BernoulliNB
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X, Y <span style=color:#ff6ac1>=</span> df[[<span style=color:#5af78e>&#34;has_name&#34;</span>, <span style=color:#5af78e>&#34;has_win&#34;</span>]], df<span style=color:#ff6ac1>.</span>spam
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>nb <span style=color:#ff6ac1>=</span> BernoulliNB()
</span></span><span style=display:flex><span>nb<span style=color:#ff6ac1>.</span>fit(X, Y)
</span></span><span style=display:flex><span>predictions <span style=color:#ff6ac1>=</span> nb<span style=color:#ff6ac1>.</span>predict(X)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df[<span style=color:#5af78e>&#34;BernoulliNB_pred&#34;</span>] <span style=color:#ff6ac1>=</span> predictions
</span></span><span style=display:flex><span>df
</span></span></code></pre></div></div><div><table class="table-auto table dataframe"><thead><tr style=text-align:right><th></th><th>content</th><th>spam</th><th>has_name</th><th>has_win</th><th>prediction</th><th>BernoulliNB_pred</th></tr></thead><tbody><tr><th>0</th><td>Hey Sam, I hope everyone's keep up!</td><td>False</td><td>True</td><td>False</td><td>False</td><td>False</td></tr><tr><th>1</th><td>Dear Sir/Madam. I hope this email finds you we...</td><td>True</td><td>False</td><td>False</td><td>True</td><td>True</td></tr><tr><th>2</th><td>Hi Sam, any updates on our clients win?</td><td>False</td><td>True</td><td>True</td><td>False</td><td>False</td></tr><tr><th>3</th><td>WHAT AN OFFER! CLICK HERE TO WIN BIG!</td><td>True</td><td>False</td><td>True</td><td>True</td><td>True</td></tr><tr><th>4</th><td>Weekly article curation list from someone I want</td><td>False</td><td>False</td><td>False</td><td>True</td><td>True</td></tr></tbody></table></div><p>Exactly what we got! You might have noticed the <code>Bernoulli</code> part above. There are different implementations of Naive Bayes, depending on the distribution of the data. Because all our features are 0 or 1, True or False (aka Bernoulli), we use the Bernoulli implementation. <a href=https://scikit-learn.org/stable/modules/naive_bayes.html>You can see the other implementations here</a>, including an example on the standard IRIS dataset. You can control how much smoothing is done using the <code>alpha</code> parameter when you create the fitter.</p><h2 id=when-to-use-naive-bayes>When to use Naive Bayes</h2><p>Now that you can see how easy it is to imlpement a Naive Bayes model, lets pause and just outline use cases, assumptions, and pitfalls so you know when you <em>should</em> and when you <em>shouldn&rsquo;t</em> use the technique.</p><ul><li>Naive Baues is <strong>fast</strong>. It is a great tool to get off the ground.</li><li>It <strong>does not</strong> handle correlated features well, due to the assumption of independence.</li><li>It <strong>doesn&rsquo;t need much data to train</strong>, so great for small datasets.</li><li>It can be <strong>misled easily</strong> by irrelevant features.</li><li>If you have a lot of data, <a href=https://cosmiccoding.com.au/tutorials/logistic_regression>logistic regression</a> may be a <a href=http://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf><strong>better choice</strong></a></li></ul><h2 id=how-to-improve-our-spam-classifier>How to improve our spam classifier</h2><p>Obviously this write up is just providing an easy to understand example. If we wanted to take it more seriously we could:</p><ul><li>Include more words and determine a way to rank their importance</li><li>Removing filler words (the, this, a, it, I, etc)</li><li>Use lemmatization to group words (work, working, worked)</li><li>Using n-grams to find multi-word matches.</li><li>Include information from sender, time, location, etc</li><li>Get a ton more data!</li></ul><h2 id=summary>Summary</h2><p>Naive Bayes is a super faster, super simple classifier, that will work wonders even when you don&rsquo;t have much data. Its a great model for getting something off the ground, and you might be surprised at how well it performs.</p><p>If theres one thing in general to take away from this, its Bayes theorem. I&rsquo;ll put it big down below to drum it home!</p><p><div><figure class="img-main small img-screen rounded"><picture><source srcset=/tutorials/naivebayes/cover_hu177a6c07e7039a31d1a49b7a4e261c2c_107201_1000x0_resize_q90_h2_box.webp width=1000 height=650 type=image/webp><source srcset=/tutorials/naivebayes/cover.jpg width=1000 height=650 type=image/jpg><img width=1000 height=650 loading=lazy decoding=async alt=jpeg src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><hr><p>For your convenience, here&rsquo;s the code in one block:</p><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>import</span> pandas <span style=color:#ff6ac1>as</span> pd
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>emails <span style=color:#ff6ac1>=</span> [
</span></span><span style=display:flex><span>    <span style=color:#5af78e>&#34;Hey Sam, I hope everyone&#39;s keep up!&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#5af78e>&#34;Dear Sir/Madam. I hope this email finds you well...&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#5af78e>&#34;Hi Sam, any updates on our clients win?&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#5af78e>&#34;WHAT AN OFFER! CLICK HERE TO WIN BIG!&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#5af78e>&#34;Weekly article curation list from someone I want&#34;</span>
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>spam <span style=color:#ff6ac1>=</span> [<span style=color:#ff6ac1>False</span>, <span style=color:#ff6ac1>True</span>, <span style=color:#ff6ac1>False</span>, <span style=color:#ff6ac1>True</span>, <span style=color:#ff6ac1>False</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># And turn this into a dataset</span>
</span></span><span style=display:flex><span>df <span style=color:#ff6ac1>=</span> pd<span style=color:#ff6ac1>.</span>DataFrame(<span style=color:#ff5c57>dict</span>(content<span style=color:#ff6ac1>=</span>emails, spam<span style=color:#ff6ac1>=</span>spam))
</span></span><span style=display:flex><span>df[<span style=color:#5af78e>&#34;has_name&#34;</span>] <span style=color:#ff6ac1>=</span> df<span style=color:#ff6ac1>.</span>content<span style=color:#ff6ac1>.</span>str<span style=color:#ff6ac1>.</span>lower()<span style=color:#ff6ac1>.</span>str<span style=color:#ff6ac1>.</span>contains(<span style=color:#5af78e>&#34;sam&#34;</span>)
</span></span><span style=display:flex><span>df[<span style=color:#5af78e>&#34;has_win&#34;</span>] <span style=color:#ff6ac1>=</span> df<span style=color:#ff6ac1>.</span>content<span style=color:#ff6ac1>.</span>str<span style=color:#ff6ac1>.</span>lower()<span style=color:#ff6ac1>.</span>str<span style=color:#ff6ac1>.</span>contains(<span style=color:#5af78e>&#34;win&#34;</span>)
</span></span><span style=display:flex><span>df
</span></span><span style=display:flex><span><span style=color:#ff6ac1>import</span> numpy <span style=color:#ff6ac1>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>frac</span>(x):
</span></span><span style=display:flex><span>    <span style=color:#78787e># Get smoothed fraction of array thats true</span>
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> (x<span style=color:#ff6ac1>.</span>sum() <span style=color:#ff6ac1>+</span> <span style=color:#ff9f43>1</span>) <span style=color:#ff6ac1>/</span> (x<span style=color:#ff6ac1>.</span>size <span style=color:#ff6ac1>+</span> <span style=color:#ff9f43>2</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>conditional</span>(x_vec, x_val, y_vec, y_val):
</span></span><span style=display:flex><span>    <span style=color:#78787e># Compute P(x_vec = x_val | y_vec = y_val)</span>
</span></span><span style=display:flex><span>     <span style=color:#ff6ac1>return</span> frac(x_vec[y_vec <span style=color:#ff6ac1>==</span> y_val] <span style=color:#ff6ac1>==</span> x_val)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>classify_outcome</span>(X, Y, y):
</span></span><span style=display:flex><span>    <span style=color:#78787e># Get unnormalised prob for outcome y using X and Y</span>
</span></span><span style=display:flex><span>    probs <span style=color:#ff6ac1>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>for</span> index, row <span style=color:#ff6ac1>in</span> X<span style=color:#ff6ac1>.</span>iterrows():
</span></span><span style=display:flex><span>        <span style=color:#78787e># P(Y=y) term</span>
</span></span><span style=display:flex><span>        prob <span style=color:#ff6ac1>=</span> (y <span style=color:#ff6ac1>==</span> Y)<span style=color:#ff6ac1>.</span>astype(<span style=color:#ff5c57>float</span>)<span style=color:#ff6ac1>.</span>mean()
</span></span><span style=display:flex><span>        <span style=color:#78787e># For everying X_i we have</span>
</span></span><span style=display:flex><span>        <span style=color:#ff6ac1>for</span> i, x <span style=color:#ff6ac1>in</span> <span style=color:#ff5c57>enumerate</span>(row):
</span></span><span style=display:flex><span>            X_vec <span style=color:#ff6ac1>=</span> X<span style=color:#ff6ac1>.</span>iloc[:, i]
</span></span><span style=display:flex><span>            <span style=color:#78787e># P(X_i = x | Y = y) term</span>
</span></span><span style=display:flex><span>            prob <span style=color:#ff6ac1>*=</span> conditional(X_vec, x, Y, y)
</span></span><span style=display:flex><span>        probs<span style=color:#ff6ac1>.</span>append(prob)
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> np<span style=color:#ff6ac1>.</span>array(probs)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>classify_emails</span>(X, Y):
</span></span><span style=display:flex><span>    <span style=color:#78787e># Inefficient row by row calculation to illustrate</span>
</span></span><span style=display:flex><span>    prob_spam <span style=color:#ff6ac1>=</span> classify_outcome(X, Y, <span style=color:#ff9f43>1</span>)
</span></span><span style=display:flex><span>    prob_not_spam <span style=color:#ff6ac1>=</span> classify_outcome(X, Y, <span style=color:#ff9f43>0</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> prob_spam <span style=color:#ff6ac1>&gt;</span> prob_not_spam
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>df[<span style=color:#5af78e>&#34;prediction&#34;</span>] <span style=color:#ff6ac1>=</span> classify_emails(df[[<span style=color:#5af78e>&#34;has_name&#34;</span>, <span style=color:#5af78e>&#34;has_win&#34;</span>]], df<span style=color:#ff6ac1>.</span>spam)
</span></span><span style=display:flex><span>df
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> sklearn.naive_bayes <span style=color:#ff6ac1>import</span> BernoulliNB
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X, Y <span style=color:#ff6ac1>=</span> df[[<span style=color:#5af78e>&#34;has_name&#34;</span>, <span style=color:#5af78e>&#34;has_win&#34;</span>]], df<span style=color:#ff6ac1>.</span>spam
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>nb <span style=color:#ff6ac1>=</span> BernoulliNB()
</span></span><span style=display:flex><span>nb<span style=color:#ff6ac1>.</span>fit(X, Y)
</span></span><span style=display:flex><span>predictions <span style=color:#ff6ac1>=</span> nb<span style=color:#ff6ac1>.</span>predict(X)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df[<span style=color:#5af78e>&#34;BernoulliNB_pred&#34;</span>] <span style=color:#ff6ac1>=</span> predictions
</span></span><span style=display:flex><span>df
</span></span></code></pre></div><div class="relative max-w-xl mx-auto my-20"><div class="fancy_card horizontal"><div class=card_translator><div class="card_rotator tiny_rot card_layer"><div class="card_layer newsletter p-2"><div class="newsletter-inner h-full"><div class="relative h-full sib-form-container" id=sib-form-container><div class="mb-6 lg:mb-12 text-center"><h3 class="text-main-200 mb-2 lg:mb-8">Stay in the loop!</h3><p class="text-main-100 text-lg text-opacity-70">For new releases, exclusive giveaways, and reviews.</p></div><form action="https://Cosmiccoding.us5.list-manage.com/subscribe/post?u=aebe5de1d2e40dccb3aeca491&id=3987dd4a1f" method=post id=mc-embedded-subscribe-form name=mc-embedded-subscribe-form class="validate w-full" target=_blank novalidate><div><input type=email class="w-full appearance-none bg-main-600 mb-4 border border-main-600 bg-opacity-5 focus:border-main-300 rounded-sm px-4 py-3 text-main-100 placeholder-main-100 required" placeholder="Your best email…" aria-label="Your best email…" name=EMAIL required data-required=true id=EMAIL><div style=position:absolute;left:-5000px aria-hidden=true><input type=text name=b_aebe5de1d2e40dccb3aeca491_3987dd4a1f tabindex=-1></div><input type=submit value=Subscribe name=subscribe id=mc-embedded-subscribe class="button btn bg-main-600 hover:bg-main-400 shadow w-full"></div><p id=mce-error-response style=display:none class="text-center mt-2 opacity-50 text-main-200">There was a problem, please try again.</p><p id=mce-success-response style=display:none class="text-center mt-2 opacity-50 text-main-200">Thanks mate, won't let ya down.</p></form></div></div></div><div class="card_layer card_effect card_soft_glare" style=pointer-events:none></div></div></div></div></div></div><script type=text/x-mathjax-config>
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script></div><script language=javascript type=text/javascript src=https://cosmiccoding.com.au/js/main.min.11673d10a962fb5205229607e62ea1d23424d35dad33db7bfe2a09a63d5409fa.js></script>
<script async defer src="https://www.googletagmanager.com/gtag/js?id=G-GRX6QE03YR"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-GRX6QE03YR")</script></div></body></html>