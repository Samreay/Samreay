<!doctype html><html lang=en-gb><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>Principle Component Analaysis Explained - Samuel Hinton</title><link rel=stylesheet href="https://cosmiccoding.com.au/css/main.min.b842bc4f8d358708f7b5666fe7108ed6474cd18dad036996cd6f85d3bb7b6a42.css" integrity="sha256-uEK8T401hwj3tWZv5xCO1kdM0Y2tA2mWzW+F07t7akI="><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel="shortcut icon" href=https://cosmiccoding.com.au/img/favicon.png type=image/x-icon></head><meta name=description content="What is PCA? How does it work? Is the math that bad? How do I use it? All answers lie within!"><meta name=robots content="noodp"><link rel=canonical href=https://cosmiccoding.com.au/tutorials/pca/><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://cosmiccoding.com.au/tutorials/pca/cover.png"><meta name=twitter:title content="Principle Component Analaysis Explained"><meta name=twitter:description content="What is PCA? How does it work? Is the math that bad? How do I use it? All answers lie within!"><meta property="og:title" content="Principle Component Analaysis Explained"><meta property="og:description" content="What is PCA? How does it work? Is the math that bad? How do I use it? All answers lie within!"><meta property="og:type" content="article"><meta property="og:url" content="https://cosmiccoding.com.au/tutorials/pca/"><meta property="og:image" content="https://cosmiccoding.com.au/tutorials/pca/cover.png"><meta property="article:section" content="tutorials"><meta property="article:published_time" content="2020-08-27T00:00:00+00:00"><meta property="article:modified_time" content="2020-08-27T00:00:00+00:00"><meta property="article:section" content="tutorial"><meta property="article:published_time" content="2020-08-27 00:00:00 +0000 UTC"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Principle Component Analaysis Explained","genre":"tutorial","url":"https:\/\/cosmiccoding.com.au\/tutorials\/pca\/","datePublished":"2020-08-27 00:00:00 \u002b0000 UTC","description":"What is PCA? How does it work? Is the math that bad? How do I use it? All answers lie within!","author":{"@type":"Person","name":""}}</script><body><div class="flex flex-col min-h-screen overflow-hidden"><header class="absolute w-full z-30"><div class="max-w-6xl mx-auto px-4 sm:px-6"><div class="flex items-center justify-between h-20"><div class="flex-shrink-0 mr-4"><a class=block href=/ aria-label="Samuel Hinton"><h2 class=logo>SRH</h2></a></div><nav class="hidden md:flex md:flex-grow"><ul class="flex flex-grow justify-end flex-wrap items-center"><li><a href=/#books class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Books</a></li><li><a href=/reviews class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Reviews</a></li><li><a href=/tutorials class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Tutorials</a></li><li><a href=/blogs class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Blog</a></li><li><a href=/#courses class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Courses</a></li><li><a href=/static/resume/HintonCV.pdf class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">CV</a></li></ul></nav><div class=md:hidden x-data="{ expanded: false }"><button class=hamburger :class="{ 'active': expanded }" @click.stop="expanded = !expanded" aria-controls=mobile-nav :aria-expanded=expanded>
<span class=sr-only>Menu</span><svg class="w-6 h-6 fill-current text-gray-300 hover:text-gray-200 transition duration-150 ease-in-out" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><rect y="4" width="24" height="2" rx="1"/><rect y="11" width="24" height="2" rx="1"/><rect y="18" width="24" height="2" rx="1"/></svg></button><nav id=mobile-nav class="absolute top-full z-20 left-0 w-full px-4 sm:px-6 overflow-hidden transition-all duration-300 ease-in-out" x-ref=mobileNav :style="expanded ? 'max-height: ' + $refs.mobileNav.scrollHeight + 'px; opacity: 1' : 'max-height: 0; opacity: .8'" @click.away="expanded = false" @keydown.escape.window="expanded = false" x-cloak><ul class="bg-gray-800 px-4 py-2"><li><a href=/#books class="flex text-gray-300 hover:text-gray-200 py-2">Books</a></li><li><a href=/reviews class="flex text-gray-300 hover:text-gray-200 py-2">Reviews</a></li><li><a href=/tutorials class="flex text-gray-300 hover:text-gray-200 py-2">Tutorials</a></li><li><a href=/blogs class="flex text-gray-300 hover:text-gray-200 py-2">Blog</a></li><li><a href=/#courses class="flex text-gray-300 hover:text-gray-200 py-2">Courses</a></li><li><a href=/static/resume/HintonCV.pdf class="flex text-gray-300 hover:text-gray-200 py-2">CV</a></li></ul></nav></div></div></div></header><div class=flex-grow><div id=post-container class="content content-wider blog-post relative"><div class="section-header blog"><h1 class=title>Principle Component Analaysis Explained</h1><p>6th August 2020</p><p>What is PCA? How does it work? Is the math that bad? How do I use it? All answers lie within!</p></div><div><ul class="grid gap-6 w-full md:grid-cols-2" style=list-style:none;padding-left:0><li><input type=radio id=show-code name=code-toggle value=show-code class="hidden peer" onchange=clickCheckbox(this) required checked>
<label for=show-code class="inline-flex justify-between items-center p-5 w-full text-gray-500 bg-gray-800 rounded-lg border border-gray-200 cursor-pointer peer-checked:border-2 peer-checked:border-main-300 peer-checked:text-white peer-checked:bg-gray-700 hover:text-gray-200 hover:bg-gray-700"><div class="block w-full"><div class="w-full text-center text-lg font-semibold">Show me everything!</div><div class="w-full text-center text-sm">Oh yeah, coding time.</div></div></label></li><li><input type=radio id=hide-code name=code-toggle value=hide-code class="hidden peer" onchange=clickCheckbox(this)>
<label for=hide-code class="inline-flex justify-between items-center p-5 w-full text-gray-500 bg-gray-800 rounded-lg border border-gray-200 cursor-pointer peer-checked:border-2 peer-checked:border-main-300 peer-checked:text-white peer-checked:bg-gray-700 hover:text-gray-200 hover:bg-gray-700"><div class="block w-full"><div class="w-full text-center text-lg font-semibold">Just the plots</div><div class="w-full text-center text-sm">Code is nasty.</div></div></label></li></ul></div><script>function clickCheckbox(e){e.id=="show-code"?document.getElementById("post-container").classList.remove("hide-code"):document.getElementById("post-container").classList.add("hide-code")}</script><p>In this small write up, we&rsquo;ll cover Principal Component Analysis from its mathematical routes, visual explanations and how to effectively utilise PCA using the sklearn library. We&rsquo;ll show PCA works by doing it manually in python without losing ourself in the mathematics!</p><h1 id=what-is-pca>What is PCA?</h1><p>PCA is a way of taking a dataset (a collection of points in some parameter/feature space) and determining what are the &ldquo;principal components&rdquo;. These principal components represent vectors that encapsulate the information in your dataset. By taking only a few of the most significant components, we can compress our dataset down into a lower dimensional space without throwing out critical information.</p><h2 id=a-visual-example>A visual example</h2><p>Let&rsquo;s generate simple 2D data to begin with:</p><p><div><figure class=rounded><picture><source srcset=/tutorials/pca/2020-08-27-PCA_files/2020-08-27-PCA_1_0_hu8bf8fb13f8373225b20ab75b40d279fc_133178_2776x1599_resize_q90_h2_box_3.webp width=2776 height=1599 type=image/webp><source srcset=/tutorials/pca/2020-08-27-PCA_files/2020-08-27-PCA_1_0_hu8bf8fb13f8373225b20ab75b40d279fc_133178_2776x1599_resize_q90_box_3.png width=2776 height=1599 type=image/png><img width=2776 height=1599 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>We have two dimensions of data, which are highly correlated. What if we wanted to remove this correlation? What if we wanted to get a single variable that encapsulates the information on where a data point lies along the bottom-left to top-right axis that we can so clearly see?</p><div class=width-76 markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>from</span> sklearn.decomposition <span style=color:#ff6ac1>import</span> PCA
</span></span><span style=display:flex><span>pca <span style=color:#ff6ac1>=</span> PCA(n_components<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>)
</span></span><span style=display:flex><span>data_pca <span style=color:#ff6ac1>=</span> pca<span style=color:#ff6ac1>.</span>fit_transform(data)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fig, axes <span style=color:#ff6ac1>=</span> plt<span style=color:#ff6ac1>.</span>subplots(ncols<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>)
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> ax, d, name <span style=color:#ff6ac1>in</span> <span style=color:#ff5c57>zip</span>(axes, [data, data_pca], [<span style=color:#5af78e>&#34;Raw&#34;</span>, <span style=color:#5af78e>&#34;PCA&#34;</span>]):
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>scatter(d[:, <span style=color:#ff9f43>0</span>], d[:, <span style=color:#ff9f43>1</span>])
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>axvline(<span style=color:#ff9f43>0</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>, ls<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;--&#34;</span>), ax<span style=color:#ff6ac1>.</span>axhline(<span style=color:#ff9f43>0</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>, ls<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;--&#34;</span>)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>set_xlabel(<span style=color:#5af78e>&#34;x&#34;</span>), ax<span style=color:#ff6ac1>.</span>set_ylabel(<span style=color:#5af78e>&#34;y&#34;</span>), ax<span style=color:#ff6ac1>.</span>set_title(name)
</span></span><span style=display:flex><span>fig<span style=color:#ff6ac1>.</span>tight_layout()
</span></span></code></pre></div></div><p><div><figure class=rounded><picture><source srcset=/tutorials/pca/2020-08-27-PCA_files/2020-08-27-PCA_3_0_hu4778a28ba6a195a67767a90638977ebf_186419_3113x1513_resize_q90_h2_box_3.webp width=3113 height=1513 type=image/webp><source srcset=/tutorials/pca/2020-08-27-PCA_files/2020-08-27-PCA_3_0_hu4778a28ba6a195a67767a90638977ebf_186419_3113x1513_resize_q90_box_3.png width=3113 height=1513 type=image/png><img width=3113 height=1513 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>We&rsquo;ll get into the details of what just happened in a tick, the takeaway here is that the PCA transformation has given us an x-axis which represents where the datapoint lies from bottom-left to top-right, and the y-axis is how far away from this bottom-left to top-right axis the data point is. We&rsquo;ll delve into this more soon, I promise!</p><h1 id=the-background-math>The background math</h1><p>To explain what is happening mathematically when we ask <code>scikit-learn</code> to do a PCA transformation, we&rsquo;ll do it ourselves manually.</p><ol><li>Get the covarariance of the dataset</li><li>Compute the eigenvectors and eigenvalues for the dataset.</li><li>Take the <code>N</code> largest eigenvalue/eigenvector pairs to reduce down to <code>N</code> dimensions</li><li>Use these <code>N</code> eigenvectors to construct a transformation matrix</li><li>Multiple this matrix onto your dataset to get a transformed output.</li></ol><p>Before we jump into this, lets pause for a second to talk about eigenvectors and eigenvalues. <a href=https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors>You can browse the wiki article on this if you want, but I also find wikipedia <strong>super</strong> dense when it comes to mathematical concepts</a>. At the simplest form, calculating the eigenvectors and values for our dataset is calculating a <em>linear</em> transformation. <strong>More intuitively, if you imagine stretching a 2D image in a given direction, the eigenvector corresponds to the direction the image is stretched, and the eigenvalue corresponds to how much it was stretched.</strong></p><p>So in a way, what we&rsquo;re trying to do with PCA is to squish our &ldquo;image&rdquo; (dataset) along specific dimensions to get a result at the end that looks uniform (ie compare the raw data vs PCA transformed data above). Or moreso it&rsquo;s the opposite; determining the eigenvalues and eigenvectors for your dataset is determining how you could go from a unitary symmetric dataset (like a square image) and asking in what ways (and by how much) you should stretch this unitary dataset out so that it looks like your actual data.</p><p>A bit more formally, if you have a square matrix $A$, vector $v$ and scalar $\lambda$ such that</p><p>$$ Av = \lambda v, $$</p><p>then $v$ represents our eigenvector and $\lambda$ the eigenvalue of $A$. We can manually solve for these by solving the linear equations to give $\text{det}(A - \lambda I)$, where det is the determinant and $I$ the identity matrix. <a href=https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#Three-dimensional_matrix_example>For a good runthrough of the math, see this example</a>. For a 3D matrix $A$, this will give us a cubic equation with three roots (solutions) for $\lambda$ - our three eigenvalues.</p><p>Let&rsquo;s code this up real quick.</p><div class=width-71 markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>manual_pca</span>(data, n_components):
</span></span><span style=display:flex><span>    <span style=color:#78787e># 1: Getting the mean and covariance</span>
</span></span><span style=display:flex><span>    mean, cov <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>mean(data, axis<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>), np<span style=color:#ff6ac1>.</span>cov(data<span style=color:#ff6ac1>.</span>T)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#78787e># 2: Compute eigenvectors and eigenvalues</span>
</span></span><span style=display:flex><span>    vals, vecs <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>linalg<span style=color:#ff6ac1>.</span>eig(cov)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#78787e># 3: Find the largest N eigenvalue indices</span>
</span></span><span style=display:flex><span>    s <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>argsort(vals)[::<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>][:n_components] 
</span></span><span style=display:flex><span>    <span style=color:#78787e># Sorts smallest to largest, so we reverse it and then grab the top</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#78787e># 4: Construct the transformation matrix</span>
</span></span><span style=display:flex><span>    eigenvals <span style=color:#ff6ac1>=</span> vals[s]
</span></span><span style=display:flex><span>    eigenvecs <span style=color:#ff6ac1>=</span> vecs[:, s]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#78787e># 5: Apply the transformation</span>
</span></span><span style=display:flex><span>    data_transformed <span style=color:#ff6ac1>=</span> (data <span style=color:#ff6ac1>-</span> mean) <span style=color:#ff6ac1>@</span> eigenvecs
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> data_transformed
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>data_transformed <span style=color:#ff6ac1>=</span> manual_pca(data, <span style=color:#ff9f43>2</span>)
</span></span></code></pre></div></div><p>Lets plot this and see how it turned out!</p><div class=width-79 markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>fig, axes <span style=color:#ff6ac1>=</span> plt<span style=color:#ff6ac1>.</span>subplots(ncols<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>)
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> ax, d, name <span style=color:#ff6ac1>in</span> <span style=color:#ff5c57>zip</span>(axes, [data, data_transformed], [<span style=color:#5af78e>&#34;Raw&#34;</span>, <span style=color:#5af78e>&#34;Transformed&#34;</span>]):
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>scatter(d[:, <span style=color:#ff9f43>0</span>], d[:, <span style=color:#ff9f43>1</span>])
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>axvline(<span style=color:#ff9f43>0</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>, ls<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;--&#34;</span>), ax<span style=color:#ff6ac1>.</span>axhline(<span style=color:#ff9f43>0</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>, ls<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;--&#34;</span>)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>set_xlabel(<span style=color:#5af78e>&#34;x&#34;</span>), ax<span style=color:#ff6ac1>.</span>set_ylabel(<span style=color:#5af78e>&#34;y&#34;</span>), ax<span style=color:#ff6ac1>.</span>set_title(name)
</span></span><span style=display:flex><span>fig<span style=color:#ff6ac1>.</span>tight_layout()
</span></span></code></pre></div></div><p><div><figure class=rounded><picture><source srcset=/tutorials/pca/2020-08-27-PCA_files/2020-08-27-PCA_8_0_hu53935710b9728fc66d9dabe14df2847a_192079_3113x1513_resize_q90_h2_box_3.webp width=3113 height=1513 type=image/webp><source srcset=/tutorials/pca/2020-08-27-PCA_files/2020-08-27-PCA_8_0_hu53935710b9728fc66d9dabe14df2847a_192079_3113x1513_resize_q90_box_3.png width=3113 height=1513 type=image/png><img width=3113 height=1513 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>You can see that this is effectively the same as the sklearn implementation!</p><h1 id=getting-the-most-out-of-sklearn>Getting the most out of sklearn</h1><p>So far you&rsquo;ve seen the <code>fit_transform</code> method, but we have at our hands <a href=https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html>a lot more functionality</a>. When we did the fit and transform manually, we had the eigenvectors and eigenvalues available. Sklearn has them under the hood as well:</p><div class="reduced-code width-27" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#78787e># Behold, the eigenvectors!</span>
</span></span><span style=display:flex><span>eigenvecs <span style=color:#ff6ac1>=</span> pca<span style=color:#ff6ac1>.</span>components_
</span></span><span style=display:flex><span>eigenvecs
</span></span></code></pre></div></div><pre><code>array([[ 0.70557211,  0.70863813],
       [ 0.70863813, -0.70557211]])
</code></pre><div class="reduced-code width-35" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#78787e># Behold, the eigenvalues!</span>
</span></span><span style=display:flex><span>eigenvals <span style=color:#ff6ac1>=</span> pca<span style=color:#ff6ac1>.</span>explained_variance_
</span></span><span style=display:flex><span>eigenvals
</span></span></code></pre></div></div><pre><code>array([1.94621132, 0.10024663])
</code></pre><div class="reduced-code width-26" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#78787e># Behold, the mean values!</span>
</span></span><span style=display:flex><span>mean <span style=color:#ff6ac1>=</span> pca<span style=color:#ff6ac1>.</span>mean_
</span></span><span style=display:flex><span>mean
</span></span></code></pre></div></div><pre><code>array([0.46853279, 0.96342773])
</code></pre><div class="expanded-code width-81" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#78787e># Behold, plotting them together with the raw data!</span>
</span></span><span style=display:flex><span>fig, axes <span style=color:#ff6ac1>=</span> plt<span style=color:#ff6ac1>.</span>subplots(ncols<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>)
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> ax, d, name <span style=color:#ff6ac1>in</span> <span style=color:#ff5c57>zip</span>(axes, [data, data_transformed], [<span style=color:#5af78e>&#34;Raw&#34;</span>, <span style=color:#5af78e>&#34;Transformed&#34;</span>]):
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>scatter(d[:, <span style=color:#ff9f43>0</span>], d[:, <span style=color:#ff9f43>1</span>])
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>scatter(d[<span style=color:#ff9f43>208</span>, <span style=color:#ff9f43>0</span>], d[<span style=color:#ff9f43>208</span>, <span style=color:#ff9f43>1</span>], s<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>50</span>, marker<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;*&#34;</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>axvline(<span style=color:#ff9f43>0</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>, ls<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;--&#34;</span>), ax<span style=color:#ff6ac1>.</span>axhline(<span style=color:#ff9f43>0</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>, ls<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;--&#34;</span>)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>set_xlabel(<span style=color:#5af78e>&#34;x&#34;</span>), ax<span style=color:#ff6ac1>.</span>set_ylabel(<span style=color:#5af78e>&#34;y&#34;</span>), ax<span style=color:#ff6ac1>.</span>set_title(name)
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>if</span> name <span style=color:#ff6ac1>==</span> <span style=color:#5af78e>&#34;Raw&#34;</span>:
</span></span><span style=display:flex><span>        ax<span style=color:#ff6ac1>.</span>arrow(<span style=color:#ff6ac1>*</span>mean, <span style=color:#ff6ac1>*</span>(eigenvecs[:, <span style=color:#ff9f43>0</span>] <span style=color:#ff6ac1>*</span> eigenvals[<span style=color:#ff9f43>0</span>]), color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;r&#34;</span>, width<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.05</span>)
</span></span><span style=display:flex><span>        ax<span style=color:#ff6ac1>.</span>arrow(<span style=color:#ff6ac1>*</span>mean, <span style=color:#ff6ac1>*</span>(eigenvecs[:, <span style=color:#ff9f43>1</span>] <span style=color:#ff6ac1>*</span> eigenvals[<span style=color:#ff9f43>1</span>]), color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;b&#34;</span>, width<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.05</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>else</span>:
</span></span><span style=display:flex><span>        ax<span style=color:#ff6ac1>.</span>arrow(<span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>0</span>, <span style=color:#ff6ac1>-</span>eigenvals[<span style=color:#ff9f43>0</span>], <span style=color:#ff9f43>0</span>, color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;r&#34;</span>, width<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.05</span>)
</span></span><span style=display:flex><span>        ax<span style=color:#ff6ac1>.</span>arrow(<span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>0</span>, <span style=color:#ff6ac1>-</span>eigenvals[<span style=color:#ff9f43>1</span>], color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;b&#34;</span>, width<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.05</span>)
</span></span><span style=display:flex><span>fig<span style=color:#ff6ac1>.</span>tight_layout()
</span></span></code></pre></div></div><p><div><figure class="img-main rounded"><picture><source srcset=/tutorials/pca/cover_hucc587d53a35cc44c1dd0824a6ee9401f_202473_3113x1513_resize_q90_h2_box_3.webp width=3113 height=1513 type=image/webp><source srcset=/tutorials/pca/cover_hucc587d53a35cc44c1dd0824a6ee9401f_202473_3113x1513_resize_q90_box_3.png width=3113 height=1513 type=image/png><img width=3113 height=1513 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>The largest eigenvector (scaled by its eigenvalue, as all eigenvectors are unit vectors) is shown in red, and the second vector is shown in blue. In the raw data, we center it on the dataset mean. On the right hand side, you can see how those two eigenvectors become the x and y axis. I&rsquo;ve selected one point in the dataset (index 208) and plotted it using a black star, to provide a concrete example of how an individual point gets translated. You can see that the black star lies close to the red eigenvector axis in the raw data, and hence its y-value on the right is close to zero.</p><p>As a note, the eigenvalues are called &ldquo;explained variance&rdquo; because the size of the eigenvalue is is given by how much of the variance in your data can be &ldquo;explained using&rdquo; (exists along) that eigenvector.</p><p>Another important thing to note here is that the eigenvalues are orthogonal to each other. This isn&rsquo;t a coincidence, its a central point to PCA and removing those correlations, but it might make you wonder if there are limits to PCA. What if your data isn&rsquo;t linearly seperable? Well, we can also do PCA using kernels which allow us to move beyond unit vectors. <a href=https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html>See this API</a> or <a href=https://scikit-learn.org/stable/auto_examples/decomposition/plot_kernel_pca.html>this example</a> for the details.</p><p>You can also use the <code>PCA</code> object from sklearn to do inverse transformation, which is handy. If you use PCA and do some model fitting to find an optimal value or solution in the reduced dimensionality, you can translate your answer back to the original dimensions. You can use the <code>score</code> function to see how well a dataset fits your PCA. Under the hood, its calculating the probability the input new data, if it was drawn from a normal distribution configured using the mean and covariance calculated from your eigenvalues and eigenvalues.</p><h1 id=summary>Summary</h1><p>PCA is a highly useful and efficient method of applying dimensional reduction to a dataset. Kernel based PCA methods allow for even more flexibility.</p><hr><p>For your convenience, here&rsquo;s the code in one block:</p><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>from</span> sklearn.decomposition <span style=color:#ff6ac1>import</span> PCA
</span></span><span style=display:flex><span>pca <span style=color:#ff6ac1>=</span> PCA(n_components<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>)
</span></span><span style=display:flex><span>data_pca <span style=color:#ff6ac1>=</span> pca<span style=color:#ff6ac1>.</span>fit_transform(data)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fig, axes <span style=color:#ff6ac1>=</span> plt<span style=color:#ff6ac1>.</span>subplots(ncols<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>)
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> ax, d, name <span style=color:#ff6ac1>in</span> <span style=color:#ff5c57>zip</span>(axes, [data, data_pca], [<span style=color:#5af78e>&#34;Raw&#34;</span>, <span style=color:#5af78e>&#34;PCA&#34;</span>]):
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>scatter(d[:, <span style=color:#ff9f43>0</span>], d[:, <span style=color:#ff9f43>1</span>])
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>axvline(<span style=color:#ff9f43>0</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>, ls<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;--&#34;</span>), ax<span style=color:#ff6ac1>.</span>axhline(<span style=color:#ff9f43>0</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>, ls<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;--&#34;</span>)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>set_xlabel(<span style=color:#5af78e>&#34;x&#34;</span>), ax<span style=color:#ff6ac1>.</span>set_ylabel(<span style=color:#5af78e>&#34;y&#34;</span>), ax<span style=color:#ff6ac1>.</span>set_title(name)
</span></span><span style=display:flex><span>fig<span style=color:#ff6ac1>.</span>tight_layout()
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>manual_pca</span>(data, n_components):
</span></span><span style=display:flex><span>    <span style=color:#78787e># 1: Getting the mean and covariance</span>
</span></span><span style=display:flex><span>    mean, cov <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>mean(data, axis<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>), np<span style=color:#ff6ac1>.</span>cov(data<span style=color:#ff6ac1>.</span>T)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#78787e># 2: Compute eigenvectors and eigenvalues</span>
</span></span><span style=display:flex><span>    vals, vecs <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>linalg<span style=color:#ff6ac1>.</span>eig(cov)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#78787e># 3: Find the largest N eigenvalue indices</span>
</span></span><span style=display:flex><span>    s <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>argsort(vals)[::<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>][:n_components] 
</span></span><span style=display:flex><span>    <span style=color:#78787e># Sorts smallest to largest, so we reverse it and then grab the top</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#78787e># 4: Construct the transformation matrix</span>
</span></span><span style=display:flex><span>    eigenvals <span style=color:#ff6ac1>=</span> vals[s]
</span></span><span style=display:flex><span>    eigenvecs <span style=color:#ff6ac1>=</span> vecs[:, s]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#78787e># 5: Apply the transformation</span>
</span></span><span style=display:flex><span>    data_transformed <span style=color:#ff6ac1>=</span> (data <span style=color:#ff6ac1>-</span> mean) <span style=color:#ff6ac1>@</span> eigenvecs
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> data_transformed
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>data_transformed <span style=color:#ff6ac1>=</span> manual_pca(data, <span style=color:#ff9f43>2</span>)
</span></span><span style=display:flex><span>fig, axes <span style=color:#ff6ac1>=</span> plt<span style=color:#ff6ac1>.</span>subplots(ncols<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>)
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> ax, d, name <span style=color:#ff6ac1>in</span> <span style=color:#ff5c57>zip</span>(axes, [data, data_transformed], [<span style=color:#5af78e>&#34;Raw&#34;</span>, <span style=color:#5af78e>&#34;Transformed&#34;</span>]):
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>scatter(d[:, <span style=color:#ff9f43>0</span>], d[:, <span style=color:#ff9f43>1</span>])
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>axvline(<span style=color:#ff9f43>0</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>, ls<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;--&#34;</span>), ax<span style=color:#ff6ac1>.</span>axhline(<span style=color:#ff9f43>0</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>, ls<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;--&#34;</span>)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>set_xlabel(<span style=color:#5af78e>&#34;x&#34;</span>), ax<span style=color:#ff6ac1>.</span>set_ylabel(<span style=color:#5af78e>&#34;y&#34;</span>), ax<span style=color:#ff6ac1>.</span>set_title(name)
</span></span><span style=display:flex><span>fig<span style=color:#ff6ac1>.</span>tight_layout()
</span></span><span style=display:flex><span><span style=color:#78787e># Behold, the eigenvectors!</span>
</span></span><span style=display:flex><span>eigenvecs <span style=color:#ff6ac1>=</span> pca<span style=color:#ff6ac1>.</span>components_
</span></span><span style=display:flex><span>eigenvecs
</span></span><span style=display:flex><span><span style=color:#78787e># Behold, the eigenvalues!</span>
</span></span><span style=display:flex><span>eigenvals <span style=color:#ff6ac1>=</span> pca<span style=color:#ff6ac1>.</span>explained_variance_
</span></span><span style=display:flex><span>eigenvals
</span></span><span style=display:flex><span><span style=color:#78787e># Behold, the mean values!</span>
</span></span><span style=display:flex><span>mean <span style=color:#ff6ac1>=</span> pca<span style=color:#ff6ac1>.</span>mean_
</span></span><span style=display:flex><span>mean
</span></span><span style=display:flex><span><span style=color:#78787e># Behold, plotting them together with the raw data!</span>
</span></span><span style=display:flex><span>fig, axes <span style=color:#ff6ac1>=</span> plt<span style=color:#ff6ac1>.</span>subplots(ncols<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>)
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> ax, d, name <span style=color:#ff6ac1>in</span> <span style=color:#ff5c57>zip</span>(axes, [data, data_transformed], [<span style=color:#5af78e>&#34;Raw&#34;</span>, <span style=color:#5af78e>&#34;Transformed&#34;</span>]):
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>scatter(d[:, <span style=color:#ff9f43>0</span>], d[:, <span style=color:#ff9f43>1</span>])
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>scatter(d[<span style=color:#ff9f43>208</span>, <span style=color:#ff9f43>0</span>], d[<span style=color:#ff9f43>208</span>, <span style=color:#ff9f43>1</span>], s<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>50</span>, marker<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;*&#34;</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>axvline(<span style=color:#ff9f43>0</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>, ls<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;--&#34;</span>), ax<span style=color:#ff6ac1>.</span>axhline(<span style=color:#ff9f43>0</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>, ls<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;--&#34;</span>)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>set_xlabel(<span style=color:#5af78e>&#34;x&#34;</span>), ax<span style=color:#ff6ac1>.</span>set_ylabel(<span style=color:#5af78e>&#34;y&#34;</span>), ax<span style=color:#ff6ac1>.</span>set_title(name)
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>if</span> name <span style=color:#ff6ac1>==</span> <span style=color:#5af78e>&#34;Raw&#34;</span>:
</span></span><span style=display:flex><span>        ax<span style=color:#ff6ac1>.</span>arrow(<span style=color:#ff6ac1>*</span>mean, <span style=color:#ff6ac1>*</span>(eigenvecs[:, <span style=color:#ff9f43>0</span>] <span style=color:#ff6ac1>*</span> eigenvals[<span style=color:#ff9f43>0</span>]), color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;r&#34;</span>, width<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.05</span>)
</span></span><span style=display:flex><span>        ax<span style=color:#ff6ac1>.</span>arrow(<span style=color:#ff6ac1>*</span>mean, <span style=color:#ff6ac1>*</span>(eigenvecs[:, <span style=color:#ff9f43>1</span>] <span style=color:#ff6ac1>*</span> eigenvals[<span style=color:#ff9f43>1</span>]), color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;b&#34;</span>, width<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.05</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>else</span>:
</span></span><span style=display:flex><span>        ax<span style=color:#ff6ac1>.</span>arrow(<span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>0</span>, <span style=color:#ff6ac1>-</span>eigenvals[<span style=color:#ff9f43>0</span>], <span style=color:#ff9f43>0</span>, color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;r&#34;</span>, width<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.05</span>)
</span></span><span style=display:flex><span>        ax<span style=color:#ff6ac1>.</span>arrow(<span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>0</span>, <span style=color:#ff6ac1>-</span>eigenvals[<span style=color:#ff9f43>1</span>], color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;b&#34;</span>, width<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.05</span>)
</span></span><span style=display:flex><span>fig<span style=color:#ff6ac1>.</span>tight_layout()
</span></span></code></pre></div><div class="relative max-w-xl mx-auto my-20"><div class="fancy_card horizontal"><div class=card_translator><div class="card_rotator tiny_rot card_layer"><div class="card_layer newsletter p-2"><div class="newsletter-inner h-full"><div class="relative h-full sib-form-container" id=sib-form-container><div class="mb-6 lg:mb-12 text-center"><h3 class="text-main-200 mb-2 lg:mb-8">Stay in the loop!</h3><p class="text-main-100 text-lg text-opacity-70">For new releases, exclusive giveaways, and reviews.</p></div><form action="https://Cosmiccoding.us5.list-manage.com/subscribe/post?u=aebe5de1d2e40dccb3aeca491&id=3987dd4a1f" method=post id=mc-embedded-subscribe-form name=mc-embedded-subscribe-form class="validate w-full" target=_blank novalidate><div><input type=email class="w-full appearance-none bg-main-600 mb-4 border border-main-600 bg-opacity-5 focus:border-main-300 rounded-sm px-4 py-3 text-main-100 placeholder-main-100 required" placeholder="Your best email…" aria-label="Your best email…" name=EMAIL required data-required=true id=EMAIL><div style=position:absolute;left:-5000px aria-hidden=true><input type=text name=b_aebe5de1d2e40dccb3aeca491_3987dd4a1f tabindex=-1></div><input type=submit value=Subscribe name=subscribe id=mc-embedded-subscribe class="button btn bg-main-600 hover:bg-main-400 shadow w-full"></div><p id=mce-error-response style=display:none class="text-center mt-2 opacity-50 text-main-200">There was a problem, please try again.</p><p id=mce-success-response style=display:none class="text-center mt-2 opacity-50 text-main-200">Thanks mate, won't let ya down.</p></form></div></div></div><div class="card_layer card_effect card_soft_glare" style=pointer-events:none></div></div></div></div></div></div><script type=text/x-mathjax-config>
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script></div><script language=javascript type=text/javascript src=https://cosmiccoding.com.au/js/main.min.11673d10a962fb5205229607e62ea1d23424d35dad33db7bfe2a09a63d5409fa.js></script>
<script async defer src="https://www.googletagmanager.com/gtag/js?id=UA-72691106-1"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","UA-72691106-1")</script></div></body></html>