<!doctype html><html lang=en-gb><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>Linear Regression Regularization Explained - Samuel Hinton</title><link rel=stylesheet href="https://cosmiccoding.com.au/css/main.min.b842bc4f8d358708f7b5666fe7108ed6474cd18dad036996cd6f85d3bb7b6a42.css" integrity="sha256-uEK8T401hwj3tWZv5xCO1kdM0Y2tA2mWzW+F07t7akI="><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel="shortcut icon" href=https://cosmiccoding.com.au/img/favicon.png type=image/x-icon></head><meta name=description content="Why do we regularize our linear models? When do we turn this on or off?"><meta name=robots content="noodp"><link rel=canonical href=https://cosmiccoding.com.au/tutorials/linear_regression_regularisation/><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://cosmiccoding.com.au/tutorials/linear_regression_regularisation/cover.png"><meta name=twitter:title content="Linear Regression Regularization Explained"><meta name=twitter:description content="Why do we regularize our linear models? When do we turn this on or off?"><meta property="og:title" content="Linear Regression Regularization Explained"><meta property="og:description" content="Why do we regularize our linear models? When do we turn this on or off?"><meta property="og:type" content="article"><meta property="og:url" content="https://cosmiccoding.com.au/tutorials/linear_regression_regularisation/"><meta property="og:image" content="https://cosmiccoding.com.au/tutorials/linear_regression_regularisation/cover.png"><meta property="article:section" content="tutorials"><meta property="article:published_time" content="2020-12-27T00:00:00+00:00"><meta property="article:modified_time" content="2020-12-27T00:00:00+00:00"><meta property="article:section" content="tutorial"><meta property="article:published_time" content="2020-12-27 00:00:00 +0000 UTC"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Linear Regression Regularization Explained","genre":"tutorial","url":"https:\/\/cosmiccoding.com.au\/tutorials\/linear_regression_regularisation\/","datePublished":"2020-12-27 00:00:00 \u002b0000 UTC","description":"Why do we regularize our linear models? When do we turn this on or off?","author":{"@type":"Person","name":""}}</script><body><div class="flex flex-col min-h-screen overflow-hidden"><header class="absolute w-full z-30"><div class="max-w-6xl mx-auto px-4 sm:px-6"><div class="flex items-center justify-between h-20"><div class="flex-shrink-0 mr-4"><a class=block href=/ aria-label="Samuel Hinton"><h2 class=logo>SRH</h2></a></div><nav class="hidden md:flex md:flex-grow"><ul class="flex flex-grow justify-end flex-wrap items-center"><li><a href=/#books class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Books</a></li><li><a href=/reviews class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Reviews</a></li><li><a href=/tutorials class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Tutorials</a></li><li><a href=/blogs class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Blog</a></li><li><a href=/#courses class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Courses</a></li><li><a href=/static/resume/HintonCV.pdf class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">CV</a></li></ul></nav><div class=md:hidden x-data="{ expanded: false }"><button class=hamburger :class="{ 'active': expanded }" @click.stop="expanded = !expanded" aria-controls=mobile-nav :aria-expanded=expanded>
<span class=sr-only>Menu</span><svg class="w-6 h-6 fill-current text-gray-300 hover:text-gray-200 transition duration-150 ease-in-out" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><rect y="4" width="24" height="2" rx="1"/><rect y="11" width="24" height="2" rx="1"/><rect y="18" width="24" height="2" rx="1"/></svg></button><nav id=mobile-nav class="absolute top-full z-20 left-0 w-full px-4 sm:px-6 overflow-hidden transition-all duration-300 ease-in-out" x-ref=mobileNav :style="expanded ? 'max-height: ' + $refs.mobileNav.scrollHeight + 'px; opacity: 1' : 'max-height: 0; opacity: .8'" @click.away="expanded = false" @keydown.escape.window="expanded = false" x-cloak><ul class="bg-gray-800 px-4 py-2"><li><a href=/#books class="flex text-gray-300 hover:text-gray-200 py-2">Books</a></li><li><a href=/reviews class="flex text-gray-300 hover:text-gray-200 py-2">Reviews</a></li><li><a href=/tutorials class="flex text-gray-300 hover:text-gray-200 py-2">Tutorials</a></li><li><a href=/blogs class="flex text-gray-300 hover:text-gray-200 py-2">Blog</a></li><li><a href=/#courses class="flex text-gray-300 hover:text-gray-200 py-2">Courses</a></li><li><a href=/static/resume/HintonCV.pdf class="flex text-gray-300 hover:text-gray-200 py-2">CV</a></li></ul></nav></div></div></div></header><div class=flex-grow><div id=post-container class="content content-wider blog-post relative"><div class="section-header blog"><h1 class=title>Linear Regression Regularization Explained</h1><p>6th December 2020</p><p>Why do we regularize our linear models? When do we turn this on or off?</p></div><div><ul class="grid gap-6 w-full md:grid-cols-2" style=list-style:none;padding-left:0><li><input type=radio id=show-code name=code-toggle value=show-code class="hidden peer" onchange=clickCheckbox(this) required checked>
<label for=show-code class="inline-flex justify-between items-center p-5 w-full text-gray-500 bg-gray-800 rounded-lg border border-gray-200 cursor-pointer peer-checked:border-2 peer-checked:border-main-300 peer-checked:text-white peer-checked:bg-gray-700 hover:text-gray-200 hover:bg-gray-700"><div class="block w-full"><div class="w-full text-center text-lg font-semibold">Show me everything!</div><div class="w-full text-center text-sm">Oh yeah, coding time.</div></div></label></li><li><input type=radio id=hide-code name=code-toggle value=hide-code class="hidden peer" onchange=clickCheckbox(this)>
<label for=hide-code class="inline-flex justify-between items-center p-5 w-full text-gray-500 bg-gray-800 rounded-lg border border-gray-200 cursor-pointer peer-checked:border-2 peer-checked:border-main-300 peer-checked:text-white peer-checked:bg-gray-700 hover:text-gray-200 hover:bg-gray-700"><div class="block w-full"><div class="w-full text-center text-lg font-semibold">Just the plots</div><div class="w-full text-center text-sm">Code is nasty.</div></div></label></li></ul></div><script>function clickCheckbox(e){e.id=="show-code"?document.getElementById("post-container").classList.remove("hide-code"):document.getElementById("post-container").classList.add("hide-code")}</script><p>In this small write up, I&rsquo;ll be trying to fill a bit of a void I&rsquo;ve seen online. That is, there are plenty of definitions of lasso, ridge, and elastic regularization, but most of them aren&rsquo;t accompanied by useful examples showing when these terms become critically important! To address this, we&rsquo;re going to look at regularization using three different use cases:</p><ol><li>A perfect dataset! White noise on top of a direct linear relationship.</li><li>A dataset with confounding variables. Not all of our features will be useful.</li><li>A dataset with multicolinearity. Super high correlation in some of our features will cause issues.</li></ol><h1 id=regularization-recap>Regularization Recap</h1><p>There a few different types commonly employed.</p><h2 id=a-normal-linear-model>A normal linear model</h2><p>Without regularization, a stock standard linear regression is simply going to fit a model of</p><p>$$ y = \beta X, $$</p><p>where $X$ are our features and $\beta$ an array of coefficients, one for each feature. For example, a 1D model will have $y=\beta_0 x_0$. A 2D model will have $y = \beta_0 x_0 + \beta_1 x_1$. To fit the model, we optimise our value of $\beta$ to minimise the difference between the predicted $y$ value and the observed data points $y_i$, something like this:</p><p>$$ f = \sum_i (y_i - \beta X_i), $$</p><p>where the $i$ simple represents counting over each datapoint we have.</p><h2 id=ridge-regression>Ridge Regression</h2><p>Ridge regression adds what regularization via adding the L2 penalty to the optimisation function. The amount of penalty is controlled by the $\lambda$ parameter.</p><p>$$ f = \sum_i (y_i - \beta X_i) + \lambda \sum \beta^2 $$</p><p>If we minimise the above function, we now want the sum of all the $\beta$ values to be as small as possible, with a quadratic pull.</p><h2 id=lasso-regression>Lasso Regression</h2><p>Lasso Regression adds a penalty just like ridge regression, but we add whats now called the L1 penalty. Again, the amount is controlled by $\lambda$.</p><p>$$ f = \sum_i (y_i - \beta X_i) + \lambda \sum |\beta| $$</p><p>Using an absolute value instead of the square from ridge means that Lasso regression can and will pull $\beta$ terms all the way to zero if they aren&rsquo;t adding to the model.</p><p><em>To easily remember, Lasso is linear (L1). Ridge is a &ldquo;runaway&rdquo; quadratic (L2 like $L^2$).</em></p><h2 id=elasticnet>ElasticNet</h2><p>This is the easiest of all. Add both terms. Control the values of the two $\lambda$s to tweak behaviour.</p><p>$$ f = \sum_i (y_i - \beta X_i) + \lambda_1 \sum |\beta| + \lambda_2 \sum \beta^2 $$</p><h1 id=what-does-regularization-do-on-perfect-data>What does regularization do on perfect data?</h1><p>To investigate this, we&rsquo;re going to run a few different models in each of the sections. They are all from the amazing <code>scikit-learn</code> library too: <code>LinearRegression</code> (no regularization), <code>Lasso</code>, <code>Ridge</code> and <code>ElasticNet</code>.</p><p>But first, let&rsquo;s not get ahead of ourselves. Let&rsquo;s make some nice data with a super simple gradient of one. Yup, the simplest linear model possible.</p><div class=width-63 markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>import</span> numpy <span style=color:#ff6ac1>as</span> np
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> scipy.stats <span style=color:#ff6ac1>import</span> norm
</span></span><span style=display:flex><span><span style=color:#ff6ac1>import</span> matplotlib.pyplot <span style=color:#ff6ac1>as</span> plt
</span></span><span style=display:flex><span>np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>seed(<span style=color:#ff9f43>12</span>)
</span></span><span style=display:flex><span>n <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>30</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>xs <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>linspace(<span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>5</span>, n)
</span></span><span style=display:flex><span>es <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>ones(n)
</span></span><span style=display:flex><span>ys <span style=color:#ff6ac1>=</span> xs <span style=color:#ff6ac1>+</span> es <span style=color:#ff6ac1>*</span> norm<span style=color:#ff6ac1>.</span>rvs(size<span style=color:#ff6ac1>=</span>n)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>xs <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>atleast_2d(xs)<span style=color:#ff6ac1>.</span>T <span style=color:#78787e># As sklearn expects a 2D feature list</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>errorbar(xs, ys, yerr<span style=color:#ff6ac1>=</span>es, fmt<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;o&#34;</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Data&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.7</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot([<span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>5</span>], [<span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>5</span>], label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Truth&#34;</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend();
</span></span></code></pre></div></div><p><div><figure class=rounded><picture><source srcset=/tutorials/linear_regression_regularisation/2020-12-27-Linear_Regression_Regularisation_files/2020-12-27-Linear_Regression_Regularisation_3_0_hued36391645122fb7fa251a928a552f52_114310_1920x0_resize_q90_h2_box_3.webp width=1920 height=1000 type=image/webp><source srcset=/tutorials/linear_regression_regularisation/2020-12-27-Linear_Regression_Regularisation_files/2020-12-27-Linear_Regression_Regularisation_3_0.png width=4012 height=2090 type=image/png><img width=4012 height=2090 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>Okay, so lets run our models over it now!</p><div class=width-75 markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>from</span> sklearn.linear_model <span style=color:#ff6ac1>import</span> LinearRegression, Lasso, Ridge, ElasticNet
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>errorbar(xs, ys, yerr<span style=color:#ff6ac1>=</span>es, fmt<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;o&#34;</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Data&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.7</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot([<span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>5</span>], [<span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>5</span>], label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Truth&#34;</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>, ls<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;--&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> m <span style=color:#ff6ac1>in</span> [LinearRegression(), Lasso(), Ridge(), ElasticNet()]:
</span></span><span style=display:flex><span>    m<span style=color:#ff6ac1>.</span>fit(xs, ys)
</span></span><span style=display:flex><span>    plt<span style=color:#ff6ac1>.</span>plot(xs, m<span style=color:#ff6ac1>.</span>predict(xs), label<span style=color:#ff6ac1>=</span>m<span style=color:#ff6ac1>.</span>__class__<span style=color:#ff6ac1>.</span>__name__)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend(ncol<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>);
</span></span></code></pre></div></div><p><div><figure class=rounded><picture><source srcset=/tutorials/linear_regression_regularisation/2020-12-27-Linear_Regression_Regularisation_files/2020-12-27-Linear_Regression_Regularisation_5_0_hu439c37011d3703a084774d6243d94413_225231_1920x0_resize_q90_h2_box_3.webp width=1920 height=1000 type=image/webp><source srcset=/tutorials/linear_regression_regularisation/2020-12-27-Linear_Regression_Regularisation_files/2020-12-27-Linear_Regression_Regularisation_5_0.png width=4012 height=2090 type=image/png><img width=4012 height=2090 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>Unsurprisingly, with our perfect data, it seems like the regularization isn&rsquo;t helping us at all. But we can clearly see the impact of the penalties bringing our singular $
\beta$ value down.</p><h1 id=regularization-on-confounded-data>Regularization on confounded data</h1><p>No more simple models for us now, lets preserve a nice simple relationship, but add in a bunch of useless variables. Here we&rsquo;ll mostly use the first feature, a hint of the second, and none of the other eight!</p><div class="expanded-code width-89" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#78787e># 50 observations of 20 features, assuming 0.2 uncertainty on observations</span>
</span></span><span style=display:flex><span>np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>seed(<span style=color:#ff9f43>1</span>)
</span></span><span style=display:flex><span>xs <span style=color:#ff6ac1>=</span> norm<span style=color:#ff6ac1>.</span>rvs(scale<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>, size<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>50</span>, <span style=color:#ff9f43>20</span>))
</span></span><span style=display:flex><span>xs <span style=color:#ff6ac1>=</span> xs[xs[:, <span style=color:#ff9f43>0</span>]<span style=color:#ff6ac1>.</span>argsort()] <span style=color:#78787e># Sort array by first dimension for easier plotting</span>
</span></span><span style=display:flex><span>es <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>0.2</span> <span style=color:#ff6ac1>*</span> np<span style=color:#ff6ac1>.</span>ones(shape<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>50</span>)
</span></span><span style=display:flex><span>ys <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>1</span> <span style=color:#ff6ac1>*</span> xs[:, <span style=color:#ff9f43>0</span>] <span style=color:#ff6ac1>+</span> <span style=color:#ff9f43>0.1</span> <span style=color:#ff6ac1>*</span> xs[:, <span style=color:#ff9f43>1</span>] <span style=color:#ff6ac1>+</span> es <span style=color:#ff6ac1>*</span> norm<span style=color:#ff6ac1>.</span>rvs(size<span style=color:#ff6ac1>=</span>es<span style=color:#ff6ac1>.</span>shape)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>errorbar(xs[:, <span style=color:#ff9f43>0</span>], ys, yerr<span style=color:#ff6ac1>=</span>es, fmt<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;o&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.7</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Data (first dimension only)&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend();
</span></span></code></pre></div></div><p><div><figure class=rounded><picture><source srcset=/tutorials/linear_regression_regularisation/2020-12-27-Linear_Regression_Regularisation_files/2020-12-27-Linear_Regression_Regularisation_8_0_hu7daf8c9d867e4d3a1b3242a6d265716b_109215_1920x0_resize_q90_h2_box_3.webp width=1920 height=1000 type=image/webp><source srcset=/tutorials/linear_regression_regularisation/2020-12-27-Linear_Regression_Regularisation_files/2020-12-27-Linear_Regression_Regularisation_8_0.png width=4012 height=2090 type=image/png><img width=4012 height=2090 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>So lets fit some more models, and note that even though X is now 20-dimensional, because plotting is hard, Im just going to show $X_0$ primarily, just like above.</p><p>Lets now fit this, just like before, but making sure to save out the vaue of the coefficients so that we can see how the different models are treating their $\beta$ values.</p><div class=width-67 markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>errorbar(xs[:, <span style=color:#ff9f43>0</span>], ys, yerr<span style=color:#ff6ac1>=</span>es, fmt<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;o&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.7</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Data&#34;</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>beta_dict <span style=color:#ff6ac1>=</span> {}
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> m <span style=color:#ff6ac1>in</span> [LinearRegression(), Lasso(), Ridge(), ElasticNet()]:
</span></span><span style=display:flex><span>    m<span style=color:#ff6ac1>.</span>fit(xs, ys)
</span></span><span style=display:flex><span>    predict <span style=color:#ff6ac1>=</span> m<span style=color:#ff6ac1>.</span>predict(xs)
</span></span><span style=display:flex><span>    name <span style=color:#ff6ac1>=</span> m<span style=color:#ff6ac1>.</span>__class__<span style=color:#ff6ac1>.</span>__name__
</span></span><span style=display:flex><span>    beta_dict[name] <span style=color:#ff6ac1>=</span> m<span style=color:#ff6ac1>.</span>coef_
</span></span><span style=display:flex><span>    plt<span style=color:#ff6ac1>.</span>plot(xs[:, <span style=color:#ff9f43>0</span>], predict, label<span style=color:#ff6ac1>=</span>name)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend(ncol<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>);
</span></span></code></pre></div></div><p><div><figure class=rounded><picture><source srcset=/tutorials/linear_regression_regularisation/2020-12-27-Linear_Regression_Regularisation_files/2020-12-27-Linear_Regression_Regularisation_11_0_hu2874c772d2b1d1ac7d5f1042ed2d2c45_269788_1920x0_resize_q90_h2_box_3.webp width=1920 height=1000 type=image/webp><source srcset=/tutorials/linear_regression_regularisation/2020-12-27-Linear_Regression_Regularisation_files/2020-12-27-Linear_Regression_Regularisation_11_0.png width=4012 height=2090 type=image/png><img width=4012 height=2090 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>Another busy plot, but the take away here is that the models with regularization, are in general, <em>smoother</em> than the other models. That is, they are less prone to overfitting. You can see, for example, the ElasticNet and Lasso regularization models (with the two strongest penalties) show that the model that comes out at the end is essentially only dependent on our first feature (hence the straight line), with other smaller effects marginalised in the model fitting.</p><p>I saved the coefficient values out into <code>beta_dict</code> so we can see this plotted now:</p><div class="expanded-code width-83" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>size<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>20</span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> name, betas <span style=color:#ff6ac1>in</span> beta_dict<span style=color:#ff6ac1>.</span>items():
</span></span><span style=display:flex><span>    plt<span style=color:#ff6ac1>.</span>scatter(np<span style=color:#ff6ac1>.</span>arange(betas<span style=color:#ff6ac1>.</span>shape[<span style=color:#ff9f43>0</span>]), betas, label<span style=color:#ff6ac1>=</span>name, s<span style=color:#ff6ac1>=</span>size, zorder<span style=color:#ff6ac1>=-</span>size)
</span></span><span style=display:flex><span>    size <span style=color:#ff6ac1>*=</span> <span style=color:#ff9f43>2</span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>axhline(<span style=color:#ff9f43>0</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;k&#34;</span>), plt<span style=color:#ff6ac1>.</span>ylabel(<span style=color:#5af78e>r</span><span style=color:#5af78e>&#34;$\beta$&#34;</span>), plt<span style=color:#ff6ac1>.</span>xlabel(<span style=color:#5af78e>&#34;Feature number&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend();
</span></span></code></pre></div></div><p><div><figure class="img-main rounded"><picture><source srcset=/tutorials/linear_regression_regularisation/cover_hud0e717d633c1c4e623ad512f348449ee_192712_1920x0_resize_q90_h2_box_3.webp width=1920 height=1027 type=image/webp><source srcset=/tutorials/linear_regression_regularisation/cover.png width=4190 height=2241 type=image/png><img width=4190 height=2241 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>We can see how effectively methods like Lasso regression pull down the importance of features that aren&rsquo;t significant and try and stop overfitting, leading to a more generalised model at the end. When training these models, the $\lambda$ parameter would need to be tuned to try and make sure we are not losing information we want. For example, in the above plot, we might have too much penalty in the ElasticNet and Lasso models (as they have a 0 value for the second feature, instead of the 0.1), whilst the Ridge regression - even though it recovers that second feature at the right value - fails to constrain all the other superflous features!</p><h1 id=regularization-on-correlated-data>Regularization on correlated data</h1><p>When our input features are very highly correlated with each other, we can start to run into issues. Whether its perfect correlation (accidentally input temperature in both Farenheit and Celcius) or natural correlations (difficulty of getting up in the morning vs temperature out of the bed), these things can be an issue. So, in normal fashion, lets make us some issues.</p><div class="reduced-code width-49" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>from</span> scipy.stats <span style=color:#ff6ac1>import</span> multivariate_normal <span style=color:#ff6ac1>as</span> mn
</span></span><span style=display:flex><span>n <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>50</span>
</span></span><span style=display:flex><span>es <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>0.1</span>
</span></span><span style=display:flex><span>x0 <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>linspace(<span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>10</span>, n)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Now some highly correlated variables!</span>
</span></span><span style=display:flex><span>x1 <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>0.5</span> <span style=color:#ff6ac1>*</span> x0 <span style=color:#ff6ac1>+</span> norm<span style=color:#ff6ac1>.</span>rvs(scale<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.01</span>, size<span style=color:#ff6ac1>=</span>n)
</span></span><span style=display:flex><span>x2 <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>0.5</span> <span style=color:#ff6ac1>*</span> x1 <span style=color:#ff6ac1>+</span> norm<span style=color:#ff6ac1>.</span>rvs(scale<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>5</span>, size<span style=color:#ff6ac1>=</span>n)
</span></span><span style=display:flex><span>xs <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>vstack((x0, x1, x2))<span style=color:#ff6ac1>.</span>T
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ys <span style=color:#ff6ac1>=</span> x0 <span style=color:#ff6ac1>+</span> es <span style=color:#ff6ac1>*</span> norm<span style=color:#ff6ac1>.</span>rvs(size<span style=color:#ff6ac1>=</span>n)
</span></span></code></pre></div></div><p>Let&rsquo;s check the variance inflation factor. If this is above ten is generally means high correlation.</p><div class=width-74 markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>from</span> statsmodels.stats.outliers_influence <span style=color:#ff6ac1>import</span> variance_inflation_factor
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> index <span style=color:#ff6ac1>in</span> <span style=color:#ff5c57>range</span>(xs<span style=color:#ff6ac1>.</span>shape[<span style=color:#ff9f43>1</span>]):
</span></span><span style=display:flex><span>    <span style=color:#ff5c57>print</span>(index, variance_inflation_factor(xs, index))
</span></span></code></pre></div></div><pre><code>0 74563.33713200882
1 74559.06668872922
2 1.0190891029253206
</code></pre><p>Thats a lot of correlation. We shall proceed with the normal modelling fitting, which you can see below. Same thing as normal, except now instead of just plotting the predictions in our one-dimensional view, we also plot the distribution of $\beta_0$ values (just so we can see how important each model rated the first feature).</p><div class=width-78 markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>do_monte_carlo_fit</span>(xs, ys, es, model, samps<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>100</span>):
</span></span><span style=display:flex><span>    <span style=color:#5af78e>&#34;&#34;&#34; A small function that perturbs the data and refits a model.
</span></span></span><span style=display:flex><span><span style=color:#5af78e>    Returns the mean prediction and also all the beta (coefficient) values.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    predictions <span style=color:#ff6ac1>=</span> []
</span></span><span style=display:flex><span>    betas <span style=color:#ff6ac1>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>for</span> i <span style=color:#ff6ac1>in</span> <span style=color:#ff5c57>range</span>(samps):
</span></span><span style=display:flex><span>        np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>seed(i)
</span></span><span style=display:flex><span>        m<span style=color:#ff6ac1>.</span>fit(xs, ys <span style=color:#ff6ac1>+</span> es <span style=color:#ff6ac1>*</span> norm<span style=color:#ff6ac1>.</span>rvs(size<span style=color:#ff6ac1>=</span>ys<span style=color:#ff6ac1>.</span>shape))
</span></span><span style=display:flex><span>        betas<span style=color:#ff6ac1>.</span>append(m<span style=color:#ff6ac1>.</span>coef_)
</span></span><span style=display:flex><span>        predictions<span style=color:#ff6ac1>.</span>append(m<span style=color:#ff6ac1>.</span>predict(xs))
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> np<span style=color:#ff6ac1>.</span>mean(predictions, axis<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>), np<span style=color:#ff6ac1>.</span>array(betas)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Set up the figure and bins for the beta histogram</span>
</span></span><span style=display:flex><span>fig, axes <span style=color:#ff6ac1>=</span> plt<span style=color:#ff6ac1>.</span>subplots(nrows<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>3</span>, figsize<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>8</span>, <span style=color:#ff9f43>8</span>))
</span></span><span style=display:flex><span>axes[<span style=color:#ff9f43>0</span>]<span style=color:#ff6ac1>.</span>errorbar(xs[:, <span style=color:#ff9f43>0</span>], ys, yerr<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.1</span>, fmt<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;o&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.7</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Data&#34;</span>);
</span></span><span style=display:flex><span>bins <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>linspace(<span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>1.2</span>, <span style=color:#ff9f43>200</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> m <span style=color:#ff6ac1>in</span> [LinearRegression(), Lasso(), Ridge(), ElasticNet()]:
</span></span><span style=display:flex><span>    <span style=color:#78787e># We get the mean prediction and all beta values</span>
</span></span><span style=display:flex><span>    mean, betas <span style=color:#ff6ac1>=</span> do_monte_carlo_fit(xs, ys, es, m)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#78787e># And plot it all out</span>
</span></span><span style=display:flex><span>    line, <span style=color:#ff6ac1>*</span>_ <span style=color:#ff6ac1>=</span> axes[<span style=color:#ff9f43>0</span>]<span style=color:#ff6ac1>.</span>plot(xs[:, <span style=color:#ff9f43>0</span>], mean, label<span style=color:#ff6ac1>=</span>m<span style=color:#ff6ac1>.</span>__class__<span style=color:#ff6ac1>.</span>__name__)
</span></span><span style=display:flex><span>    axes[<span style=color:#ff9f43>1</span>]<span style=color:#ff6ac1>.</span>hist(betas[:, <span style=color:#ff9f43>0</span>], color<span style=color:#ff6ac1>=</span>line<span style=color:#ff6ac1>.</span>get_color(), bins<span style=color:#ff6ac1>=</span>bins)
</span></span><span style=display:flex><span>    axes[<span style=color:#ff9f43>2</span>]<span style=color:#ff6ac1>.</span>hist(betas[:, <span style=color:#ff9f43>1</span>], color<span style=color:#ff6ac1>=</span>line<span style=color:#ff6ac1>.</span>get_color(), bins<span style=color:#ff6ac1>=</span>bins)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>axes[<span style=color:#ff9f43>0</span>]<span style=color:#ff6ac1>.</span>legend(loc<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>)
</span></span><span style=display:flex><span>axes[<span style=color:#ff9f43>0</span>]<span style=color:#ff6ac1>.</span>set_xlabel(<span style=color:#5af78e>r</span><span style=color:#5af78e>&#34;$X_0$&#34;</span>)
</span></span><span style=display:flex><span>axes[<span style=color:#ff9f43>1</span>]<span style=color:#ff6ac1>.</span>set_xlabel(<span style=color:#5af78e>r</span><span style=color:#5af78e>&#34;$\beta_0$&#34;</span>)
</span></span><span style=display:flex><span>axes[<span style=color:#ff9f43>2</span>]<span style=color:#ff6ac1>.</span>set_xlabel(<span style=color:#5af78e>r</span><span style=color:#5af78e>&#34;$\beta_1$&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>tight_layout();
</span></span></code></pre></div></div><p><div><figure class="img-large rounded"><picture><source srcset=/tutorials/linear_regression_regularisation/2020-12-27-Linear_Regression_Regularisation_files/2020-12-27-Linear_Regression_Regularisation_20_0_hu77a92fe7cef3c7a5e1095e51728c3da9_331825_1920x0_resize_q90_h2_box_3.webp width=1920 height=1919 type=image/webp><source srcset=/tutorials/linear_regression_regularisation/2020-12-27-Linear_Regression_Regularisation_files/2020-12-27-Linear_Regression_Regularisation_20_0.png width=4667 height=4665 type=image/png><img width=4667 height=4665 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>The takeaway here is that, <strong>even though the predictions look to be pretty similar</strong>, the methods which include regularization have much more consistent values for their $\beta$ values. To put this another way, the stock standard LinearRegression model, if we perturb our data, could have wildly changing coefficients. Because our input features are highly correlated, sometimes $\beta_0$ might be low, but it will be compensated for by $\beta_1$ being higher. This is why the top plot looks fine, but the red distribution in the histograms is spread out. Regularization will put a stop to that, as it will effectively select whatever feature fits the data with the lowest possible $\beta$ values, allowing (in this case), for a far better localisation of $\beta$.</p><p>This can become important in various machine learning pipelines. In some data sets, we have a huge number of potential features, and often we select a few of these features as ones of interest, and create models of those. These features can be extracted often by simple methods like checking the correlations with the dependent variable, but sometimes simple linear models are fit, and any with significant $\beta$ values are selected to continue down the pipeline. A correlation approach would pass all our incredibly correlated features down the line. Normal linear regression would constantly change what it is deciding to send down the line. But a nicely regularized linear regression will pass on features correlated with the dependent variable, whilst removing independent variables which are very highly correlated with each other.</p><p>Caveat: only when it all works nicely.</p><p>Caveat: reality rarely works nicely.</p><h1 id=summary>Summary</h1><p>To recap:</p><ol><li>Lasso regression, adds a penalty (L1) thats the sum of the absolute value of the coefficients.</li><li>Ridge regression, adds a penalty (L2) thats the sum of the squares of the coefficients.</li><li>ElasticNet adds both penalties.</li><li>When you have lots of features but only some are important, you can tune your regularization so that your model only uses useful features.</li><li>When you have highly correlated features, regularization can be used to ensure more consistent model behaviour and feature selection.</li></ol><p>Regularization on other models is, unsurprisingly, even more complicated.</p><hr><p>For your convenience, here&rsquo;s the code in one block:</p><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>import</span> numpy <span style=color:#ff6ac1>as</span> np
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> scipy.stats <span style=color:#ff6ac1>import</span> norm
</span></span><span style=display:flex><span><span style=color:#ff6ac1>import</span> matplotlib.pyplot <span style=color:#ff6ac1>as</span> plt
</span></span><span style=display:flex><span>np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>seed(<span style=color:#ff9f43>12</span>)
</span></span><span style=display:flex><span>n <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>30</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>xs <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>linspace(<span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>5</span>, n)
</span></span><span style=display:flex><span>es <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>ones(n)
</span></span><span style=display:flex><span>ys <span style=color:#ff6ac1>=</span> xs <span style=color:#ff6ac1>+</span> es <span style=color:#ff6ac1>*</span> norm<span style=color:#ff6ac1>.</span>rvs(size<span style=color:#ff6ac1>=</span>n)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>xs <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>atleast_2d(xs)<span style=color:#ff6ac1>.</span>T <span style=color:#78787e># As sklearn expects a 2D feature list</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>errorbar(xs, ys, yerr<span style=color:#ff6ac1>=</span>es, fmt<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;o&#34;</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Data&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.7</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot([<span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>5</span>], [<span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>5</span>], label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Truth&#34;</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend();
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> sklearn.linear_model <span style=color:#ff6ac1>import</span> LinearRegression, Lasso, Ridge, ElasticNet
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>errorbar(xs, ys, yerr<span style=color:#ff6ac1>=</span>es, fmt<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;o&#34;</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Data&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.7</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot([<span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>5</span>], [<span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>5</span>], label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Truth&#34;</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>, ls<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;--&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> m <span style=color:#ff6ac1>in</span> [LinearRegression(), Lasso(), Ridge(), ElasticNet()]:
</span></span><span style=display:flex><span>    m<span style=color:#ff6ac1>.</span>fit(xs, ys)
</span></span><span style=display:flex><span>    plt<span style=color:#ff6ac1>.</span>plot(xs, m<span style=color:#ff6ac1>.</span>predict(xs), label<span style=color:#ff6ac1>=</span>m<span style=color:#ff6ac1>.</span>__class__<span style=color:#ff6ac1>.</span>__name__)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend(ncol<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>);
</span></span><span style=display:flex><span><span style=color:#78787e># 50 observations of 20 features, assuming 0.2 uncertainty on observations</span>
</span></span><span style=display:flex><span>np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>seed(<span style=color:#ff9f43>1</span>)
</span></span><span style=display:flex><span>xs <span style=color:#ff6ac1>=</span> norm<span style=color:#ff6ac1>.</span>rvs(scale<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>, size<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>50</span>, <span style=color:#ff9f43>20</span>))
</span></span><span style=display:flex><span>xs <span style=color:#ff6ac1>=</span> xs[xs[:, <span style=color:#ff9f43>0</span>]<span style=color:#ff6ac1>.</span>argsort()] <span style=color:#78787e># Sort array by first dimension for easier plotting</span>
</span></span><span style=display:flex><span>es <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>0.2</span> <span style=color:#ff6ac1>*</span> np<span style=color:#ff6ac1>.</span>ones(shape<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>50</span>)
</span></span><span style=display:flex><span>ys <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>1</span> <span style=color:#ff6ac1>*</span> xs[:, <span style=color:#ff9f43>0</span>] <span style=color:#ff6ac1>+</span> <span style=color:#ff9f43>0.1</span> <span style=color:#ff6ac1>*</span> xs[:, <span style=color:#ff9f43>1</span>] <span style=color:#ff6ac1>+</span> es <span style=color:#ff6ac1>*</span> norm<span style=color:#ff6ac1>.</span>rvs(size<span style=color:#ff6ac1>=</span>es<span style=color:#ff6ac1>.</span>shape)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>errorbar(xs[:, <span style=color:#ff9f43>0</span>], ys, yerr<span style=color:#ff6ac1>=</span>es, fmt<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;o&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.7</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Data (first dimension only)&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend();
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>errorbar(xs[:, <span style=color:#ff9f43>0</span>], ys, yerr<span style=color:#ff6ac1>=</span>es, fmt<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;o&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.7</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Data&#34;</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>beta_dict <span style=color:#ff6ac1>=</span> {}
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> m <span style=color:#ff6ac1>in</span> [LinearRegression(), Lasso(), Ridge(), ElasticNet()]:
</span></span><span style=display:flex><span>    m<span style=color:#ff6ac1>.</span>fit(xs, ys)
</span></span><span style=display:flex><span>    predict <span style=color:#ff6ac1>=</span> m<span style=color:#ff6ac1>.</span>predict(xs)
</span></span><span style=display:flex><span>    name <span style=color:#ff6ac1>=</span> m<span style=color:#ff6ac1>.</span>__class__<span style=color:#ff6ac1>.</span>__name__
</span></span><span style=display:flex><span>    beta_dict[name] <span style=color:#ff6ac1>=</span> m<span style=color:#ff6ac1>.</span>coef_
</span></span><span style=display:flex><span>    plt<span style=color:#ff6ac1>.</span>plot(xs[:, <span style=color:#ff9f43>0</span>], predict, label<span style=color:#ff6ac1>=</span>name)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend(ncol<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>);
</span></span><span style=display:flex><span>size<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>20</span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> name, betas <span style=color:#ff6ac1>in</span> beta_dict<span style=color:#ff6ac1>.</span>items():
</span></span><span style=display:flex><span>    plt<span style=color:#ff6ac1>.</span>scatter(np<span style=color:#ff6ac1>.</span>arange(betas<span style=color:#ff6ac1>.</span>shape[<span style=color:#ff9f43>0</span>]), betas, label<span style=color:#ff6ac1>=</span>name, s<span style=color:#ff6ac1>=</span>size, zorder<span style=color:#ff6ac1>=-</span>size)
</span></span><span style=display:flex><span>    size <span style=color:#ff6ac1>*=</span> <span style=color:#ff9f43>2</span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>axhline(<span style=color:#ff9f43>0</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;k&#34;</span>), plt<span style=color:#ff6ac1>.</span>ylabel(<span style=color:#5af78e>r</span><span style=color:#5af78e>&#34;$\beta$&#34;</span>), plt<span style=color:#ff6ac1>.</span>xlabel(<span style=color:#5af78e>&#34;Feature number&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend();
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> scipy.stats <span style=color:#ff6ac1>import</span> multivariate_normal <span style=color:#ff6ac1>as</span> mn
</span></span><span style=display:flex><span>n <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>50</span>
</span></span><span style=display:flex><span>es <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>0.1</span>
</span></span><span style=display:flex><span>x0 <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>linspace(<span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>10</span>, n)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Now some highly correlated variables!</span>
</span></span><span style=display:flex><span>x1 <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>0.5</span> <span style=color:#ff6ac1>*</span> x0 <span style=color:#ff6ac1>+</span> norm<span style=color:#ff6ac1>.</span>rvs(scale<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.01</span>, size<span style=color:#ff6ac1>=</span>n)
</span></span><span style=display:flex><span>x2 <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>0.5</span> <span style=color:#ff6ac1>*</span> x1 <span style=color:#ff6ac1>+</span> norm<span style=color:#ff6ac1>.</span>rvs(scale<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>5</span>, size<span style=color:#ff6ac1>=</span>n)
</span></span><span style=display:flex><span>xs <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>vstack((x0, x1, x2))<span style=color:#ff6ac1>.</span>T
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ys <span style=color:#ff6ac1>=</span> x0 <span style=color:#ff6ac1>+</span> es <span style=color:#ff6ac1>*</span> norm<span style=color:#ff6ac1>.</span>rvs(size<span style=color:#ff6ac1>=</span>n)
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> statsmodels.stats.outliers_influence <span style=color:#ff6ac1>import</span> variance_inflation_factor
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> index <span style=color:#ff6ac1>in</span> <span style=color:#ff5c57>range</span>(xs<span style=color:#ff6ac1>.</span>shape[<span style=color:#ff9f43>1</span>]):
</span></span><span style=display:flex><span>    <span style=color:#ff5c57>print</span>(index, variance_inflation_factor(xs, index))
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>do_monte_carlo_fit</span>(xs, ys, es, model, samps<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>100</span>):
</span></span><span style=display:flex><span>    <span style=color:#5af78e>&#34;&#34;&#34; A small function that perturbs the data and refits a model.
</span></span></span><span style=display:flex><span><span style=color:#5af78e>    Returns the mean prediction and also all the beta (coefficient) values.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    predictions <span style=color:#ff6ac1>=</span> []
</span></span><span style=display:flex><span>    betas <span style=color:#ff6ac1>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>for</span> i <span style=color:#ff6ac1>in</span> <span style=color:#ff5c57>range</span>(samps):
</span></span><span style=display:flex><span>        np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>seed(i)
</span></span><span style=display:flex><span>        m<span style=color:#ff6ac1>.</span>fit(xs, ys <span style=color:#ff6ac1>+</span> es <span style=color:#ff6ac1>*</span> norm<span style=color:#ff6ac1>.</span>rvs(size<span style=color:#ff6ac1>=</span>ys<span style=color:#ff6ac1>.</span>shape))
</span></span><span style=display:flex><span>        betas<span style=color:#ff6ac1>.</span>append(m<span style=color:#ff6ac1>.</span>coef_)
</span></span><span style=display:flex><span>        predictions<span style=color:#ff6ac1>.</span>append(m<span style=color:#ff6ac1>.</span>predict(xs))
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> np<span style=color:#ff6ac1>.</span>mean(predictions, axis<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>), np<span style=color:#ff6ac1>.</span>array(betas)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Set up the figure and bins for the beta histogram</span>
</span></span><span style=display:flex><span>fig, axes <span style=color:#ff6ac1>=</span> plt<span style=color:#ff6ac1>.</span>subplots(nrows<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>3</span>, figsize<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>8</span>, <span style=color:#ff9f43>8</span>))
</span></span><span style=display:flex><span>axes[<span style=color:#ff9f43>0</span>]<span style=color:#ff6ac1>.</span>errorbar(xs[:, <span style=color:#ff9f43>0</span>], ys, yerr<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.1</span>, fmt<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;o&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.7</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Data&#34;</span>);
</span></span><span style=display:flex><span>bins <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>linspace(<span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>1.2</span>, <span style=color:#ff9f43>200</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> m <span style=color:#ff6ac1>in</span> [LinearRegression(), Lasso(), Ridge(), ElasticNet()]:
</span></span><span style=display:flex><span>    <span style=color:#78787e># We get the mean prediction and all beta values</span>
</span></span><span style=display:flex><span>    mean, betas <span style=color:#ff6ac1>=</span> do_monte_carlo_fit(xs, ys, es, m)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#78787e># And plot it all out</span>
</span></span><span style=display:flex><span>    line, <span style=color:#ff6ac1>*</span>_ <span style=color:#ff6ac1>=</span> axes[<span style=color:#ff9f43>0</span>]<span style=color:#ff6ac1>.</span>plot(xs[:, <span style=color:#ff9f43>0</span>], mean, label<span style=color:#ff6ac1>=</span>m<span style=color:#ff6ac1>.</span>__class__<span style=color:#ff6ac1>.</span>__name__)
</span></span><span style=display:flex><span>    axes[<span style=color:#ff9f43>1</span>]<span style=color:#ff6ac1>.</span>hist(betas[:, <span style=color:#ff9f43>0</span>], color<span style=color:#ff6ac1>=</span>line<span style=color:#ff6ac1>.</span>get_color(), bins<span style=color:#ff6ac1>=</span>bins)
</span></span><span style=display:flex><span>    axes[<span style=color:#ff9f43>2</span>]<span style=color:#ff6ac1>.</span>hist(betas[:, <span style=color:#ff9f43>1</span>], color<span style=color:#ff6ac1>=</span>line<span style=color:#ff6ac1>.</span>get_color(), bins<span style=color:#ff6ac1>=</span>bins)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>axes[<span style=color:#ff9f43>0</span>]<span style=color:#ff6ac1>.</span>legend(loc<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>)
</span></span><span style=display:flex><span>axes[<span style=color:#ff9f43>0</span>]<span style=color:#ff6ac1>.</span>set_xlabel(<span style=color:#5af78e>r</span><span style=color:#5af78e>&#34;$X_0$&#34;</span>)
</span></span><span style=display:flex><span>axes[<span style=color:#ff9f43>1</span>]<span style=color:#ff6ac1>.</span>set_xlabel(<span style=color:#5af78e>r</span><span style=color:#5af78e>&#34;$\beta_0$&#34;</span>)
</span></span><span style=display:flex><span>axes[<span style=color:#ff9f43>2</span>]<span style=color:#ff6ac1>.</span>set_xlabel(<span style=color:#5af78e>r</span><span style=color:#5af78e>&#34;$\beta_1$&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>tight_layout();
</span></span></code></pre></div><div class="relative max-w-xl mx-auto my-20"><div class="fancy_card horizontal"><div class=card_translator><div class="card_rotator tiny_rot card_layer"><div class="card_layer newsletter p-2"><div class="newsletter-inner h-full"><div class="relative h-full sib-form-container" id=sib-form-container><div class="mb-6 lg:mb-12 text-center"><h3 class="text-main-200 mb-2 lg:mb-8">Stay in the loop!</h3><p class="text-main-100 text-lg text-opacity-70">For new releases, exclusive giveaways, and reviews.</p></div><form action="https://Cosmiccoding.us5.list-manage.com/subscribe/post?u=aebe5de1d2e40dccb3aeca491&id=3987dd4a1f" method=post id=mc-embedded-subscribe-form name=mc-embedded-subscribe-form class="validate w-full" target=_blank novalidate><div><input type=email class="w-full appearance-none bg-main-600 mb-4 border border-main-600 bg-opacity-5 focus:border-main-300 rounded-sm px-4 py-3 text-main-100 placeholder-main-100 required" placeholder="Your best email…" aria-label="Your best email…" name=EMAIL required data-required=true id=EMAIL><div style=position:absolute;left:-5000px aria-hidden=true><input type=text name=b_aebe5de1d2e40dccb3aeca491_3987dd4a1f tabindex=-1></div><input type=submit value=Subscribe name=subscribe id=mc-embedded-subscribe class="button btn bg-main-600 hover:bg-main-400 shadow w-full"></div><p id=mce-error-response style=display:none class="text-center mt-2 opacity-50 text-main-200">There was a problem, please try again.</p><p id=mce-success-response style=display:none class="text-center mt-2 opacity-50 text-main-200">Thanks mate, won't let ya down.</p></form></div></div></div><div class="card_layer card_effect card_soft_glare" style=pointer-events:none></div></div></div></div></div></div><script type=text/x-mathjax-config>
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script></div><script language=javascript type=text/javascript src=https://cosmiccoding.com.au/js/main.min.11673d10a962fb5205229607e62ea1d23424d35dad33db7bfe2a09a63d5409fa.js></script>
<script async defer src="https://www.googletagmanager.com/gtag/js?id=G-GRX6QE03YR"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-GRX6QE03YR")</script></div></body></html>