<!doctype html><html lang=en-gb><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>Propagating (non-gaussian) uncertainty - Samuel Hinton</title><link rel=stylesheet href="https://cosmiccoding.com.au/css/main.min.5400481537df6ded7c631d7584eaeb522e0bea3c952908a01d5163e524dfbcde.css" integrity="sha256-VABIFTffbe18Yx11hOrrUi4L6jyVKQigHVFj5STfvN4="><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel="shortcut icon" href=https://cosmiccoding.com.au/img/favicon.png type=image/x-icon></head><meta name=description content="From samples back to model confidence regions"><meta name=robots content="noodp"><link rel=canonical href=https://cosmiccoding.com.au/tutorials/propagating/><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://cosmiccoding.com.au/tutorials/propagating/cover.png"><meta name=twitter:title content="Propagating (non-gaussian) uncertainty"><meta name=twitter:description content="From samples back to model confidence regions"><meta property="og:title" content="Propagating (non-gaussian) uncertainty"><meta property="og:description" content="From samples back to model confidence regions"><meta property="og:type" content="article"><meta property="og:url" content="https://cosmiccoding.com.au/tutorials/propagating/"><meta property="og:image" content="https://cosmiccoding.com.au/tutorials/propagating/cover.png"><meta property="article:section" content="tutorials"><meta property="article:published_time" content="2019-08-02T00:00:00+00:00"><meta property="article:modified_time" content="2019-08-02T00:00:00+00:00"><meta property="article:section" content="tutorial"><meta property="article:published_time" content="2019-08-02 00:00:00 +0000 UTC"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Propagating (non-gaussian) uncertainty","genre":"tutorial","url":"https:\/\/cosmiccoding.com.au\/tutorials\/propagating\/","datePublished":"2019-08-02 00:00:00 \u002b0000 UTC","description":"From samples back to model confidence regions","author":{"@type":"Person","name":""}}</script><body><div class="flex flex-col min-h-screen overflow-hidden"><header class="absolute w-full z-30"><div class="max-w-6xl mx-auto px-4 sm:px-6"><div class="flex items-center justify-between h-20"><div class="flex-shrink-0 mr-4"><a class=block href=/ aria-label="Samuel Hinton"><h2 class=logo>SRH</h2></a></div><nav class="hidden md:flex md:flex-grow"><ul class="flex flex-grow justify-end flex-wrap items-center"><li><a href=/#books class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Books</a></li><li><a href=/reviews class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Reviews</a></li><li><a href=/tutorials class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Tutorials</a></li><li><a href=/blogs class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Blog</a></li><li><a href=/#courses class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Courses</a></li><li><a href=/static/resume/HintonCV.pdf class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">CV</a></li></ul></nav><div class=md:hidden x-data="{ expanded: false }"><button class=hamburger :class="{ 'active': expanded }" @click.stop="expanded = !expanded" aria-controls=mobile-nav :aria-expanded=expanded>
<span class=sr-only>Menu</span><svg class="w-6 h-6 fill-current text-gray-300 hover:text-gray-200 transition duration-150 ease-in-out" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><rect y="4" width="24" height="2" rx="1"/><rect y="11" width="24" height="2" rx="1"/><rect y="18" width="24" height="2" rx="1"/></svg></button><nav id=mobile-nav class="absolute top-full z-20 left-0 w-full px-4 sm:px-6 overflow-hidden transition-all duration-300 ease-in-out" x-ref=mobileNav :style="expanded ? 'max-height: ' + $refs.mobileNav.scrollHeight + 'px; opacity: 1' : 'max-height: 0; opacity: .8'" @click.away="expanded = false" @keydown.escape.window="expanded = false" x-cloak><ul class="bg-gray-800 px-4 py-2"><li><a href=/#books class="flex text-gray-300 hover:text-gray-200 py-2">Books</a></li><li><a href=/reviews class="flex text-gray-300 hover:text-gray-200 py-2">Reviews</a></li><li><a href=/tutorials class="flex text-gray-300 hover:text-gray-200 py-2">Tutorials</a></li><li><a href=/blogs class="flex text-gray-300 hover:text-gray-200 py-2">Blog</a></li><li><a href=/#courses class="flex text-gray-300 hover:text-gray-200 py-2">Courses</a></li><li><a href=/static/resume/HintonCV.pdf class="flex text-gray-300 hover:text-gray-200 py-2">CV</a></li></ul></nav></div></div></div></header><div class=flex-grow><div id=post-container class="content content-wider blog-post relative"><div class="section-header blog"><h1 class=title>Propagating (non-gaussian) uncertainty</h1><p>6th August 2019</p><p>From samples back to model confidence regions</p></div><div><ul class="grid gap-6 w-full md:grid-cols-2" style=list-style:none;padding-left:0><li><input type=radio id=show-code name=code-toggle value=show-code class="hidden peer" onchange=clickCheckbox(this) required checked>
<label for=show-code class="inline-flex justify-between items-center p-5 w-full text-gray-500 bg-gray-800 rounded-lg border border-gray-200 cursor-pointer peer-checked:border-2 peer-checked:border-main-300 peer-checked:text-white peer-checked:bg-gray-700 hover:text-gray-200 hover:bg-gray-700"><div class="block w-full"><div class="w-full text-center text-lg font-semibold">Show me everything!</div><div class="w-full text-center text-sm">Oh yeah, coding time.</div></div></label></li><li><input type=radio id=hide-code name=code-toggle value=hide-code class="hidden peer" onchange=clickCheckbox(this)>
<label for=hide-code class="inline-flex justify-between items-center p-5 w-full text-gray-500 bg-gray-800 rounded-lg border border-gray-200 cursor-pointer peer-checked:border-2 peer-checked:border-main-300 peer-checked:text-white peer-checked:bg-gray-700 hover:text-gray-200 hover:bg-gray-700"><div class="block w-full"><div class="w-full text-center text-lg font-semibold">Just the plots</div><div class="w-full text-center text-sm">Code is nasty.</div></div></label></li></ul></div><script>function clickCheckbox(e){e.id=="show-code"?document.getElementById("post-container").classList.remove("hide-code"):document.getElementById("post-container").classList.add("hide-code")}</script><p>You&rsquo;ve fit a model to some data. You have some mean, covariance or samples characterising that fit, but now how can you clearly show the $1$ and $2\sigma$ confidence regions of this fit back onto your model. In other words, how do you get this sort of image back at the end:</p><p><div><figure class=rounded><picture><source srcset=/tutorials/propagating/2019-08-02-Propagating_files/2019-08-02-Propagating_2_0_hu9e620dd60eb1a833d7c75d9a7aa05b89_163948_1920x0_resize_q90_h2_box_3.webp width=1920 height=1020 type=image/webp><source srcset=/tutorials/propagating/2019-08-02-Propagating_files/2019-08-02-Propagating_2_0.png width=2814 height=1495 type=image/png><img width=2814 height=1495 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>Let&rsquo;s run through a bunch of different ways, depending on how you&rsquo;ve fit your model to the data. Firstly, let&rsquo;s generate a model and some fake data to fit it with. In our case, lets have a function <code>fn</code> which implements $f(x) = a\sin(x) + ba$, where our &rsquo;true&rsquo; values for $a$ and $b$ will both be $1$ for simplicity. Why this - no particular reason, you can use anything, but this will explicitly not give Gaussian uncertainty, which makes it a more general use case.</p><p>From that function, we&rsquo;ll sample $20$ points, add some error, and then plot them.</p><div class=width-69 markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>import</span> numpy <span style=color:#ff6ac1>as</span> np
</span></span><span style=display:flex><span><span style=color:#ff6ac1>import</span> matplotlib.pyplot <span style=color:#ff6ac1>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>fn</span>(xs, a<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>, b<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>):
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> a <span style=color:#ff6ac1>*</span> np<span style=color:#ff6ac1>.</span>sin(xs) <span style=color:#ff6ac1>+</span> b <span style=color:#ff6ac1>*</span> a
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>truth <span style=color:#ff6ac1>=</span> [<span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>1</span>]
</span></span><span style=display:flex><span>x_val <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>linspace(<span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>5</span>, <span style=color:#ff9f43>500</span>)
</span></span><span style=display:flex><span>y_val <span style=color:#ff6ac1>=</span> fn(x_val)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>num_samps, err <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>20</span>, <span style=color:#ff9f43>0.3</span>
</span></span><span style=display:flex><span>np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>seed(<span style=color:#ff9f43>0</span>)
</span></span><span style=display:flex><span>x_samps <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>uniform(size<span style=color:#ff6ac1>=</span>num_samps) <span style=color:#ff6ac1>*</span> <span style=color:#ff9f43>5</span>
</span></span><span style=display:flex><span>y_samps <span style=color:#ff6ac1>=</span> fn(x_samps) <span style=color:#ff6ac1>+</span> err <span style=color:#ff6ac1>*</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>normal(size<span style=color:#ff6ac1>=</span>num_samps)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(x_val, y_val, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Underlying function&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>errorbar(x_samps, y_samps, yerr<span style=color:#ff6ac1>=</span>err, fmt<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;o&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Data&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend(loc<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>3</span>);
</span></span></code></pre></div></div><p><div><figure class=rounded><picture><source srcset=/tutorials/propagating/2019-08-02-Propagating_files/2019-08-02-Propagating_4_0_hu95d497825fff7db00531542141a7fb1c_87966_1920x0_resize_q90_h2_box_3.webp width=1920 height=991 type=image/webp><source srcset=/tutorials/propagating/2019-08-02-Propagating_files/2019-08-02-Propagating_4_0.png width=2731 height=1409 type=image/png><img width=2731 height=1409 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>Now, let&rsquo;s construct a likelihood for our data given our model. Note that normally this is one function, but I&rsquo;ve split it into two because I&rsquo;ll also do an example using <code>scipy.optimize.leastsq</code>, and that takes a difference between model and data, not a singular log-likelihood value. All we&rsquo;re implementing here - because we&rsquo;ve assumed normally distributed uncertainty in our observations (rightfully so), is the stock standard $\chi^2$ likelihood:</p><p>$$ \mathcal{L} \propto \mathcal{N}\left( \frac{y_{obs} - y_{pred}}{\sigma} \right) $$</p><div class="reduced-code width-52" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>from</span> scipy.stats <span style=color:#ff6ac1>import</span> norm
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>get_diff</span>(theta):
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> (y_samps <span style=color:#ff6ac1>-</span> fn(x_samps, <span style=color:#ff6ac1>*</span>theta)) <span style=color:#ff6ac1>/</span> err
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>log_likelihood</span>(theta):
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>if</span> theta[<span style=color:#ff9f43>0</span>] <span style=color:#ff6ac1>&lt;</span> <span style=color:#ff9f43>0</span> <span style=color:#ff6ac1>or</span> theta[<span style=color:#ff9f43>0</span>] <span style=color:#ff6ac1>&gt;</span> <span style=color:#ff9f43>2</span>:
</span></span><span style=display:flex><span>        <span style=color:#ff6ac1>return</span> <span style=color:#ff6ac1>-</span>np<span style=color:#ff6ac1>.</span>inf
</span></span><span style=display:flex><span>    diff <span style=color:#ff6ac1>=</span> get_diff(theta)
</span></span><span style=display:flex><span>    <span style=color:#78787e># Below is equivalent to norm.logpdf(diff).sum()</span>
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> (<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>0.5</span> <span style=color:#ff6ac1>*</span> diff <span style=color:#ff6ac1>**</span> <span style=color:#ff9f43>2</span>)<span style=color:#ff6ac1>.</span>sum()
</span></span></code></pre></div></div><p>Anyway, no need to get bogged down this. Let&rsquo;s quickly have a look at three different ways we could try and use these functions to fit our model to data.</p><h1 id=1-minimisation-techniques-without-samples>1. Minimisation techniques without samples</h1><p>Quick and dirty, we can use <code>minimize</code>, <code>leastsq</code> or <code>curve_fit</code> to do a very rough fit of our model to the data. At the end of the function we get some &ldquo;best fit&rdquo; position and hopefully a covariance matrix describing our uncertainty. Now, if we have those two, we can just <em>generate the samples ourselves</em>. We&rsquo;ll want them later, and its absolutely trivial to do. Why we want to do this is that a covariance matrix is Gaussian, and working with samples rather than a covariance matrix allows us to break that assumption for other methods of fitting the model to the data.</p><div class=width-67 markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>from</span> scipy.optimize <span style=color:#ff6ac1>import</span> leastsq
</span></span><span style=display:flex><span>mean, cov, <span style=color:#ff6ac1>*</span>_ <span style=color:#ff6ac1>=</span> leastsq(get_diff, [<span style=color:#ff9f43>2</span>, <span style=color:#ff9f43>2</span>], full_output<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span><span style=color:#78787e># If we don&#39;t have samples, *just make them*</span>
</span></span><span style=display:flex><span>cov_samples <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>multivariate_normal(mean, cov, size<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>100000</span>)
</span></span></code></pre></div></div><h1 id=2-equal-weighted-samplers>2. Equal-weighted Samplers</h1><p>Some samplers, like <code>emcee</code> have the useful property of each returning equally-weighted samples. Most don&rsquo;t do this - Metropolis-Hastings samplers will have a weight for how many attempts it took before they accepted a new point, Nested Sampling will have an increasing weight as samples increase in likelihood, etc. But let&rsquo;s cover equal-weighted samplers for completeness. We run <code>emcee</code> as shown below to generate fifty thousand samples of the likelihood surface.</p><div class=width-63 markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>import</span> emcee
</span></span><span style=display:flex><span>ndim <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>2</span>
</span></span><span style=display:flex><span>nwalkers <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>100</span>
</span></span><span style=display:flex><span>p0 <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>uniform(size<span style=color:#ff6ac1>=</span>(nwalkers, ndim)) <span style=color:#ff6ac1>*</span> <span style=color:#ff9f43>2</span>
</span></span><span style=display:flex><span>sampler <span style=color:#ff6ac1>=</span> emcee<span style=color:#ff6ac1>.</span>EnsembleSampler(nwalkers, ndim, log_likelihood)
</span></span><span style=display:flex><span>state <span style=color:#ff6ac1>=</span> sampler<span style=color:#ff6ac1>.</span>run_mcmc(p0, <span style=color:#ff9f43>1000</span>)
</span></span><span style=display:flex><span>chain <span style=color:#ff6ac1>=</span> sampler<span style=color:#ff6ac1>.</span>chain[:, <span style=color:#ff9f43>500</span>:, :]
</span></span><span style=display:flex><span>uniform <span style=color:#ff6ac1>=</span> chain<span style=color:#ff6ac1>.</span>reshape((<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>, ndim))
</span></span></code></pre></div></div><h1 id=3-weighted-samplers>3. Weighted Samplers</h1><p>This is the norm, but unfortunately the slowest type of samples deal with. To sample this surface, we&rsquo;ll use <code>nestle</code> and run it with <code>3000</code> live points as shown below. Note that for the prior transform (the <code>lambda</code> part, all we&rsquo;re saying here is that the allowed ranges for the parameters are between 0 and 2 - as the unit hypercube is simply doubled).</p><div class="expanded-code width-85" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>import</span> nestle
</span></span><span style=display:flex><span>res <span style=color:#ff6ac1>=</span> nestle<span style=color:#ff6ac1>.</span>sample(log_likelihood, <span style=color:#ff6ac1>lambda</span> x: x <span style=color:#ff6ac1>*</span> <span style=color:#ff9f43>2</span>, <span style=color:#ff9f43>2</span>, npoints<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>3000</span>, method<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#39;multi&#39;</span>)
</span></span><span style=display:flex><span>non_uniform <span style=color:#ff6ac1>=</span> res<span style=color:#ff6ac1>.</span>samples
</span></span><span style=display:flex><span>non_uniform_weights <span style=color:#ff6ac1>=</span> res<span style=color:#ff6ac1>.</span>weights
</span></span></code></pre></div></div><h1 id=comparing-the-three-methods>Comparing the three methods</h1><p>Now that we have three collections of samples, each representing a different algorithms attempt to map out the likelihood surface given by our model and dataset, let&rsquo;s quickly compare what they look like:</p><div class=width-73 markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>from</span> chainconsumer <span style=color:#ff6ac1>import</span> ChainConsumer
</span></span><span style=display:flex><span>c <span style=color:#ff6ac1>=</span> ChainConsumer()
</span></span><span style=display:flex><span>c<span style=color:#ff6ac1>.</span>add_chain(uniform, parameters<span style=color:#ff6ac1>=</span>[<span style=color:#5af78e>&#34;a&#34;</span>, <span style=color:#5af78e>&#34;b&#34;</span>], name<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Uniform&#34;</span>)
</span></span><span style=display:flex><span>c<span style=color:#ff6ac1>.</span>add_chain(non_uniform, weights<span style=color:#ff6ac1>=</span>non_uniform_weights, name<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Non-uniform&#34;</span>)
</span></span><span style=display:flex><span>c<span style=color:#ff6ac1>.</span>add_chain(cov_samples, name<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Covariance samples&#34;</span>)
</span></span><span style=display:flex><span>c<span style=color:#ff6ac1>.</span>configure(flip<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>False</span>)
</span></span><span style=display:flex><span>c<span style=color:#ff6ac1>.</span>plotter<span style=color:#ff6ac1>.</span>plot(truth<span style=color:#ff6ac1>=</span>truth, figsize<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1.4</span>);
</span></span></code></pre></div></div><p><div><figure class=rounded><picture><source srcset=/tutorials/propagating/2019-08-02-Propagating_files/2019-08-02-Propagating_15_1_huc37eff7bf783efce193f390280ca9822_198169_1712x0_resize_q90_h2_box_3.webp width=1712 height=1712 type=image/webp><source srcset=/tutorials/propagating/2019-08-02-Propagating_files/2019-08-02-Propagating_15_1.png width=1712 height=1712 type=image/png><img width=1712 height=1712 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>You can see <code>emcee</code> and <code>nestle</code> (shown in blue and green respectively) agree perfectly. In red, the <code>leastsq</code> method shows some issues (as expected). The surface isn&rsquo;t a perfect normal and it cannot capture that detail with it&rsquo;s simplistic method and local estimation of uncertainty around the best fit point. Just something to keep in mind on why model fitting should essentially never be done with any such method.</p><h1 id=propagating-uniform-samples---the-fast-and-hard-way>Propagating Uniform Samples - The Fast and Hard Way</h1><p>Now that we have samplers, we can propagate them back into model space for visualisation (or scientific use). If we want blazing speed, we really have to make sure our model is vectorised, such that we can ask for a set of $N$ $x$ values and a set of $M$ values for $\theta$ (in our case, $\theta = \lbrace a, b \rbrace$) and get out an $N\times M$ result. If it cannot be vectorised, we&rsquo;ll handle that next.</p><p>The <code>realisation</code> line below shows a vectorised implementation of <code>fn</code>. Note that I&rsquo;m using <code>prod</code> to speed up calculate $a\times b$ over all samples. So if we have $M$ samples, we now have $M$ realisations of our model at some input vector of <code>xs</code>. We can then ask <code>numpy</code> for the percentile across the first axis (corresponding to across the $M$ samples), and that will let us characterise the <em>distribution of our model values, given those samples, at each location in <code>xs</code></em>, where we summarise the distribution by asking for the $1$ and $2\sigma$ limits that you can see being passed into <code>norm.cdf</code>. We then fill between those sigma-levels in the plot to produce the fully-propagated uncertainty bounds in our model space.</p><div class="expanded-code width-82" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>xs <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>linspace(<span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>5</span>, <span style=color:#ff9f43>50</span>)
</span></span><span style=display:flex><span>realisations <span style=color:#ff6ac1>=</span> uniform[:, <span style=color:#ff9f43>0</span>][:, <span style=color:#ff6ac1>None</span>] <span style=color:#ff6ac1>*</span> np<span style=color:#ff6ac1>.</span>sin(xs) <span style=color:#ff6ac1>+</span> uniform<span style=color:#ff6ac1>.</span>prod(axis<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>)[:, <span style=color:#ff6ac1>None</span>]
</span></span><span style=display:flex><span>bounds <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>percentile(realisations, <span style=color:#ff9f43>100</span> <span style=color:#ff6ac1>*</span> norm<span style=color:#ff6ac1>.</span>cdf([<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>2</span>, <span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>2</span>]), axis<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Plot everything</span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>plot</span>(bounds, title):
</span></span><span style=display:flex><span>    fig, ax <span style=color:#ff6ac1>=</span> plt<span style=color:#ff6ac1>.</span>subplots()
</span></span><span style=display:flex><span>    plt<span style=color:#ff6ac1>.</span>plot(x_val, y_val, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Underlying function&#34;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#ff6ac1>.</span>errorbar(x_samps, y_samps, yerr<span style=color:#ff6ac1>=</span>err, fmt<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;o&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Data&#34;</span>)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>plot(xs, bounds[<span style=color:#ff9f43>2</span>, :], label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Best Fit&#34;</span>, ls<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;--&#34;</span>),
</span></span><span style=display:flex><span>    plt<span style=color:#ff6ac1>.</span>fill_between(xs, bounds[<span style=color:#ff9f43>0</span>, :], bounds[<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>, :], 
</span></span><span style=display:flex><span>                     label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;95\</span><span style=color:#5af78e>% u</span><span style=color:#5af78e>ncertainty&#34;</span>, fc<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;#21cbff&#34;</span>, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.1</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#ff6ac1>.</span>fill_between(xs, bounds[<span style=color:#ff9f43>1</span>, :], bounds[<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>2</span>, :], 
</span></span><span style=display:flex><span>                     label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;68\</span><span style=color:#5af78e>% u</span><span style=color:#5af78e>ncertainty&#34;</span>, fc<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;#219bff&#34;</span>, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.5</span>)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>legend(loc<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>3</span>)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>set_title(title, fontsize<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>10</span>);
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>set_xlabel(<span style=color:#5af78e>&#34;x&#34;</span>), ax<span style=color:#ff6ac1>.</span>set_ylabel(<span style=color:#5af78e>&#34;y&#34;</span>), ax<span style=color:#ff6ac1>.</span>set_xlim(<span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>5</span>);
</span></span><span style=display:flex><span>plot(bounds, <span style=color:#5af78e>&#34;Vectorised uniform-weight sample construction&#34;</span>)
</span></span></code></pre></div></div><p><div><figure class=rounded><picture><source srcset=/tutorials/propagating/2019-08-02-Propagating_files/2019-08-02-Propagating_18_0_hu32270c620b66ca991235229314e4a2e3_238647_1920x0_resize_q90_h2_box_3.webp width=1920 height=1059 type=image/webp><source srcset=/tutorials/propagating/2019-08-02-Propagating_files/2019-08-02-Propagating_18_0.png width=2851 height=1572 type=image/png><img width=2851 height=1572 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><h1 id=propagating-uniform-samples---the-easy-and-slow-way>Propagating Uniform Samples - The Easy and Slow Way</h1><p>So, we can&rsquo;t vectorise, our model isn&rsquo;t trivial enough, our lives are in ruin. Well, no matter, we can generate the model realisations one-by-one, and note that you don&rsquo;t need <em>all that many</em> realisations. We&rsquo;re characting a distribution to be able to plot it, so you&rsquo;d only need around a thousand points to do it to sufficient accuracy. Firstly, we take our samples and shuffle them. Doing this isn&rsquo;t always needed, but some samplers have auto-correlation and shuffling is a good way to make sure you don&rsquo;t bias your plots by selecting the first thousand samples&mldr; which all happen to be right next to each other.</p><p>From there, we manually construct the realisations from the first thousand shuffled samples, and do exactly the same steps as before with <code>np.percentile</code>.</p><div class="expanded-code width-81" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#78787e># Copy our samples then shuffle them to ensure randomness and low autocorrelation</span>
</span></span><span style=display:flex><span>shuffled <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>copy(uniform)
</span></span><span style=display:flex><span>np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>shuffle(shuffled)
</span></span><span style=display:flex><span><span style=color:#78787e># Row by row construct the prediction of our model using 1000 model realisations</span>
</span></span><span style=display:flex><span>realisations <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>array([fn(xs, row[<span style=color:#ff9f43>0</span>], row[<span style=color:#ff9f43>1</span>]) <span style=color:#ff6ac1>for</span> row <span style=color:#ff6ac1>in</span> shuffled[:<span style=color:#ff9f43>1000</span>, :]])
</span></span><span style=display:flex><span>bounds <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>percentile(realisations, <span style=color:#ff9f43>100</span> <span style=color:#ff6ac1>*</span> norm<span style=color:#ff6ac1>.</span>cdf([<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>2</span>, <span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>2</span>]), axis<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>)
</span></span><span style=display:flex><span>plot(bounds, <span style=color:#5af78e>&#34;Iterative uniform-weight sample construction&#34;</span>)
</span></span></code></pre></div></div><p><div><figure class=rounded><picture><source srcset=/tutorials/propagating/2019-08-02-Propagating_files/2019-08-02-Propagating_21_0_hu5f99315a949b12ac3f6105a43b7a4ecd_238029_1920x0_resize_q90_h2_box_3.webp width=1920 height=1059 type=image/webp><source srcset=/tutorials/propagating/2019-08-02-Propagating_files/2019-08-02-Propagating_21_0.png width=2851 height=1572 type=image/png><img width=2851 height=1572 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><h1 id=non-uniformly-weighted-samples---the-hard-and-slow-way>Non-uniformly weighted samples - The Hard and Slow Way</h1><p>Finally, if we don&rsquo;t have uniformly-weighted samples, still no issue. It would be nice if <code>numpy.percentile</code> supported weights, but it doesn&rsquo;t, so what we&rsquo;ll do is resample from our samples such that they <em>become</em> equally weighted. We can do this via stock-standard rejection sampling. We have a weight (which we should make sure is normalised from 0 to 1 initially, if its not the normalise it), and we compare it against a random number drawn uniformly between 0 and 1. If our weight is above the random number, we keep that sample, if not, we move on.</p><p>So, a sample with weight $0.1$ will only appear in our final sample $10%$ of the time. A sample with weight $0.5$ will be in it $50%$ of the time. So we&rsquo;re dropping points according to their weight, and that means we can treat all samples that made the cut as having equal weight. In the code below, we do this step in one line to generate a <code>mask</code> which we can apply to our initial samples. We still shuffle them (for possible autocorrelation reasons), but then we do the same final steps as before.</p><div class="expanded-code width-80" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#78787e># Sample our weights according to their weight</span>
</span></span><span style=display:flex><span>mask <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>uniform(size<span style=color:#ff6ac1>=</span>non_uniform_weights<span style=color:#ff6ac1>.</span>shape) <span style=color:#ff6ac1>&lt;</span> non_uniform_weights
</span></span><span style=display:flex><span>sampled <span style=color:#ff6ac1>=</span> non_uniform[mask, :]
</span></span><span style=display:flex><span>np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>shuffle(sampled)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Row by row construct the prediction of our model using 1000 model realisations</span>
</span></span><span style=display:flex><span>realisations <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>array([fn(xs, row[<span style=color:#ff9f43>0</span>], row[<span style=color:#ff9f43>1</span>]) <span style=color:#ff6ac1>for</span> row <span style=color:#ff6ac1>in</span> shuffled[:<span style=color:#ff9f43>1000</span>, :]])
</span></span><span style=display:flex><span>bounds <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>percentile(realisations, <span style=color:#ff9f43>100</span> <span style=color:#ff6ac1>*</span> norm<span style=color:#ff6ac1>.</span>cdf([<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>2</span>, <span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>2</span>]), axis<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>)
</span></span><span style=display:flex><span>plot(bounds, <span style=color:#5af78e>&#34;Iterative non-uniform-weight sample construction&#34;</span>)
</span></span></code></pre></div></div><p><div><figure class="img-main rounded"><picture><source srcset=/tutorials/propagating/cover_hudbac8895c4e516457860c514ecf524eb_237984_1920x0_resize_q90_h2_box_3.webp width=1920 height=1059 type=image/webp><source srcset=/tutorials/propagating/cover.png width=2851 height=1572 type=image/png><img width=2851 height=1572 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>And you should notice that all three mothods have produced exactly the same plot. Which is good. Hopefully these methods are useful!</p><hr><p>For your convenience, here&rsquo;s the code in one block:</p><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>import</span> numpy <span style=color:#ff6ac1>as</span> np
</span></span><span style=display:flex><span><span style=color:#ff6ac1>import</span> matplotlib.pyplot <span style=color:#ff6ac1>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>fn</span>(xs, a<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>, b<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>):
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> a <span style=color:#ff6ac1>*</span> np<span style=color:#ff6ac1>.</span>sin(xs) <span style=color:#ff6ac1>+</span> b <span style=color:#ff6ac1>*</span> a
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>truth <span style=color:#ff6ac1>=</span> [<span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>1</span>]
</span></span><span style=display:flex><span>x_val <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>linspace(<span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>5</span>, <span style=color:#ff9f43>500</span>)
</span></span><span style=display:flex><span>y_val <span style=color:#ff6ac1>=</span> fn(x_val)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>num_samps, err <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>20</span>, <span style=color:#ff9f43>0.3</span>
</span></span><span style=display:flex><span>np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>seed(<span style=color:#ff9f43>0</span>)
</span></span><span style=display:flex><span>x_samps <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>uniform(size<span style=color:#ff6ac1>=</span>num_samps) <span style=color:#ff6ac1>*</span> <span style=color:#ff9f43>5</span>
</span></span><span style=display:flex><span>y_samps <span style=color:#ff6ac1>=</span> fn(x_samps) <span style=color:#ff6ac1>+</span> err <span style=color:#ff6ac1>*</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>normal(size<span style=color:#ff6ac1>=</span>num_samps)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>plot(x_val, y_val, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Underlying function&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>errorbar(x_samps, y_samps, yerr<span style=color:#ff6ac1>=</span>err, fmt<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;o&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Data&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>legend(loc<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>3</span>);
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> scipy.stats <span style=color:#ff6ac1>import</span> norm
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>get_diff</span>(theta):
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> (y_samps <span style=color:#ff6ac1>-</span> fn(x_samps, <span style=color:#ff6ac1>*</span>theta)) <span style=color:#ff6ac1>/</span> err
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>log_likelihood</span>(theta):
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>if</span> theta[<span style=color:#ff9f43>0</span>] <span style=color:#ff6ac1>&lt;</span> <span style=color:#ff9f43>0</span> <span style=color:#ff6ac1>or</span> theta[<span style=color:#ff9f43>0</span>] <span style=color:#ff6ac1>&gt;</span> <span style=color:#ff9f43>2</span>:
</span></span><span style=display:flex><span>        <span style=color:#ff6ac1>return</span> <span style=color:#ff6ac1>-</span>np<span style=color:#ff6ac1>.</span>inf
</span></span><span style=display:flex><span>    diff <span style=color:#ff6ac1>=</span> get_diff(theta)
</span></span><span style=display:flex><span>    <span style=color:#78787e># Below is equivalent to norm.logpdf(diff).sum()</span>
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> (<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>0.5</span> <span style=color:#ff6ac1>*</span> diff <span style=color:#ff6ac1>**</span> <span style=color:#ff9f43>2</span>)<span style=color:#ff6ac1>.</span>sum()
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> scipy.optimize <span style=color:#ff6ac1>import</span> leastsq
</span></span><span style=display:flex><span>mean, cov, <span style=color:#ff6ac1>*</span>_ <span style=color:#ff6ac1>=</span> leastsq(get_diff, [<span style=color:#ff9f43>2</span>, <span style=color:#ff9f43>2</span>], full_output<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span><span style=color:#78787e># If we don&#39;t have samples, *just make them*</span>
</span></span><span style=display:flex><span>cov_samples <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>multivariate_normal(mean, cov, size<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>100000</span>)
</span></span><span style=display:flex><span><span style=color:#ff6ac1>import</span> emcee
</span></span><span style=display:flex><span>ndim <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>2</span>
</span></span><span style=display:flex><span>nwalkers <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>100</span>
</span></span><span style=display:flex><span>p0 <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>uniform(size<span style=color:#ff6ac1>=</span>(nwalkers, ndim)) <span style=color:#ff6ac1>*</span> <span style=color:#ff9f43>2</span>
</span></span><span style=display:flex><span>sampler <span style=color:#ff6ac1>=</span> emcee<span style=color:#ff6ac1>.</span>EnsembleSampler(nwalkers, ndim, log_likelihood)
</span></span><span style=display:flex><span>state <span style=color:#ff6ac1>=</span> sampler<span style=color:#ff6ac1>.</span>run_mcmc(p0, <span style=color:#ff9f43>1000</span>)
</span></span><span style=display:flex><span>chain <span style=color:#ff6ac1>=</span> sampler<span style=color:#ff6ac1>.</span>chain[:, <span style=color:#ff9f43>500</span>:, :]
</span></span><span style=display:flex><span>uniform <span style=color:#ff6ac1>=</span> chain<span style=color:#ff6ac1>.</span>reshape((<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>, ndim))
</span></span><span style=display:flex><span><span style=color:#ff6ac1>import</span> nestle
</span></span><span style=display:flex><span>res <span style=color:#ff6ac1>=</span> nestle<span style=color:#ff6ac1>.</span>sample(log_likelihood, <span style=color:#ff6ac1>lambda</span> x: x <span style=color:#ff6ac1>*</span> <span style=color:#ff9f43>2</span>, <span style=color:#ff9f43>2</span>, npoints<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>3000</span>, method<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#39;multi&#39;</span>)
</span></span><span style=display:flex><span>non_uniform <span style=color:#ff6ac1>=</span> res<span style=color:#ff6ac1>.</span>samples
</span></span><span style=display:flex><span>non_uniform_weights <span style=color:#ff6ac1>=</span> res<span style=color:#ff6ac1>.</span>weights
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> chainconsumer <span style=color:#ff6ac1>import</span> ChainConsumer
</span></span><span style=display:flex><span>c <span style=color:#ff6ac1>=</span> ChainConsumer()
</span></span><span style=display:flex><span>c<span style=color:#ff6ac1>.</span>add_chain(uniform, parameters<span style=color:#ff6ac1>=</span>[<span style=color:#5af78e>&#34;a&#34;</span>, <span style=color:#5af78e>&#34;b&#34;</span>], name<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Uniform&#34;</span>)
</span></span><span style=display:flex><span>c<span style=color:#ff6ac1>.</span>add_chain(non_uniform, weights<span style=color:#ff6ac1>=</span>non_uniform_weights, name<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Non-uniform&#34;</span>)
</span></span><span style=display:flex><span>c<span style=color:#ff6ac1>.</span>add_chain(cov_samples, name<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Covariance samples&#34;</span>)
</span></span><span style=display:flex><span>c<span style=color:#ff6ac1>.</span>configure(flip<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>False</span>)
</span></span><span style=display:flex><span>c<span style=color:#ff6ac1>.</span>plotter<span style=color:#ff6ac1>.</span>plot(truth<span style=color:#ff6ac1>=</span>truth, figsize<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1.4</span>);
</span></span><span style=display:flex><span>xs <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>linspace(<span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>5</span>, <span style=color:#ff9f43>50</span>)
</span></span><span style=display:flex><span>realisations <span style=color:#ff6ac1>=</span> uniform[:, <span style=color:#ff9f43>0</span>][:, <span style=color:#ff6ac1>None</span>] <span style=color:#ff6ac1>*</span> np<span style=color:#ff6ac1>.</span>sin(xs) <span style=color:#ff6ac1>+</span> uniform<span style=color:#ff6ac1>.</span>prod(axis<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>)[:, <span style=color:#ff6ac1>None</span>]
</span></span><span style=display:flex><span>bounds <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>percentile(realisations, <span style=color:#ff9f43>100</span> <span style=color:#ff6ac1>*</span> norm<span style=color:#ff6ac1>.</span>cdf([<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>2</span>, <span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>2</span>]), axis<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Plot everything</span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>plot</span>(bounds, title):
</span></span><span style=display:flex><span>    fig, ax <span style=color:#ff6ac1>=</span> plt<span style=color:#ff6ac1>.</span>subplots()
</span></span><span style=display:flex><span>    plt<span style=color:#ff6ac1>.</span>plot(x_val, y_val, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Underlying function&#34;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#ff6ac1>.</span>errorbar(x_samps, y_samps, yerr<span style=color:#ff6ac1>=</span>err, fmt<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;o&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Data&#34;</span>)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>plot(xs, bounds[<span style=color:#ff9f43>2</span>, :], label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Best Fit&#34;</span>, ls<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;--&#34;</span>),
</span></span><span style=display:flex><span>    plt<span style=color:#ff6ac1>.</span>fill_between(xs, bounds[<span style=color:#ff9f43>0</span>, :], bounds[<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>, :], 
</span></span><span style=display:flex><span>                     label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;95\</span><span style=color:#5af78e>% u</span><span style=color:#5af78e>ncertainty&#34;</span>, fc<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;#21cbff&#34;</span>, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.1</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#ff6ac1>.</span>fill_between(xs, bounds[<span style=color:#ff9f43>1</span>, :], bounds[<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>2</span>, :], 
</span></span><span style=display:flex><span>                     label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;68\</span><span style=color:#5af78e>% u</span><span style=color:#5af78e>ncertainty&#34;</span>, fc<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;#219bff&#34;</span>, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.5</span>)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>legend(loc<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>3</span>)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>set_title(title, fontsize<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>10</span>);
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>set_xlabel(<span style=color:#5af78e>&#34;x&#34;</span>), ax<span style=color:#ff6ac1>.</span>set_ylabel(<span style=color:#5af78e>&#34;y&#34;</span>), ax<span style=color:#ff6ac1>.</span>set_xlim(<span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>5</span>);
</span></span><span style=display:flex><span>plot(bounds, <span style=color:#5af78e>&#34;Vectorised uniform-weight sample construction&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#78787e># Copy our samples then shuffle them to ensure randomness and low autocorrelation</span>
</span></span><span style=display:flex><span>shuffled <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>copy(uniform)
</span></span><span style=display:flex><span>np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>shuffle(shuffled)
</span></span><span style=display:flex><span><span style=color:#78787e># Row by row construct the prediction of our model using 1000 model realisations</span>
</span></span><span style=display:flex><span>realisations <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>array([fn(xs, row[<span style=color:#ff9f43>0</span>], row[<span style=color:#ff9f43>1</span>]) <span style=color:#ff6ac1>for</span> row <span style=color:#ff6ac1>in</span> shuffled[:<span style=color:#ff9f43>1000</span>, :]])
</span></span><span style=display:flex><span>bounds <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>percentile(realisations, <span style=color:#ff9f43>100</span> <span style=color:#ff6ac1>*</span> norm<span style=color:#ff6ac1>.</span>cdf([<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>2</span>, <span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>2</span>]), axis<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>)
</span></span><span style=display:flex><span>plot(bounds, <span style=color:#5af78e>&#34;Iterative uniform-weight sample construction&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#78787e># Sample our weights according to their weight</span>
</span></span><span style=display:flex><span>mask <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>uniform(size<span style=color:#ff6ac1>=</span>non_uniform_weights<span style=color:#ff6ac1>.</span>shape) <span style=color:#ff6ac1>&lt;</span> non_uniform_weights
</span></span><span style=display:flex><span>sampled <span style=color:#ff6ac1>=</span> non_uniform[mask, :]
</span></span><span style=display:flex><span>np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>shuffle(sampled)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Row by row construct the prediction of our model using 1000 model realisations</span>
</span></span><span style=display:flex><span>realisations <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>array([fn(xs, row[<span style=color:#ff9f43>0</span>], row[<span style=color:#ff9f43>1</span>]) <span style=color:#ff6ac1>for</span> row <span style=color:#ff6ac1>in</span> shuffled[:<span style=color:#ff9f43>1000</span>, :]])
</span></span><span style=display:flex><span>bounds <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>percentile(realisations, <span style=color:#ff9f43>100</span> <span style=color:#ff6ac1>*</span> norm<span style=color:#ff6ac1>.</span>cdf([<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>2</span>, <span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>2</span>]), axis<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>)
</span></span><span style=display:flex><span>plot(bounds, <span style=color:#5af78e>&#34;Iterative non-uniform-weight sample construction&#34;</span>)
</span></span></code></pre></div><div class="relative max-w-xl mx-auto my-20"><div class="fancy_card horizontal"><div class=card_translator><div class="card_rotator tiny_rot card_layer"><div class="card_layer newsletter p-2"><div class="newsletter-inner h-full"><div class="relative h-full sib-form-container" id=sib-form-container><div class="mb-6 lg:mb-12 text-center"><h3 class="text-main-200 mb-2 lg:mb-8">Stay in the loop!</h3><p class="text-main-100 text-lg text-opacity-70">For new releases, exclusive giveaways, and reviews.</p></div><form action="https://Cosmiccoding.us5.list-manage.com/subscribe/post?u=aebe5de1d2e40dccb3aeca491&amp;id=3987dd4a1f" method=post id=mc-embedded-subscribe-form name=mc-embedded-subscribe-form class="validate w-full" target=_blank novalidate><div><input type=email class="w-full appearance-none bg-main-600 mb-4 border border-main-600 bg-opacity-5 focus:border-main-300 rounded-sm px-4 py-3 text-main-100 placeholder-main-100 required" placeholder="Your best email…" aria-label="Your best email…" name=EMAIL required data-required=true id=EMAIL><div style=position:absolute;left:-5000px aria-hidden=true><input type=text name=b_aebe5de1d2e40dccb3aeca491_3987dd4a1f tabindex=-1></div><input type=submit value=Subscribe name=subscribe id=mc-embedded-subscribe class="button btn bg-main-600 hover:bg-main-400 shadow w-full"></div><p id=mce-error-response style=display:none class="text-center mt-2 opacity-50 text-main-200">There was a problem, please try again.</p><p id=mce-success-response style=display:none class="text-center mt-2 opacity-50 text-main-200">Thanks mate, won't let ya down.</p></form></div></div></div><div class="card_layer card_effect card_soft_glare" style=pointer-events:none></div></div></div></div></div></div><script type=text/x-mathjax-config>
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script></div><script language=javascript type=text/javascript src=https://cosmiccoding.com.au/js/main.min.11673d10a962fb5205229607e62ea1d23424d35dad33db7bfe2a09a63d5409fa.js></script>
<script async defer src="https://www.googletagmanager.com/gtag/js?id=G-GRX6QE03YR"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-GRX6QE03YR")</script></div></body></html>