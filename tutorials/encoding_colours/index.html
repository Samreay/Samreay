<!doctype html><html lang=en-gb><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>Training a Neural Network Embedding Layer with Keras - Samuel Hinton</title><link rel=stylesheet href="https://cosmiccoding.com.au/css/main.min.b842bc4f8d358708f7b5666fe7108ed6474cd18dad036996cd6f85d3bb7b6a42.css" integrity="sha256-uEK8T401hwj3tWZv5xCO1kdM0Y2tA2mWzW+F07t7akI="><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel="shortcut icon" href=https://cosmiccoding.com.au/img/favicon.png type=image/x-icon></head><meta name=description content="Using python, Keras and some colours to illustrate encoding as simply as possible"><meta name=robots content="noodp"><link rel=canonical href=https://cosmiccoding.com.au/tutorials/encoding_colours/><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://cosmiccoding.com.au/tutorials/encoding_colours/cover.png"><meta name=twitter:title content="Training a Neural Network Embedding Layer with Keras"><meta name=twitter:description content="Using python, Keras and some colours to illustrate encoding as simply as possible"><meta property="og:title" content="Training a Neural Network Embedding Layer with Keras"><meta property="og:description" content="Using python, Keras and some colours to illustrate encoding as simply as possible"><meta property="og:type" content="article"><meta property="og:url" content="https://cosmiccoding.com.au/tutorials/encoding_colours/"><meta property="og:image" content="https://cosmiccoding.com.au/tutorials/encoding_colours/cover.png"><meta property="article:section" content="tutorials"><meta property="article:published_time" content="2020-07-26T00:00:00+00:00"><meta property="article:modified_time" content="2020-07-26T00:00:00+00:00"><meta property="article:section" content="tutorial"><meta property="article:published_time" content="2020-07-26 00:00:00 +0000 UTC"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Training a Neural Network Embedding Layer with Keras","genre":"tutorial","url":"https:\/\/cosmiccoding.com.au\/tutorials\/encoding_colours\/","datePublished":"2020-07-26 00:00:00 \u002b0000 UTC","description":"Using python, Keras and some colours to illustrate encoding as simply as possible","author":{"@type":"Person","name":""}}</script><body><div class="flex flex-col min-h-screen overflow-hidden"><header class="absolute w-full z-30"><div class="max-w-6xl mx-auto px-4 sm:px-6"><div class="flex items-center justify-between h-20"><div class="flex-shrink-0 mr-4"><a class=block href=/ aria-label="Samuel Hinton"><h2 class=logo>SRH</h2></a></div><nav class="hidden md:flex md:flex-grow"><ul class="flex flex-grow justify-end flex-wrap items-center"><li><a href=/#books class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Books</a></li><li><a href=/reviews class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Reviews</a></li><li><a href=/tutorials class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Tutorials</a></li><li><a href=/blogs class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Blog</a></li><li><a href=/#courses class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Courses</a></li><li><a href=/static/resume/HintonCV.pdf class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">CV</a></li></ul></nav><div class=md:hidden x-data="{ expanded: false }"><button class=hamburger :class="{ 'active': expanded }" @click.stop="expanded = !expanded" aria-controls=mobile-nav :aria-expanded=expanded>
<span class=sr-only>Menu</span><svg class="w-6 h-6 fill-current text-gray-300 hover:text-gray-200 transition duration-150 ease-in-out" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><rect y="4" width="24" height="2" rx="1"/><rect y="11" width="24" height="2" rx="1"/><rect y="18" width="24" height="2" rx="1"/></svg></button><nav id=mobile-nav class="absolute top-full z-20 left-0 w-full px-4 sm:px-6 overflow-hidden transition-all duration-300 ease-in-out" x-ref=mobileNav :style="expanded ? 'max-height: ' + $refs.mobileNav.scrollHeight + 'px; opacity: 1' : 'max-height: 0; opacity: .8'" @click.away="expanded = false" @keydown.escape.window="expanded = false" x-cloak><ul class="bg-gray-800 px-4 py-2"><li><a href=/#books class="flex text-gray-300 hover:text-gray-200 py-2">Books</a></li><li><a href=/reviews class="flex text-gray-300 hover:text-gray-200 py-2">Reviews</a></li><li><a href=/tutorials class="flex text-gray-300 hover:text-gray-200 py-2">Tutorials</a></li><li><a href=/blogs class="flex text-gray-300 hover:text-gray-200 py-2">Blog</a></li><li><a href=/#courses class="flex text-gray-300 hover:text-gray-200 py-2">Courses</a></li><li><a href=/static/resume/HintonCV.pdf class="flex text-gray-300 hover:text-gray-200 py-2">CV</a></li></ul></nav></div></div></div></header><div class=flex-grow><div id=post-container class="content content-wider blog-post relative"><div class="section-header blog"><h1 class=title>Training a Neural Network Embedding Layer with Keras</h1><p>6th July 2020</p><p>Using python, Keras and some colours to illustrate encoding as simply as possible</p></div><div><ul class="grid gap-6 w-full md:grid-cols-2" style=list-style:none;padding-left:0><li><input type=radio id=show-code name=code-toggle value=show-code class="hidden peer" onchange=clickCheckbox(this) required checked>
<label for=show-code class="inline-flex justify-between items-center p-5 w-full text-gray-500 bg-gray-800 rounded-lg border border-gray-200 cursor-pointer peer-checked:border-2 peer-checked:border-main-300 peer-checked:text-white peer-checked:bg-gray-700 hover:text-gray-200 hover:bg-gray-700"><div class="block w-full"><div class="w-full text-center text-lg font-semibold">Show me everything!</div><div class="w-full text-center text-sm">Oh yeah, coding time.</div></div></label></li><li><input type=radio id=hide-code name=code-toggle value=hide-code class="hidden peer" onchange=clickCheckbox(this)>
<label for=hide-code class="inline-flex justify-between items-center p-5 w-full text-gray-500 bg-gray-800 rounded-lg border border-gray-200 cursor-pointer peer-checked:border-2 peer-checked:border-main-300 peer-checked:text-white peer-checked:bg-gray-700 hover:text-gray-200 hover:bg-gray-700"><div class="block w-full"><div class="w-full text-center text-lg font-semibold">Just the plots</div><div class="w-full text-center text-sm">Code is nasty.</div></div></label></li></ul></div><script>function clickCheckbox(e){e.id=="show-code"?document.getElementById("post-container").classList.remove("hide-code"):document.getElementById("post-container").classList.add("hide-code")}</script><p>This little write is designed to try and explain what <strong>embeddings</strong> are, and how we can train a naive version of an embedding to understand and visualise the process. We&rsquo;ll do this using a colour dataset, Keras and good old-fashioned matplotlib.</p><h1 id=introduction>Introduction</h1><p>Let&rsquo;s start simple: <em>What is an embedding?</em></p><p>An embedding is a way to represent some categorical feature (like a word), as a dense parameter. Specifically, this is normally a unit vector in a high dimensional hypersphere.</p><p>A common way of encoding a categorical feature of machine learning is to one-hot-encode them. However, for a large number of categories, this creates a very spare matrix. Imagine encoding the names of babies born in 2020. You might have a million records, but with ten thousand possible names, that is a <strong>very</strong> big matrix filled with mostly zeros. Also, names like Mat and Matt are just as similar as Mat and Patrica when you one-hot-encode. That is, not similar at all.</p><p>Instead, if we can create a dense vector (aka a vector filled with numbers and not mostly zeros), we can represent Mat, Matt and Patrica as some location in higher dimensional space, where the Mat and Matt vectors are similar to each other. This is what we are trying to do with embeddings. To learn the translation from a categorical feature to this vector in higher dimensional space.</p><p>Most of the time when you use embeddings, you&rsquo;ll use them already trained and available - you won&rsquo;t be training them yourself. However, to understand what they are better, we&rsquo;ll mock up a dataset based on colour combinations, and learn the embeddings to turn a colour name into a location in both 2D and 3D space.</p><p>So, for the rest of this write up, the goal is to:</p><ol><li>Start with some colours.</li><li>Create a data product similar to how Word2Vec and others embeddings are trained.</li><li>Create a model with a 2D embedding layer and train it.</li><li>Visualise the embedding layer.</li><li>Do the same for a 3D normalised embedding just for fun.</li></ol><p>Let&rsquo;s get cracking!</p><h1 id=the-colour-dataset>The colour dataset</h1><p>We&rsquo;ll source the colour dataset available from <a href=https://www.kaggle.com/ravikanth/colour-name-and-rgb-codes>Kaggle here</a>. Let&rsquo;s load it in and view a few samples from it.</p><div class="reduced-code width-57" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>import</span> pandas <span style=color:#ff6ac1>as</span> pd
</span></span><span style=display:flex><span><span style=color:#ff6ac1>import</span> numpy <span style=color:#ff6ac1>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df_original <span style=color:#ff6ac1>=</span> pd<span style=color:#ff6ac1>.</span>read_csv(<span style=color:#5af78e>&#34;encoding_colours/colours.csv&#34;</span>)
</span></span><span style=display:flex><span>df_original <span style=color:#ff6ac1>=</span> df_original<span style=color:#ff6ac1>.</span>dropna(subset<span style=color:#ff6ac1>=</span>[<span style=color:#5af78e>&#34;Color Name&#34;</span>])
</span></span><span style=display:flex><span>num_colours <span style=color:#ff6ac1>=</span> df_original<span style=color:#ff6ac1>.</span>shape[<span style=color:#ff9f43>0</span>]
</span></span><span style=display:flex><span><span style=color:#ff5c57>print</span>(<span style=color:#5af78e>f</span><span style=color:#5af78e>&#34;We have </span><span style=color:#5af78e>{</span>num_colours<span style=color:#5af78e>}</span><span style=color:#5af78e> colours&#34;</span>)
</span></span><span style=display:flex><span>df_original<span style=color:#ff6ac1>.</span>sample(<span style=color:#ff9f43>5</span>)
</span></span></code></pre></div></div><pre><code>We have 646 colours
</code></pre><div><table class="table-auto table dataframe"><thead><tr style=text-align:right><th></th><th>Color Name</th><th>Credits</th><th>R;G;B Dec</th><th>RGB Hex</th><th>CSS Hex</th><th>BG/FG color sample</th></tr></thead><tbody><tr><th>398</th><td>honeydew4</td><td>X</td><td>131;139;131</td><td>838B83</td><td>NaN</td><td>### SAMPLE ###</td></tr><tr><th>126</th><td>CadetBlue</td><td>X</td><td>95;158;160</td><td>5F9EA0</td><td>NaN</td><td>### SAMPLE ###</td></tr><tr><th>335</th><td>SpringGreen4</td><td>X</td><td>0;139;69</td><td>008B45</td><td>NaN</td><td>#©2006 walsh@njit.edu#</td></tr><tr><th>310</th><td>GreenYellow</td><td>X</td><td>173;255;47</td><td>ADFF2F</td><td>NaN</td><td>### SAMPLE ###</td></tr><tr><th>426</th><td>HotPink4</td><td>X</td><td>139;58;98</td><td>8B3A62</td><td>NaN</td><td>### SAMPLE ###</td></tr></tbody></table></div><p>So after dropping NaNs, we have 646 different colour names. Lets throw out columns we don&rsquo;t want, and split the R;G;B Dec into separate columns (and then normalise them to 1).</p><div class="expanded-code width-83" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>df <span style=color:#ff6ac1>=</span> df_original<span style=color:#ff6ac1>.</span>loc[:, [<span style=color:#5af78e>&#34;Color Name&#34;</span>, <span style=color:#5af78e>&#34;R;G;B Dec&#34;</span>]]
</span></span><span style=display:flex><span>df[[<span style=color:#5af78e>&#34;r&#34;</span>, <span style=color:#5af78e>&#34;g&#34;</span>, <span style=color:#5af78e>&#34;b&#34;</span>]] <span style=color:#ff6ac1>=</span> df[<span style=color:#5af78e>&#34;R;G;B Dec&#34;</span>]<span style=color:#ff6ac1>.</span>str<span style=color:#ff6ac1>.</span>split(<span style=color:#5af78e>&#34;;&#34;</span>, expand<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)<span style=color:#ff6ac1>.</span>astype(<span style=color:#ff5c57>int</span>) <span style=color:#ff6ac1>/</span> <span style=color:#ff9f43>255</span>
</span></span><span style=display:flex><span>df <span style=color:#ff6ac1>=</span> df<span style=color:#ff6ac1>.</span>drop(columns<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;R;G;B Dec&#34;</span>)
</span></span><span style=display:flex><span>df <span style=color:#ff6ac1>=</span> df<span style=color:#ff6ac1>.</span>rename(columns<span style=color:#ff6ac1>=</span>{<span style=color:#5af78e>&#34;Color Name&#34;</span>: <span style=color:#5af78e>&#34;name&#34;</span>})
</span></span><span style=display:flex><span>df <span style=color:#ff6ac1>=</span> df<span style=color:#ff6ac1>.</span>reset_index(drop<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span>df<span style=color:#ff6ac1>.</span>sample(<span style=color:#ff9f43>10</span>)
</span></span></code></pre></div></div><div><table class="table-auto table dataframe"><thead><tr style=text-align:right><th></th><th>name</th><th>r</th><th>g</th><th>b</th></tr></thead><tbody><tr><th>93</th><td>grey82</td><td>0.819608</td><td>0.819608</td><td>0.819608</td></tr><tr><th>556</th><td>NavajoWhite4</td><td>0.545098</td><td>0.474510</td><td>0.368627</td></tr><tr><th>440</th><td>PaleVioletRed4</td><td>0.545098</td><td>0.278431</td><td>0.364706</td></tr><tr><th>401</th><td>sienna4</td><td>0.545098</td><td>0.278431</td><td>0.149020</td></tr><tr><th>58</th><td>grey47</td><td>0.470588</td><td>0.470588</td><td>0.470588</td></tr><tr><th>392</th><td>salmon</td><td>0.980392</td><td>0.501961</td><td>0.447059</td></tr><tr><th>626</th><td>gold2</td><td>0.933333</td><td>0.788235</td><td>0.000000</td></tr><tr><th>240</th><td>Free Speech Blue</td><td>0.254902</td><td>0.337255</td><td>0.772549</td></tr><tr><th>212</th><td>cyan2</td><td>0.000000</td><td>0.933333</td><td>0.933333</td></tr><tr><th>319</th><td>SpringGreen</td><td>0.000000</td><td>1.000000</td><td>0.498039</td></tr></tbody></table></div><p>Now theres just one more issue - you dont pass in strings or text to a neural network. You pass in numbers. So lets one-hot encode our colours to give them a numeric representation. We <em>could</em> use the Keras preprocessing <code>one_hot</code> here&mldr; but we&rsquo;ve got this nice dataframe which already has an index&mldr; so we&rsquo;ll use that, and I&rsquo;ll make it explicit and add it as a column.</p><div class="reduced-code width-20" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>df[<span style=color:#5af78e>&#34;num&#34;</span>] <span style=color:#ff6ac1>=</span> df<span style=color:#ff6ac1>.</span>index
</span></span><span style=display:flex><span>df<span style=color:#ff6ac1>.</span>head(<span style=color:#ff9f43>10</span>)
</span></span></code></pre></div></div><div><table class="table-auto table dataframe"><thead><tr style=text-align:right><th></th><th>name</th><th>r</th><th>g</th><th>b</th><th>num</th></tr></thead><tbody><tr><th>0</th><td>Grey</td><td>0.329412</td><td>0.329412</td><td>0.329412</td><td>0</td></tr><tr><th>1</th><td>Grey, Silver</td><td>0.752941</td><td>0.752941</td><td>0.752941</td><td>1</td></tr><tr><th>2</th><td>grey</td><td>0.745098</td><td>0.745098</td><td>0.745098</td><td>2</td></tr><tr><th>3</th><td>LightGray</td><td>0.827451</td><td>0.827451</td><td>0.827451</td><td>3</td></tr><tr><th>4</th><td>LightSlateGrey</td><td>0.466667</td><td>0.533333</td><td>0.600000</td><td>4</td></tr><tr><th>5</th><td>SlateGray</td><td>0.439216</td><td>0.501961</td><td>0.564706</td><td>5</td></tr><tr><th>6</th><td>SlateGray1</td><td>0.776471</td><td>0.886275</td><td>1.000000</td><td>6</td></tr><tr><th>7</th><td>SlateGray2</td><td>0.725490</td><td>0.827451</td><td>0.933333</td><td>7</td></tr><tr><th>8</th><td>SlateGray3</td><td>0.623529</td><td>0.713725</td><td>0.803922</td><td>8</td></tr><tr><th>9</th><td>SlateGray4</td><td>0.423529</td><td>0.482353</td><td>0.545098</td><td>9</td></tr></tbody></table></div><p>At this point, we have a nice data product, but it doesn&rsquo;t look like how you might train embeddings for words.</p><p>Words don&rsquo;t have a well defined mathematical representation to start with, instead we simply see certain words next to each other (or close to each other) more often, and we learn from that. So lets start by generating pairs of colours to emulate pairs of sequential words in what is now a colour-palette-like example.</p><div class=width-79 markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>n <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>100000</span> <span style=color:#78787e># Num samples</span>
</span></span><span style=display:flex><span>colour_1 <span style=color:#ff6ac1>=</span> df<span style=color:#ff6ac1>.</span>sample(n<span style=color:#ff6ac1>=</span>n, replace<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>, random_state<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>)<span style=color:#ff6ac1>.</span>reset_index(drop<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span>colour_2 <span style=color:#ff6ac1>=</span> df<span style=color:#ff6ac1>.</span>sample(n<span style=color:#ff6ac1>=</span>n, replace<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>, random_state<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>42</span>)<span style=color:#ff6ac1>.</span>reset_index(drop<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span>c <span style=color:#ff6ac1>=</span> colour_1<span style=color:#ff6ac1>.</span>merge(colour_2, left_index<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>, right_index<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span>c[[<span style=color:#5af78e>&#34;name_x&#34;</span>, <span style=color:#5af78e>&#34;name_y&#34;</span>]]<span style=color:#ff6ac1>.</span>sample(<span style=color:#ff9f43>10</span>)
</span></span></code></pre></div></div><div><table class="table-auto table dataframe"><thead><tr style=text-align:right><th></th><th>name_x</th><th>name_y</th></tr></thead><tbody><tr><th>9838</th><td>LightSalmon1</td><td>bright gold</td></tr><tr><th>44160</th><td>DarkSeaGreen2</td><td>goldenrod4</td></tr><tr><th>38804</th><td>firebrick</td><td>CadetBlue</td></tr><tr><th>52395</th><td>DarkKhaki</td><td>grey76</td></tr><tr><th>80675</th><td>DarkOliveGreen</td><td>OliveDrab2</td></tr><tr><th>28397</th><td>Dark Turquoise</td><td>DarkGoldenrod</td></tr><tr><th>35853</th><td>aquamarine4</td><td>orange</td></tr><tr><th>67955</th><td>LightSalmon3</td><td>aquamarine3, MediumAquamarine</td></tr><tr><th>42872</th><td>grey32</td><td>grey13</td></tr><tr><th>68885</th><td>Neon Pink</td><td>coral1</td></tr></tbody></table></div><p>Great! A hundred thousand colour combinations. However, this is still useless. When we train embeddings on words, we want positive <strong>and</strong> negative examples.</p><p>In a textual example, the pairs that come from words being next to each other are positive examples. We can generate negative examples by scrambling the document to remove meaning, and then taking pairs (of course, there many formal methods of doing this that I&rsquo;m not going to go into!).</p><p>In our case, we&rsquo;ll define a similarity metric ourselves, by just using the distance the two colours are from each other.</p><div class=width-78 markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#78787e># Calculate the distance, and drop the RGB columns.</span>
</span></span><span style=display:flex><span>c[<span style=color:#5af78e>&#34;diff&#34;</span>] <span style=color:#ff6ac1>=</span> ((c<span style=color:#ff6ac1>.</span>r_x <span style=color:#ff6ac1>-</span> c<span style=color:#ff6ac1>.</span>r_y)<span style=color:#ff6ac1>**</span><span style=color:#ff9f43>2</span> <span style=color:#ff6ac1>+</span> (c<span style=color:#ff6ac1>.</span>g_x <span style=color:#ff6ac1>-</span> c<span style=color:#ff6ac1>.</span>g_y)<span style=color:#ff6ac1>**</span><span style=color:#ff9f43>2</span> <span style=color:#ff6ac1>+</span> (c<span style=color:#ff6ac1>.</span>b_x <span style=color:#ff6ac1>-</span> c<span style=color:#ff6ac1>.</span>b_y)<span style=color:#ff6ac1>**</span><span style=color:#ff9f43>2</span>) <span style=color:#ff6ac1>/</span> <span style=color:#ff9f43>3</span>
</span></span><span style=display:flex><span>c <span style=color:#ff6ac1>=</span> c<span style=color:#ff6ac1>.</span>drop(columns<span style=color:#ff6ac1>=</span>[<span style=color:#5af78e>&#34;r_x&#34;</span>, <span style=color:#5af78e>&#34;r_y&#34;</span>, <span style=color:#5af78e>&#34;g_x&#34;</span>, <span style=color:#5af78e>&#34;g_y&#34;</span>, <span style=color:#5af78e>&#34;b_x&#34;</span>, <span style=color:#5af78e>&#34;b_y&#34;</span>])
</span></span><span style=display:flex><span>c
</span></span></code></pre></div></div><div><table class="table-auto table dataframe"><thead><tr style=text-align:right><th></th><th>name_x</th><th>num_x</th><th>name_y</th><th>num_y</th><th>diff</th></tr></thead><tbody><tr><th>0</th><td>gainsboro</td><td>559</td><td>grey91</td><td>102</td><td>0.002215</td></tr><tr><th>1</th><td>Goldenrod</td><td>629</td><td>OrangeRed4</td><td>435</td><td>0.266913</td></tr><tr><th>2</th><td>SteelBlue4</td><td>192</td><td>tan2</td><td>270</td><td>0.210832</td></tr><tr><th>3</th><td>DarkSalmon</td><td>359</td><td>grey95</td><td>106</td><td>0.117621</td></tr><tr><th>4</th><td>SlateGray4</td><td>9</td><td>grey60</td><td>71</td><td>0.015999</td></tr><tr><th>...</th><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr><tr><th>99995</th><td>grey65</td><td>76</td><td>Very Dark Brown</td><td>281</td><td>0.149199</td></tr><tr><th>99996</th><td>SkyBlue2</td><td>180</td><td>DarkOrchid2</td><td>479</td><td>0.105908</td></tr><tr><th>99997</th><td>LemonChiffon2</td><td>592</td><td>purple1</td><td>525</td><td>0.231757</td></tr><tr><th>99998</th><td>cornsilk1</td><td>609</td><td>grey74</td><td>85</td><td>0.045101</td></tr><tr><th>99999</th><td>brown3</td><td>253</td><td>LightYellow2</td><td>603</td><td>0.312813</td></tr></tbody></table><p>100000 rows × 5 columns</p></div><p>Now that we have a difference, lets turn that into a set of positive and negative examples. I&rsquo;m just going to compare the difference to a random number, and you can see this will generate a bunch of predicted values of 1 and 0.</p><div class="expanded-code width-80" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>seed(<span style=color:#ff9f43>0</span>)
</span></span><span style=display:flex><span>c[<span style=color:#5af78e>&#34;predict&#34;</span>] <span style=color:#ff6ac1>=</span> (c[<span style=color:#5af78e>&#34;diff&#34;</span>] <span style=color:#ff6ac1>&lt;</span> <span style=color:#ff9f43>0.2</span> <span style=color:#ff6ac1>*</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>random(c<span style=color:#ff6ac1>.</span>shape[<span style=color:#ff9f43>0</span>]) <span style=color:#ff6ac1>**</span> <span style=color:#ff9f43>2</span>)<span style=color:#ff6ac1>.</span>astype(<span style=color:#ff5c57>int</span>)
</span></span><span style=display:flex><span><span style=color:#ff5c57>print</span>(<span style=color:#5af78e>f</span><span style=color:#5af78e>&#34;</span><span style=color:#5af78e>{</span><span style=color:#ff9f43>100</span> <span style=color:#ff6ac1>*</span> c<span style=color:#ff6ac1>.</span>predict<span style=color:#ff6ac1>.</span>mean()<span style=color:#5af78e>:</span><span style=color:#5af78e> 0.1f</span><span style=color:#5af78e>}</span><span style=color:#5af78e>% positive values&#34;</span>)
</span></span><span style=display:flex><span>c<span style=color:#ff6ac1>.</span>sample(<span style=color:#ff9f43>10</span>)
</span></span></code></pre></div></div><pre><code> 22.5% positive values
</code></pre><div><table class="table-auto table dataframe"><thead><tr style=text-align:right><th></th><th>name_x</th><th>num_x</th><th>name_y</th><th>num_y</th><th>diff</th><th>predict</th></tr></thead><tbody><tr><th>63834</th><td>LightYellow1</td><td>602</td><td>burlywood1</td><td>257</td><td>0.034330</td><td>0</td></tr><tr><th>12504</th><td>Dusty Rose</td><td>468</td><td>VioletRed3</td><td>444</td><td>0.041143</td><td>1</td></tr><tr><th>48828</th><td>salmon1</td><td>393</td><td>Free Speech Green</td><td>352</td><td>0.410821</td><td>0</td></tr><tr><th>32737</th><td>orange3</td><td>390</td><td>grey89</td><td>100</td><td>0.311926</td><td>0</td></tr><tr><th>97626</th><td>red4</td><td>462</td><td>grey44</td><td>55</td><td>0.132344</td><td>1</td></tr><tr><th>31753</th><td>grey57</td><td>68</td><td>grey78</td><td>89</td><td>0.044844</td><td>0</td></tr><tr><th>81431</th><td>DarkSeaGreen4</td><td>296</td><td>burlywood3</td><td>259</td><td>0.058239</td><td>1</td></tr><tr><th>32403</th><td>grey15</td><td>26</td><td>BlueViolet</td><td>117</td><td>0.232572</td><td>0</td></tr><tr><th>62143</th><td>DarkOliveGreen1</td><td>287</td><td>SlateBlue4</td><td>187</td><td>0.286633</td><td>0</td></tr><tr><th>99415</th><td>MistyRose2</td><td>428</td><td>SkyBlue1</td><td>179</td><td>0.065016</td><td>1</td></tr></tbody></table></div><p>Its common to have more negative values than positive, both because you can generate essentially infinite negative values (just keep sticking random words together for text), so I&rsquo;ve made sure to have around a quarter positive values here. You can generate whatever ratio you like, it won&rsquo;t actually change how the rest of this example runs.</p><p>Now that we have a training dataset, let&rsquo;s make a Keras model!</p><div class="expanded-code width-80" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>from</span> tensorflow.random <span style=color:#ff6ac1>import</span> set_seed
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> tensorflow <span style=color:#ff6ac1>import</span> keras
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> tensorflow.keras.layers <span style=color:#ff6ac1>import</span> Embedding, Dense, Lambda, Input, Subtract
</span></span><span style=display:flex><span><span style=color:#ff6ac1>import</span> tensorflow.keras.backend <span style=color:#ff6ac1>as</span> K
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> tensorflow.keras.callbacks <span style=color:#ff6ac1>import</span> LambdaCallback
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>sum_dist</span>(x):
</span></span><span style=display:flex><span>    n <span style=color:#ff6ac1>=</span> K<span style=color:#ff6ac1>.</span>permute_dimensions(x, pattern<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>2</span>))
</span></span><span style=display:flex><span>    a, b <span style=color:#ff6ac1>=</span> n[<span style=color:#ff9f43>0</span>], n[<span style=color:#ff9f43>1</span>]
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> K<span style=color:#ff6ac1>.</span>sum((a <span style=color:#ff6ac1>-</span> b)<span style=color:#ff6ac1>**</span><span style=color:#ff9f43>2</span>, axis<span style=color:#ff6ac1>=-</span><span style=color:#ff9f43>1</span>, keepdims<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>get_model</span>(embedding_dims<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>):
</span></span><span style=display:flex><span>    model <span style=color:#ff6ac1>=</span> keras<span style=color:#ff6ac1>.</span>Sequential()
</span></span><span style=display:flex><span>    model<span style=color:#ff6ac1>.</span>add(Embedding(num_colours, embedding_dims, input_length<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>))
</span></span><span style=display:flex><span>    model<span style=color:#ff6ac1>.</span>add(Lambda(sum_dist, output_shape<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>1</span>,), name<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Dist&#34;</span>))
</span></span><span style=display:flex><span>    model<span style=color:#ff6ac1>.</span>add(Dense(<span style=color:#ff9f43>1</span>, activation<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;sigmoid&#34;</span>))
</span></span><span style=display:flex><span>    <span style=color:#ff5c57>print</span>(model<span style=color:#ff6ac1>.</span>summary())
</span></span><span style=display:flex><span>    model<span style=color:#ff6ac1>.</span>compile(loss<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#39;binary_crossentropy&#39;</span>, optimizer<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;adam&#34;</span>, metrics<span style=color:#ff6ac1>=</span>[<span style=color:#5af78e>&#34;mse&#34;</span>])
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> model
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>setseed</span>(i):
</span></span><span style=display:flex><span>    np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>seed(i), set_seed(i)
</span></span></code></pre></div></div><p>Okay, lets step through this. In <code>get_model</code> we ask for a normal sequential model. The <code>Embedding</code> layer is a lookup table, which will have 646 rows (one for each colour), and will produce a 2D vector for each word. Because we generate two words at a time, we set <code>input_length=2</code> - which means the output of the embeding layer will be 2 2D vectors (aka a matrix of shape <code>(2,2)</code>).</p><p>If you don&rsquo;t want to combine the inputs together like I am doing, <a href=https://stackoverflow.com/questions/53356849/how-to-train-a-model-with-only-an-embedding-layer-in-keras-and-no-labels>here</a> is an example where the model has two separate inputs that are combined later.</p><p>After the embedding, we have a <code>Lambda</code> layer, which simply takes that <code>2x2</code> matrix from the embedding output, splits it into <code>a</code> and <code>b</code> (the embedding vector for each of our two colours), and then returns the sum of the squared difference. Which is the Euclidean distance squared.</p><p>The keen-eyed among you might remember that we set a positive value for our predict column before for similar vectors&mldr; and here similar vectors will have a result close to zero!</p><p>It doesn&rsquo;t matter!</p><p>The reason we don&rsquo;t care, is because the <code>Lambda</code> layer is connected to a single <code>Dense</code> layer, which will also train its weight and bias. If the weight is negative and bias positive, it will act as an inverting mechanism. Neural networks really are magic!</p><p>If this still upsets you though, you can change out the <code>K.sum</code>, but a naive invert will cause division by zero. To get around it, you could use Lidstone smoothing (aka add a number to the denominator so its never zero), but I digress.</p><p>Finally, to make sure I can reproduce these plots exactly, I added the <code>setseed</code> function.</p><p>Lets now instantiate the model, and fit it. Oh, I&rsquo;ll also save out the embedding weights using a custom callback as we go, so that we can see their evolution over epochs.</p><div class=width-73 markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>weights <span style=color:#ff6ac1>=</span> []
</span></span><span style=display:flex><span>save <span style=color:#ff6ac1>=</span> LambdaCallback(on_epoch_end<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>lambda</span> batch, logs: 
</span></span><span style=display:flex><span>                      weights<span style=color:#ff6ac1>.</span>append(model<span style=color:#ff6ac1>.</span>layers[<span style=color:#ff9f43>0</span>]<span style=color:#ff6ac1>.</span>get_weights()[<span style=color:#ff9f43>0</span>]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X, y <span style=color:#ff6ac1>=</span> c[[<span style=color:#5af78e>&#34;num_x&#34;</span>, <span style=color:#5af78e>&#34;num_y&#34;</span>]], c[<span style=color:#5af78e>&#34;predict&#34;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>setseed(<span style=color:#ff9f43>7</span>)
</span></span><span style=display:flex><span>model <span style=color:#ff6ac1>=</span> get_model()
</span></span><span style=display:flex><span>model<span style=color:#ff6ac1>.</span>fit(X, y, epochs<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>500</span>, verbose<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>, batch_size<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>512</span>, callbacks<span style=color:#ff6ac1>=</span>[save]);
</span></span></code></pre></div></div><pre><code>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 2, 2)              1292      
_________________________________________________________________
Dist (Lambda)                (None, 1)                 0         
_________________________________________________________________
dense (Dense)                (None, 1)                 2         
=================================================================
Total params: 1,294
Trainable params: 1,294
Non-trainable params: 0
_________________________________________________________________
None
</code></pre><p><strong>Warning: nastly plotting code below.</strong></p><p>Now that we have a trained model, lets see how it performed. Below is an animation of the embeddings evolving.</p><div class="expanded-code width-80" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>%%</span>capture
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> matplotlib.animation <span style=color:#ff6ac1>import</span> FuncAnimation
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> IPython.display <span style=color:#ff6ac1>import</span> HTML
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Get the list of colours</span>
</span></span><span style=display:flex><span>cs <span style=color:#ff6ac1>=</span> df[[<span style=color:#5af78e>&#34;r&#34;</span>, <span style=color:#5af78e>&#34;g&#34;</span>, <span style=color:#5af78e>&#34;b&#34;</span>]]<span style=color:#ff6ac1>.</span>to_numpy()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Create the plots</span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>with</span> plt<span style=color:#ff6ac1>.</span>style<span style=color:#ff6ac1>.</span>context(<span style=color:#5af78e>&#34;default&#34;</span>):
</span></span><span style=display:flex><span>    fig, ax <span style=color:#ff6ac1>=</span> plt<span style=color:#ff6ac1>.</span>subplots(figsize<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>10</span>, <span style=color:#ff9f43>5</span>))
</span></span><span style=display:flex><span>    fig<span style=color:#ff6ac1>.</span>subplots_adjust(left<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.1</span>, right<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.9</span>, bottom<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.15</span>, top<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.9</span>)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>set_xlabel(<span style=color:#5af78e>&#34;$x_0$&#34;</span>), ax<span style=color:#ff6ac1>.</span>set_ylabel(<span style=color:#5af78e>&#34;$x_1$&#34;</span>)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>set_title(<span style=color:#5af78e>&#34;Training of 2D colour embeddings&#34;</span>)
</span></span><span style=display:flex><span>    scat <span style=color:#ff6ac1>=</span> ax<span style=color:#ff6ac1>.</span>scatter(weights[<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>][:, <span style=color:#ff9f43>0</span>], weights[<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>][:, <span style=color:#ff9f43>1</span>], color<span style=color:#ff6ac1>=</span>cs, s<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>10</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>init</span>():
</span></span><span style=display:flex><span>        ax<span style=color:#ff6ac1>.</span>set_xlim(weights[<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>][:,<span style=color:#ff9f43>0</span>]<span style=color:#ff6ac1>.</span>min(), weights[<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>][:,<span style=color:#ff9f43>0</span>]<span style=color:#ff6ac1>.</span>max())
</span></span><span style=display:flex><span>        ax<span style=color:#ff6ac1>.</span>set_ylim(weights[<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>][:,<span style=color:#ff9f43>1</span>]<span style=color:#ff6ac1>.</span>min(), weights[<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>][:,<span style=color:#ff9f43>1</span>]<span style=color:#ff6ac1>.</span>max())
</span></span><span style=display:flex><span>        <span style=color:#ff6ac1>return</span> scat,
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>update</span>(i):
</span></span><span style=display:flex><span>        scat<span style=color:#ff6ac1>.</span>set_offsets(weights[i])
</span></span><span style=display:flex><span>        <span style=color:#ff6ac1>return</span> scat,
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#78787e># Split our epochs into frames</span>
</span></span><span style=display:flex><span>    nw <span style=color:#ff6ac1>=</span> <span style=color:#ff5c57>len</span>(weights) <span style=color:#ff6ac1>-</span> <span style=color:#ff9f43>1</span>
</span></span><span style=display:flex><span>    nf, power <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>30</span> <span style=color:#ff6ac1>*</span> <span style=color:#ff9f43>10</span>, <span style=color:#ff9f43>2</span>
</span></span><span style=display:flex><span>    frames <span style=color:#ff6ac1>=</span> pd<span style=color:#ff6ac1>.</span>unique((np<span style=color:#ff6ac1>.</span>linspace(<span style=color:#ff9f43>1</span>, nw<span style=color:#ff6ac1>**</span>(<span style=color:#ff9f43>1</span> <span style=color:#ff6ac1>/</span> power), nf)<span style=color:#ff6ac1>**</span>power)<span style=color:#ff6ac1>.</span>astype(<span style=color:#ff5c57>int</span>))
</span></span><span style=display:flex><span>    ani <span style=color:#ff6ac1>=</span> FuncAnimation(fig, update, frames<span style=color:#ff6ac1>=</span>frames, 
</span></span><span style=display:flex><span>                        init_func<span style=color:#ff6ac1>=</span>init, blit<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>, interval<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>33.3</span>);
</span></span></code></pre></div></div><p>We can then use this ugly plotting code to output PNGs and turn them into a video using ffmpeg.</p><div class=width-70 markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>ani<span style=color:#ff6ac1>.</span>save(<span style=color:#5af78e>&#39;embed_2d.mp4&#39;</span>, fps<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>30</span>, 
</span></span><span style=display:flex><span>         extra_args<span style=color:#ff6ac1>=</span>[<span style=color:#5af78e>&#39;-vcodec&#39;</span>, <span style=color:#5af78e>&#39;libx264&#39;</span>, <span style=color:#5af78e>&#39;-crf&#39;</span>, <span style=color:#5af78e>&#39;26&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># If you&#39;re running this in a Jupyter notebook, the below will do the </span>
</span></span><span style=display:flex><span><span style=color:#78787e># same without saving it to file</span>
</span></span><span style=display:flex><span><span style=color:#78787e># HTML(ani.to_html5_video())</span>
</span></span></code></pre></div></div><div class="reduced-code width-33" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>from</span> IPython.display <span style=color:#ff6ac1>import</span> Video
</span></span><span style=display:flex><span>Video(<span style=color:#5af78e>&#34;embed_2d.mp4&#34;</span>)
</span></span></code></pre></div></div><p><div class=video><video preload=auto playsinline plays-inline controls autoplay loop muted>
<source src=/tutorials/encoding_colours/embed_2d.mp4 type=video/mp4></video></div></p><p>This is beautiful. From the starting randomly initialised mess, quickly structure emerges. You can see the black to white gradient down the middle, with blue and red being split on either side. Green, being in between and limited by 2D space, gets stuck in between.</p><p>And to give you something that isn&rsquo;t moving to look at better, here is the final embedding.</p><div class=width-71 markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>fig, ax <span style=color:#ff6ac1>=</span> plt<span style=color:#ff6ac1>.</span>subplots()
</span></span><span style=display:flex><span>scat <span style=color:#ff6ac1>=</span> ax<span style=color:#ff6ac1>.</span>scatter(weights[<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>][:, <span style=color:#ff9f43>0</span>], weights[<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>][:, <span style=color:#ff9f43>1</span>], color<span style=color:#ff6ac1>=</span>cs, s<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>20</span>)
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>set_xlabel(<span style=color:#5af78e>&#34;$x_0$&#34;</span>), ax<span style=color:#ff6ac1>.</span>set_ylabel(<span style=color:#5af78e>&#34;$x_1$&#34;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>set_title(<span style=color:#5af78e>&#34;2D Colour Embeddings&#34;</span>);
</span></span></code></pre></div></div><p><div><figure class="img-main rounded"><picture><source srcset=/tutorials/encoding_colours/cover_hu1eb978d5f30ebefe40f98a82d984cb58_342735_2874x1599_resize_q90_h2_box_3.webp width=2874 height=1599 type=image/webp><source srcset=/tutorials/encoding_colours/cover_hu1eb978d5f30ebefe40f98a82d984cb58_342735_2874x1599_resize_q90_box_3.png width=2874 height=1599 type=image/png><img width=2874 height=1599 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>And thats it! The embeddings are trained, and we could now save them out and use them in a future model where - for some unknown reason, we need to ascribe numeric value to colour names, such that similar colours are close to each other in that vector space.</p><h1 id=3d-embeddings>3D Embeddings</h1><p>Just for fun, lets move to 3D space. Many embeddings are normalised such that the magnitude of each embedded vector is one, so I was tempted to do that for this example&mldr; but then we&rsquo;d be back to a 2D surface (the surface of the unit sphere). So no normalisation for this, but if you wanted to know how to do it, I&rsquo;ve redefined the model and commented out an <code>l2_normalize</code> layer, which would enforce unit vectors across arbitrary dimensions.</p><div class="expanded-code width-80" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>get_model2</span>(embedding_dims<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>3</span>):
</span></span><span style=display:flex><span>    model <span style=color:#ff6ac1>=</span> keras<span style=color:#ff6ac1>.</span>Sequential()
</span></span><span style=display:flex><span>    model<span style=color:#ff6ac1>.</span>add(Embedding(num_colours, embedding_dims, input_length<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>))
</span></span><span style=display:flex><span>    <span style=color:#78787e># model.add(Lambda(lambda x: K.l2_normalize(x, axis=-1), name=&#34;Norm&#34;))</span>
</span></span><span style=display:flex><span>    model<span style=color:#ff6ac1>.</span>add(Lambda(sum_dist, output_shape<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>1</span>,), name<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Dist&#34;</span>))
</span></span><span style=display:flex><span>    model<span style=color:#ff6ac1>.</span>add(Dense(<span style=color:#ff9f43>1</span>, activation<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;sigmoid&#34;</span>))
</span></span><span style=display:flex><span>    <span style=color:#ff5c57>print</span>(model<span style=color:#ff6ac1>.</span>summary())
</span></span><span style=display:flex><span>    model<span style=color:#ff6ac1>.</span>compile(loss<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#39;binary_crossentropy&#39;</span>, optimizer<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;adam&#34;</span>, metrics<span style=color:#ff6ac1>=</span>[<span style=color:#5af78e>&#34;mse&#34;</span>])
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> model
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>weights_3d <span style=color:#ff6ac1>=</span> []
</span></span><span style=display:flex><span>save <span style=color:#ff6ac1>=</span> LambdaCallback(on_epoch_end<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>lambda</span> batch, logs: 
</span></span><span style=display:flex><span>                      weights_3d<span style=color:#ff6ac1>.</span>append(model<span style=color:#ff6ac1>.</span>layers[<span style=color:#ff9f43>0</span>]<span style=color:#ff6ac1>.</span>get_weights()[<span style=color:#ff9f43>0</span>]))
</span></span><span style=display:flex><span>setseed(<span style=color:#ff9f43>1</span>)
</span></span><span style=display:flex><span>model <span style=color:#ff6ac1>=</span> get_model2()
</span></span><span style=display:flex><span>model<span style=color:#ff6ac1>.</span>fit(X, y, epochs<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>500</span>, verbose<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>, batch_size<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>512</span>, callbacks<span style=color:#ff6ac1>=</span>[save]);
</span></span></code></pre></div></div><pre><code>Model: &quot;sequential_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 2, 3)              1938      
_________________________________________________________________
Dist (Lambda)                (None, 1)                 0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 2         
=================================================================
Total params: 1,940
Trainable params: 1,940
Non-trainable params: 0
_________________________________________________________________
None
</code></pre><p>And because the animation code for 3D plots is even uglier than the 2D, I&rsquo;ve hidden it away. But here is the constrained 3D trained embeddings!</p><p>And with the beauty of one more dimension, we can see that the RGB colour clusters can start to be separated independently.</p><h1 id=summary>Summary</h1><p>Hopefully in this small write up you&rsquo;ve seen how we can train embeddings to map categorical features to a coordinate in vector space. In our specific example, we started with a colour dataset, and generated pairs of colours with a label of either 1 for &ldquo;similar&rdquo; or 0 for &ldquo;dissimilar&rdquo;. We then managed to recover the relationship between colours by training an embedding only using this information.</p><p>If you want to read more on how to use embeddings in Keras, Jason Brownlee has some great write ups:</p><ul><li><a href=https://machinelearningmastery.com/what-are-word-embeddings/>What are word embeddings for text?</a></li><li><a href=https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/>How to Use Word Embeddings Layers for Deep Learning with Keras</a></li><li><a href=https://machinelearningmastery.com/develop-word-embeddings-python-gensim/>How to Develop Word Embeddings in Python with Gensim</a></li></ul><p>Have fun!</p><hr><p>For your convenience, here&rsquo;s the code in one block:</p><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>import</span> pandas <span style=color:#ff6ac1>as</span> pd
</span></span><span style=display:flex><span><span style=color:#ff6ac1>import</span> numpy <span style=color:#ff6ac1>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df_original <span style=color:#ff6ac1>=</span> pd<span style=color:#ff6ac1>.</span>read_csv(<span style=color:#5af78e>&#34;encoding_colours/colours.csv&#34;</span>)
</span></span><span style=display:flex><span>df_original <span style=color:#ff6ac1>=</span> df_original<span style=color:#ff6ac1>.</span>dropna(subset<span style=color:#ff6ac1>=</span>[<span style=color:#5af78e>&#34;Color Name&#34;</span>])
</span></span><span style=display:flex><span>num_colours <span style=color:#ff6ac1>=</span> df_original<span style=color:#ff6ac1>.</span>shape[<span style=color:#ff9f43>0</span>]
</span></span><span style=display:flex><span><span style=color:#ff5c57>print</span>(<span style=color:#5af78e>f</span><span style=color:#5af78e>&#34;We have </span><span style=color:#5af78e>{</span>num_colours<span style=color:#5af78e>}</span><span style=color:#5af78e> colours&#34;</span>)
</span></span><span style=display:flex><span>df_original<span style=color:#ff6ac1>.</span>sample(<span style=color:#ff9f43>5</span>)
</span></span><span style=display:flex><span>df <span style=color:#ff6ac1>=</span> df_original<span style=color:#ff6ac1>.</span>loc[:, [<span style=color:#5af78e>&#34;Color Name&#34;</span>, <span style=color:#5af78e>&#34;R;G;B Dec&#34;</span>]]
</span></span><span style=display:flex><span>df[[<span style=color:#5af78e>&#34;r&#34;</span>, <span style=color:#5af78e>&#34;g&#34;</span>, <span style=color:#5af78e>&#34;b&#34;</span>]] <span style=color:#ff6ac1>=</span> df[<span style=color:#5af78e>&#34;R;G;B Dec&#34;</span>]<span style=color:#ff6ac1>.</span>str<span style=color:#ff6ac1>.</span>split(<span style=color:#5af78e>&#34;;&#34;</span>, expand<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)<span style=color:#ff6ac1>.</span>astype(<span style=color:#ff5c57>int</span>) <span style=color:#ff6ac1>/</span> <span style=color:#ff9f43>255</span>
</span></span><span style=display:flex><span>df <span style=color:#ff6ac1>=</span> df<span style=color:#ff6ac1>.</span>drop(columns<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;R;G;B Dec&#34;</span>)
</span></span><span style=display:flex><span>df <span style=color:#ff6ac1>=</span> df<span style=color:#ff6ac1>.</span>rename(columns<span style=color:#ff6ac1>=</span>{<span style=color:#5af78e>&#34;Color Name&#34;</span>: <span style=color:#5af78e>&#34;name&#34;</span>})
</span></span><span style=display:flex><span>df <span style=color:#ff6ac1>=</span> df<span style=color:#ff6ac1>.</span>reset_index(drop<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span>df<span style=color:#ff6ac1>.</span>sample(<span style=color:#ff9f43>10</span>)
</span></span><span style=display:flex><span>df[<span style=color:#5af78e>&#34;num&#34;</span>] <span style=color:#ff6ac1>=</span> df<span style=color:#ff6ac1>.</span>index
</span></span><span style=display:flex><span>df<span style=color:#ff6ac1>.</span>head(<span style=color:#ff9f43>10</span>)
</span></span><span style=display:flex><span>n <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>100000</span> <span style=color:#78787e># Num samples</span>
</span></span><span style=display:flex><span>colour_1 <span style=color:#ff6ac1>=</span> df<span style=color:#ff6ac1>.</span>sample(n<span style=color:#ff6ac1>=</span>n, replace<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>, random_state<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>)<span style=color:#ff6ac1>.</span>reset_index(drop<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span>colour_2 <span style=color:#ff6ac1>=</span> df<span style=color:#ff6ac1>.</span>sample(n<span style=color:#ff6ac1>=</span>n, replace<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>, random_state<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>42</span>)<span style=color:#ff6ac1>.</span>reset_index(drop<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span>c <span style=color:#ff6ac1>=</span> colour_1<span style=color:#ff6ac1>.</span>merge(colour_2, left_index<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>, right_index<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span>c[[<span style=color:#5af78e>&#34;name_x&#34;</span>, <span style=color:#5af78e>&#34;name_y&#34;</span>]]<span style=color:#ff6ac1>.</span>sample(<span style=color:#ff9f43>10</span>)
</span></span><span style=display:flex><span><span style=color:#78787e># Calculate the distance, and drop the RGB columns.</span>
</span></span><span style=display:flex><span>c[<span style=color:#5af78e>&#34;diff&#34;</span>] <span style=color:#ff6ac1>=</span> ((c<span style=color:#ff6ac1>.</span>r_x <span style=color:#ff6ac1>-</span> c<span style=color:#ff6ac1>.</span>r_y)<span style=color:#ff6ac1>**</span><span style=color:#ff9f43>2</span> <span style=color:#ff6ac1>+</span> (c<span style=color:#ff6ac1>.</span>g_x <span style=color:#ff6ac1>-</span> c<span style=color:#ff6ac1>.</span>g_y)<span style=color:#ff6ac1>**</span><span style=color:#ff9f43>2</span> <span style=color:#ff6ac1>+</span> (c<span style=color:#ff6ac1>.</span>b_x <span style=color:#ff6ac1>-</span> c<span style=color:#ff6ac1>.</span>b_y)<span style=color:#ff6ac1>**</span><span style=color:#ff9f43>2</span>) <span style=color:#ff6ac1>/</span> <span style=color:#ff9f43>3</span>
</span></span><span style=display:flex><span>c <span style=color:#ff6ac1>=</span> c<span style=color:#ff6ac1>.</span>drop(columns<span style=color:#ff6ac1>=</span>[<span style=color:#5af78e>&#34;r_x&#34;</span>, <span style=color:#5af78e>&#34;r_y&#34;</span>, <span style=color:#5af78e>&#34;g_x&#34;</span>, <span style=color:#5af78e>&#34;g_y&#34;</span>, <span style=color:#5af78e>&#34;b_x&#34;</span>, <span style=color:#5af78e>&#34;b_y&#34;</span>])
</span></span><span style=display:flex><span>c
</span></span><span style=display:flex><span>np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>seed(<span style=color:#ff9f43>0</span>)
</span></span><span style=display:flex><span>c[<span style=color:#5af78e>&#34;predict&#34;</span>] <span style=color:#ff6ac1>=</span> (c[<span style=color:#5af78e>&#34;diff&#34;</span>] <span style=color:#ff6ac1>&lt;</span> <span style=color:#ff9f43>0.2</span> <span style=color:#ff6ac1>*</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>random(c<span style=color:#ff6ac1>.</span>shape[<span style=color:#ff9f43>0</span>]) <span style=color:#ff6ac1>**</span> <span style=color:#ff9f43>2</span>)<span style=color:#ff6ac1>.</span>astype(<span style=color:#ff5c57>int</span>)
</span></span><span style=display:flex><span><span style=color:#ff5c57>print</span>(<span style=color:#5af78e>f</span><span style=color:#5af78e>&#34;</span><span style=color:#5af78e>{</span><span style=color:#ff9f43>100</span> <span style=color:#ff6ac1>*</span> c<span style=color:#ff6ac1>.</span>predict<span style=color:#ff6ac1>.</span>mean()<span style=color:#5af78e>:</span><span style=color:#5af78e> 0.1f</span><span style=color:#5af78e>}</span><span style=color:#5af78e>% positive values&#34;</span>)
</span></span><span style=display:flex><span>c<span style=color:#ff6ac1>.</span>sample(<span style=color:#ff9f43>10</span>)
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> tensorflow.random <span style=color:#ff6ac1>import</span> set_seed
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> tensorflow <span style=color:#ff6ac1>import</span> keras
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> tensorflow.keras.layers <span style=color:#ff6ac1>import</span> Embedding, Dense, Lambda, Input, Subtract
</span></span><span style=display:flex><span><span style=color:#ff6ac1>import</span> tensorflow.keras.backend <span style=color:#ff6ac1>as</span> K
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> tensorflow.keras.callbacks <span style=color:#ff6ac1>import</span> LambdaCallback
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>sum_dist</span>(x):
</span></span><span style=display:flex><span>    n <span style=color:#ff6ac1>=</span> K<span style=color:#ff6ac1>.</span>permute_dimensions(x, pattern<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>0</span>, <span style=color:#ff9f43>2</span>))
</span></span><span style=display:flex><span>    a, b <span style=color:#ff6ac1>=</span> n[<span style=color:#ff9f43>0</span>], n[<span style=color:#ff9f43>1</span>]
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> K<span style=color:#ff6ac1>.</span>sum((a <span style=color:#ff6ac1>-</span> b)<span style=color:#ff6ac1>**</span><span style=color:#ff9f43>2</span>, axis<span style=color:#ff6ac1>=-</span><span style=color:#ff9f43>1</span>, keepdims<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>get_model</span>(embedding_dims<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>):
</span></span><span style=display:flex><span>    model <span style=color:#ff6ac1>=</span> keras<span style=color:#ff6ac1>.</span>Sequential()
</span></span><span style=display:flex><span>    model<span style=color:#ff6ac1>.</span>add(Embedding(num_colours, embedding_dims, input_length<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>))
</span></span><span style=display:flex><span>    model<span style=color:#ff6ac1>.</span>add(Lambda(sum_dist, output_shape<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>1</span>,), name<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Dist&#34;</span>))
</span></span><span style=display:flex><span>    model<span style=color:#ff6ac1>.</span>add(Dense(<span style=color:#ff9f43>1</span>, activation<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;sigmoid&#34;</span>))
</span></span><span style=display:flex><span>    <span style=color:#ff5c57>print</span>(model<span style=color:#ff6ac1>.</span>summary())
</span></span><span style=display:flex><span>    model<span style=color:#ff6ac1>.</span>compile(loss<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#39;binary_crossentropy&#39;</span>, optimizer<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;adam&#34;</span>, metrics<span style=color:#ff6ac1>=</span>[<span style=color:#5af78e>&#34;mse&#34;</span>])
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> model
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>setseed</span>(i):
</span></span><span style=display:flex><span>    np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>seed(i), set_seed(i)
</span></span><span style=display:flex><span>weights <span style=color:#ff6ac1>=</span> []
</span></span><span style=display:flex><span>save <span style=color:#ff6ac1>=</span> LambdaCallback(on_epoch_end<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>lambda</span> batch, logs: 
</span></span><span style=display:flex><span>                      weights<span style=color:#ff6ac1>.</span>append(model<span style=color:#ff6ac1>.</span>layers[<span style=color:#ff9f43>0</span>]<span style=color:#ff6ac1>.</span>get_weights()[<span style=color:#ff9f43>0</span>]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X, y <span style=color:#ff6ac1>=</span> c[[<span style=color:#5af78e>&#34;num_x&#34;</span>, <span style=color:#5af78e>&#34;num_y&#34;</span>]], c[<span style=color:#5af78e>&#34;predict&#34;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>setseed(<span style=color:#ff9f43>7</span>)
</span></span><span style=display:flex><span>model <span style=color:#ff6ac1>=</span> get_model()
</span></span><span style=display:flex><span>model<span style=color:#ff6ac1>.</span>fit(X, y, epochs<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>500</span>, verbose<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>, batch_size<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>512</span>, callbacks<span style=color:#ff6ac1>=</span>[save]);
</span></span><span style=display:flex><span><span style=color:#ff6ac1>%%</span>capture
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> matplotlib.animation <span style=color:#ff6ac1>import</span> FuncAnimation
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> IPython.display <span style=color:#ff6ac1>import</span> HTML
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Get the list of colours</span>
</span></span><span style=display:flex><span>cs <span style=color:#ff6ac1>=</span> df[[<span style=color:#5af78e>&#34;r&#34;</span>, <span style=color:#5af78e>&#34;g&#34;</span>, <span style=color:#5af78e>&#34;b&#34;</span>]]<span style=color:#ff6ac1>.</span>to_numpy()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Create the plots</span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>with</span> plt<span style=color:#ff6ac1>.</span>style<span style=color:#ff6ac1>.</span>context(<span style=color:#5af78e>&#34;default&#34;</span>):
</span></span><span style=display:flex><span>    fig, ax <span style=color:#ff6ac1>=</span> plt<span style=color:#ff6ac1>.</span>subplots(figsize<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>10</span>, <span style=color:#ff9f43>5</span>))
</span></span><span style=display:flex><span>    fig<span style=color:#ff6ac1>.</span>subplots_adjust(left<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.1</span>, right<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.9</span>, bottom<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.15</span>, top<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.9</span>)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>set_xlabel(<span style=color:#5af78e>&#34;$x_0$&#34;</span>), ax<span style=color:#ff6ac1>.</span>set_ylabel(<span style=color:#5af78e>&#34;$x_1$&#34;</span>)
</span></span><span style=display:flex><span>    ax<span style=color:#ff6ac1>.</span>set_title(<span style=color:#5af78e>&#34;Training of 2D colour embeddings&#34;</span>)
</span></span><span style=display:flex><span>    scat <span style=color:#ff6ac1>=</span> ax<span style=color:#ff6ac1>.</span>scatter(weights[<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>][:, <span style=color:#ff9f43>0</span>], weights[<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>][:, <span style=color:#ff9f43>1</span>], color<span style=color:#ff6ac1>=</span>cs, s<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>10</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>init</span>():
</span></span><span style=display:flex><span>        ax<span style=color:#ff6ac1>.</span>set_xlim(weights[<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>][:,<span style=color:#ff9f43>0</span>]<span style=color:#ff6ac1>.</span>min(), weights[<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>][:,<span style=color:#ff9f43>0</span>]<span style=color:#ff6ac1>.</span>max())
</span></span><span style=display:flex><span>        ax<span style=color:#ff6ac1>.</span>set_ylim(weights[<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>][:,<span style=color:#ff9f43>1</span>]<span style=color:#ff6ac1>.</span>min(), weights[<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>][:,<span style=color:#ff9f43>1</span>]<span style=color:#ff6ac1>.</span>max())
</span></span><span style=display:flex><span>        <span style=color:#ff6ac1>return</span> scat,
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>update</span>(i):
</span></span><span style=display:flex><span>        scat<span style=color:#ff6ac1>.</span>set_offsets(weights[i])
</span></span><span style=display:flex><span>        <span style=color:#ff6ac1>return</span> scat,
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#78787e># Split our epochs into frames</span>
</span></span><span style=display:flex><span>    nw <span style=color:#ff6ac1>=</span> <span style=color:#ff5c57>len</span>(weights) <span style=color:#ff6ac1>-</span> <span style=color:#ff9f43>1</span>
</span></span><span style=display:flex><span>    nf, power <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>30</span> <span style=color:#ff6ac1>*</span> <span style=color:#ff9f43>10</span>, <span style=color:#ff9f43>2</span>
</span></span><span style=display:flex><span>    frames <span style=color:#ff6ac1>=</span> pd<span style=color:#ff6ac1>.</span>unique((np<span style=color:#ff6ac1>.</span>linspace(<span style=color:#ff9f43>1</span>, nw<span style=color:#ff6ac1>**</span>(<span style=color:#ff9f43>1</span> <span style=color:#ff6ac1>/</span> power), nf)<span style=color:#ff6ac1>**</span>power)<span style=color:#ff6ac1>.</span>astype(<span style=color:#ff5c57>int</span>))
</span></span><span style=display:flex><span>    ani <span style=color:#ff6ac1>=</span> FuncAnimation(fig, update, frames<span style=color:#ff6ac1>=</span>frames, 
</span></span><span style=display:flex><span>                        init_func<span style=color:#ff6ac1>=</span>init, blit<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>, interval<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>33.3</span>);
</span></span><span style=display:flex><span>ani<span style=color:#ff6ac1>.</span>save(<span style=color:#5af78e>&#39;embed_2d.mp4&#39;</span>, fps<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>30</span>, 
</span></span><span style=display:flex><span>         extra_args<span style=color:#ff6ac1>=</span>[<span style=color:#5af78e>&#39;-vcodec&#39;</span>, <span style=color:#5af78e>&#39;libx264&#39;</span>, <span style=color:#5af78e>&#39;-crf&#39;</span>, <span style=color:#5af78e>&#39;26&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># If you&#39;re running this in a Jupyter notebook, the below will do the </span>
</span></span><span style=display:flex><span><span style=color:#78787e># same without saving it to file</span>
</span></span><span style=display:flex><span><span style=color:#78787e># HTML(ani.to_html5_video())</span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> IPython.display <span style=color:#ff6ac1>import</span> Video
</span></span><span style=display:flex><span>Video(<span style=color:#5af78e>&#34;embed_2d.mp4&#34;</span>)
</span></span><span style=display:flex><span>fig, ax <span style=color:#ff6ac1>=</span> plt<span style=color:#ff6ac1>.</span>subplots()
</span></span><span style=display:flex><span>scat <span style=color:#ff6ac1>=</span> ax<span style=color:#ff6ac1>.</span>scatter(weights[<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>][:, <span style=color:#ff9f43>0</span>], weights[<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>][:, <span style=color:#ff9f43>1</span>], color<span style=color:#ff6ac1>=</span>cs, s<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>20</span>)
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>set_xlabel(<span style=color:#5af78e>&#34;$x_0$&#34;</span>), ax<span style=color:#ff6ac1>.</span>set_ylabel(<span style=color:#5af78e>&#34;$x_1$&#34;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>set_title(<span style=color:#5af78e>&#34;2D Colour Embeddings&#34;</span>);
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>get_model2</span>(embedding_dims<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>3</span>):
</span></span><span style=display:flex><span>    model <span style=color:#ff6ac1>=</span> keras<span style=color:#ff6ac1>.</span>Sequential()
</span></span><span style=display:flex><span>    model<span style=color:#ff6ac1>.</span>add(Embedding(num_colours, embedding_dims, input_length<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>))
</span></span><span style=display:flex><span>    <span style=color:#78787e># model.add(Lambda(lambda x: K.l2_normalize(x, axis=-1), name=&#34;Norm&#34;))</span>
</span></span><span style=display:flex><span>    model<span style=color:#ff6ac1>.</span>add(Lambda(sum_dist, output_shape<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>1</span>,), name<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Dist&#34;</span>))
</span></span><span style=display:flex><span>    model<span style=color:#ff6ac1>.</span>add(Dense(<span style=color:#ff9f43>1</span>, activation<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;sigmoid&#34;</span>))
</span></span><span style=display:flex><span>    <span style=color:#ff5c57>print</span>(model<span style=color:#ff6ac1>.</span>summary())
</span></span><span style=display:flex><span>    model<span style=color:#ff6ac1>.</span>compile(loss<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#39;binary_crossentropy&#39;</span>, optimizer<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;adam&#34;</span>, metrics<span style=color:#ff6ac1>=</span>[<span style=color:#5af78e>&#34;mse&#34;</span>])
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> model
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>weights_3d <span style=color:#ff6ac1>=</span> []
</span></span><span style=display:flex><span>save <span style=color:#ff6ac1>=</span> LambdaCallback(on_epoch_end<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>lambda</span> batch, logs: 
</span></span><span style=display:flex><span>                      weights_3d<span style=color:#ff6ac1>.</span>append(model<span style=color:#ff6ac1>.</span>layers[<span style=color:#ff9f43>0</span>]<span style=color:#ff6ac1>.</span>get_weights()[<span style=color:#ff9f43>0</span>]))
</span></span><span style=display:flex><span>setseed(<span style=color:#ff9f43>1</span>)
</span></span><span style=display:flex><span>model <span style=color:#ff6ac1>=</span> get_model2()
</span></span><span style=display:flex><span>model<span style=color:#ff6ac1>.</span>fit(X, y, epochs<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>500</span>, verbose<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>, batch_size<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>512</span>, callbacks<span style=color:#ff6ac1>=</span>[save]);
</span></span></code></pre></div><div class="relative max-w-xl mx-auto my-20"><div class="fancy_card horizontal"><div class=card_translator><div class="card_rotator tiny_rot card_layer"><div class="card_layer newsletter p-2"><div class="newsletter-inner h-full"><div class="relative h-full sib-form-container" id=sib-form-container><div class="mb-6 lg:mb-12 text-center"><h3 class="text-main-200 mb-2 lg:mb-8">Stay in the loop!</h3><p class="text-main-100 text-lg text-opacity-70">For new releases, exclusive giveaways, and reviews.</p></div><form action="https://Cosmiccoding.us5.list-manage.com/subscribe/post?u=aebe5de1d2e40dccb3aeca491&id=3987dd4a1f" method=post id=mc-embedded-subscribe-form name=mc-embedded-subscribe-form class="validate w-full" target=_blank novalidate><div><input type=email class="w-full appearance-none bg-main-600 mb-4 border border-main-600 bg-opacity-5 focus:border-main-300 rounded-sm px-4 py-3 text-main-100 placeholder-main-100 required" placeholder="Your best email…" aria-label="Your best email…" name=EMAIL required data-required=true id=EMAIL><div style=position:absolute;left:-5000px aria-hidden=true><input type=text name=b_aebe5de1d2e40dccb3aeca491_3987dd4a1f tabindex=-1></div><input type=submit value=Subscribe name=subscribe id=mc-embedded-subscribe class="button btn bg-main-600 hover:bg-main-400 shadow w-full"></div><p id=mce-error-response style=display:none class="text-center mt-2 opacity-50 text-main-200">There was a problem, please try again.</p><p id=mce-success-response style=display:none class="text-center mt-2 opacity-50 text-main-200">Thanks mate, won't let ya down.</p></form></div></div></div><div class="card_layer card_effect card_soft_glare" style=pointer-events:none></div></div></div></div></div></div></div><script language=javascript type=text/javascript src=https://cosmiccoding.com.au/js/main.min.11673d10a962fb5205229607e62ea1d23424d35dad33db7bfe2a09a63d5409fa.js></script>
<script async defer src="https://www.googletagmanager.com/gtag/js?id=UA-72691106-1"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","UA-72691106-1")</script></div></body></html>