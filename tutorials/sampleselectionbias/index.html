<!doctype html><html lang=en-gb><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>Bayesian Sample Selection Effects - Samuel Hinton</title><link rel=stylesheet href="https://cosmiccoding.com.au/css/main.min.7fbf65bef988e0cecd6d148a59756085deed3c480729d260d0a30b282b1d7da8.css" integrity="sha256-f79lvvmI4M7NbRSKWXVghd7tPEgHKdJg0KMLKCsdfag="><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel="shortcut icon" href=https://cosmiccoding.com.au/img/favicon.png type=image/x-icon></head><meta name=description content="Sample bias and selection effects are the worst. Here's one solution."><meta name=robots content="noodp"><link rel=canonical href=https://cosmiccoding.com.au/tutorials/sampleselectionbias/><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://cosmiccoding.com.au/tutorials/sampleselectionbias/cover.png"><meta name=twitter:title content="Bayesian Sample Selection Effects"><meta name=twitter:description content="Sample bias and selection effects are the worst. Here's one solution."><meta property="og:title" content="Bayesian Sample Selection Effects"><meta property="og:description" content="Sample bias and selection effects are the worst. Here's one solution."><meta property="og:type" content="article"><meta property="og:url" content="https://cosmiccoding.com.au/tutorials/sampleselectionbias/"><meta property="og:image" content="https://cosmiccoding.com.au/tutorials/sampleselectionbias/cover.png"><meta property="article:section" content="tutorials"><meta property="article:published_time" content="2019-07-30T00:00:00+00:00"><meta property="article:modified_time" content="2019-07-30T00:00:00+00:00"><meta property="article:section" content="tutorial"><meta property="article:published_time" content="2019-07-30 00:00:00 +0000 UTC"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Bayesian Sample Selection Effects","genre":"tutorial","url":"https:\/\/cosmiccoding.com.au\/tutorials\/sampleselectionbias\/","datePublished":"2019-07-30 00:00:00 \u002b0000 UTC","description":"Sample bias and selection effects are the worst. Here\u0027s one solution.","author":{"@type":"Person","name":""}}</script><body><div class="flex flex-col min-h-screen overflow-hidden"><header class="absolute w-full z-30"><div class="max-w-6xl mx-auto px-4 sm:px-6"><div class="flex items-center justify-between h-20"><div class="flex-shrink-0 mr-4"><a class=block href=/ aria-label="Samuel Hinton"><h2 class=logo>SRH</h2></a></div><nav class="hidden md:flex md:flex-grow"><ul class="flex flex-grow justify-end flex-wrap items-center"><li><a href=/#books class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Books</a></li><li><a href=/reviews class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Reviews</a></li><li><a href=/tutorials class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Tutorials</a></li><li><a href=/blogs class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Blog</a></li><li><a href=/#courses class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Courses</a></li><li><a href=/static/resume/HintonCV.pdf class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">CV</a></li></ul></nav><div class=md:hidden x-data="{ expanded: false }"><button class=hamburger :class="{ 'active': expanded }" @click.stop="expanded = !expanded" aria-controls=mobile-nav :aria-expanded=expanded>
<span class=sr-only>Menu</span><svg class="w-6 h-6 fill-current text-gray-300 hover:text-gray-200 transition duration-150 ease-in-out" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><rect y="4" width="24" height="2" rx="1"/><rect y="11" width="24" height="2" rx="1"/><rect y="18" width="24" height="2" rx="1"/></svg></button><nav id=mobile-nav class="absolute top-full z-20 left-0 w-full px-4 sm:px-6 overflow-hidden transition-all duration-300 ease-in-out" x-ref=mobileNav :style="expanded ? 'max-height: ' + $refs.mobileNav.scrollHeight + 'px; opacity: 1' : 'max-height: 0; opacity: .8'" @click.away="expanded = false" @keydown.escape.window="expanded = false" x-cloak><ul class="bg-gray-800 px-4 py-2"><li><a href=/#books class="flex text-gray-300 hover:text-gray-200 py-2">Books</a></li><li><a href=/reviews class="flex text-gray-300 hover:text-gray-200 py-2">Reviews</a></li><li><a href=/tutorials class="flex text-gray-300 hover:text-gray-200 py-2">Tutorials</a></li><li><a href=/blogs class="flex text-gray-300 hover:text-gray-200 py-2">Blog</a></li><li><a href=/#courses class="flex text-gray-300 hover:text-gray-200 py-2">Courses</a></li><li><a href=/static/resume/HintonCV.pdf class="flex text-gray-300 hover:text-gray-200 py-2">CV</a></li></ul></nav></div></div></div></header><div class=flex-grow><div id=post-container class="content content-wider blog-post relative"><div class="section-header blog"><h1 class=title>Bayesian Sample Selection Effects</h1><p>6th July 2019</p><p>Sample bias and selection effects are the worst. Here's one solution.</p></div><div><ul class="grid gap-6 w-full md:grid-cols-2" style=list-style:none;padding-left:0><li><input type=radio id=show-code name=code-toggle value=show-code class="hidden peer" onchange=clickCheckbox(this) required checked>
<label for=show-code class="inline-flex justify-between items-center p-5 w-full text-gray-500 bg-gray-800 rounded-lg border border-gray-200 cursor-pointer peer-checked:border-2 peer-checked:border-main-300 peer-checked:text-white peer-checked:bg-gray-700 hover:text-gray-200 hover:bg-gray-700"><div class="block w-full"><div class="w-full text-center text-lg font-semibold">Show me everything!</div><div class="w-full text-center text-sm">Oh yeah, coding time.</div></div></label></li><li><input type=radio id=hide-code name=code-toggle value=hide-code class="hidden peer" onchange=clickCheckbox(this)>
<label for=hide-code class="inline-flex justify-between items-center p-5 w-full text-gray-500 bg-gray-800 rounded-lg border border-gray-200 cursor-pointer peer-checked:border-2 peer-checked:border-main-300 peer-checked:text-white peer-checked:bg-gray-700 hover:text-gray-200 hover:bg-gray-700"><div class="block w-full"><div class="w-full text-center text-lg font-semibold">Just the plots</div><div class="w-full text-center text-sm">Code is nasty.</div></div></label></li></ul></div><script>function clickCheckbox(e){e.id=="show-code"?document.getElementById("post-container").classList.remove("hide-code"):document.getElementById("post-container").classList.add("hide-code")}</script><p>In a perfect world our experiments would capture all the data that exists. This is not a perfect world, and we miss a lot of data. Let&rsquo;s consider one method of accounting for this in a Bayesian formalism - integrating it out.</p><p>Let&rsquo;s begin with a motivational dataset.</p><p><div><figure class="img-main rounded"><picture><source srcset=/tutorials/sampleselectionbias/cover_hu4b292e01358f3b18db68be449ca1a009_156239_1920x0_resize_q90_h2_box_3.webp width=1920 height=1121 type=image/webp><source srcset=/tutorials/sampleselectionbias/cover.png width=2560 height=1495 type=image/png><img width=2560 height=1495 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>So it looks like for our example data, we&rsquo;ve got some gaussian-like distribution of $x$ observations, but at some point it seems like our instrument is unable to pick up the observations. Maybe its brightness and its too dim! Or maybe something else, who knows! But regardless, we can work with this.</p><p>To start at the beginning, let&rsquo;s write out the full formula for our posterior:</p><p>$$ P(\theta|d) = \frac{P(d|\theta) P(\theta)}{P(d)} $$</p><p>Everything looks good, so let&rsquo;s home in here on the likelihood. Now, what we should also do to make our lives easier if we write out the fact we have some sort of <em>selection effect</em> applying to our model. That is, some separate probability that dictates our experiment successfully observes an event which actually happened.</p><p>$$ P(d|\theta) \rightarrow P(d|\theta, S), $$</p><p>where $S$ in colloquial English represents &ldquo;we successfully observed the event&rdquo;. Now, we can normally write our selection probability given data or model easily, so we want to get things into a state where we have $P(S|d,\theta)$. Via some probability manipulation we can twist this around and get the following:</p><p>$$ P(d|\theta, S) = \frac{P(S|d,\theta) P(d|\theta)}{P(S|\theta)} $$</p><p>So let&rsquo;s break that down. Our likelihood given our model and our selection effects is given by the chance we observed our experimenal data (which is going to be $1$ for deterministic processes given we <strong>have</strong> observed it already) multiplied by our standard likelihood where we ignore selection effects, divided by the chance of observing data in general at the area in parameter space.</p><p>Now the denominator here cannot be evaluated in its current state, we need to introduce an integral over all data (denoted $D$) such that we can apply the selection effect on it.</p><p>$$ P(d|\theta, S) = \frac{P(S|d,\theta) P(d|\theta)}{\int P(S|D, \theta) P(D|\theta) dD} $$</p><h2 id=the-simplest-gaussian-example>The Simplest Gaussian Example</h2><p>Let&rsquo;s assume we have data samples of some observable $d$. In our model, $d$ is drawn from a normal distribution such that we wish to characterise two model parameters describing said normal - the mean $\mu$ and the standard deviation $\sigma$.</p><p>$$ P(d|\mu, \sigma) = \mathcal{N}(d|\mu, \sigma) $$</p><p>Now let&rsquo;s imagine the case as described in our data where we can only observe some value when its above a threshold of $\alpha$. Or more formally,</p><p>$$P(S|d, \theta) = \mathcal{H}(d-\alpha),$$</p><p>where $\mathcal{H}$ is the Heaviside step function:</p><p>$$ \mathcal{H}(y)\equiv \begin{cases}
1 \quad {\rm if }\ y \ge 0 \
0 \quad {\rm otherwise.}
\end{cases} $$</p><p>You can see that this selection probability is deterministic, so any data we did observe we observed with a probability of one. This helps simplify the numerator such that $P(S|d,\theta) = 1$. Which just leaves us the denominator:</p><p>$$ \int P(S|D, \theta) P(D|\mu, \sigma) dD = \int \mathcal{H}(d-\alpha) \mathcal{N}(D|\mu, \sigma) dD $$</p><p>And because the Heaviside step function sets all $d&lt;\alpha$ to zero, we can simply modify the bounds of the integral and do this analytically:</p><p>$$\begin{align}
\int \mathcal{H}(d-\alpha) P(D|\mu, \sigma) dD &= \int_{\alpha}^\infty \mathcal{N}(D|\mu, \sigma), dD \ &= \frac{1}{2} {\rm erfc}\left[ \frac{\alpha - \mu}{\sqrt{2}\sigma} \right]
\end{align}$$</p><p>So this means we can throw this denominator back into our full expression for the likelihood:</p><p>$$ P(d|\mu, \sigma, S) = \frac{2\mathcal{N}(d|\mu, \sigma)}{ {\rm erfc}\left[ \frac{\alpha - \mu}{\sqrt{2}\sigma} \right]} $$</p><p>Finally, we should note this likelihood (and correction) is for one data point. If we had a hundred data points, we&rsquo;d do this multiplicatively for each $d$.</p><h2 id=verifying-the-selection-corrected-likelihood>Verifying the selection-corrected likelihood</h2><p>To do this, lets first generate a dataset, create a model, fit it with <code>emcee</code> and verify that our estimations of $\mu$ and $\sigma$ are unbiased. First, the dataset.</p><div class="reduced-code width-52" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>import</span> numpy <span style=color:#ff6ac1>as</span> np
</span></span><span style=display:flex><span>np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>seed(<span style=color:#ff9f43>3</span>)
</span></span><span style=display:flex><span>mu, sigma, alpha, num_points <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>100</span>, <span style=color:#ff9f43>10</span>, <span style=color:#ff9f43>85</span>, <span style=color:#ff9f43>1000</span>
</span></span><span style=display:flex><span>d_all <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>normal(mu, sigma, size<span style=color:#ff6ac1>=</span>num_points)
</span></span><span style=display:flex><span>d <span style=color:#ff6ac1>=</span> d_all[d_all <span style=color:#ff6ac1>&gt;</span> alpha]
</span></span></code></pre></div></div><p>This is the same code used to generat the plot you saw up the top, just with less datapoints! Let&rsquo;s create our model, one with the sample selection, and one without. Remember, we work in log space for probability, so that you don&rsquo;t get tripped up when reading the code implementation of the math above.</p><div class="expanded-code width-82" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>from</span> scipy.stats <span style=color:#ff6ac1>import</span> norm
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> scipy.special <span style=color:#ff6ac1>import</span> erfc
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>uncorrected_likelihood</span>(xs, data):
</span></span><span style=display:flex><span>    mu, sigma <span style=color:#ff6ac1>=</span> xs
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>if</span> sigma <span style=color:#ff6ac1>&lt;</span> <span style=color:#ff9f43>0</span>:
</span></span><span style=display:flex><span>        <span style=color:#ff6ac1>return</span> <span style=color:#ff6ac1>-</span>np<span style=color:#ff6ac1>.</span>inf
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> norm(mu, sigma)<span style=color:#ff6ac1>.</span>logpdf(data)<span style=color:#ff6ac1>.</span>sum()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>corrected_likelihood</span>(xs, data):
</span></span><span style=display:flex><span>    mu, sigma <span style=color:#ff6ac1>=</span> xs
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>if</span> sigma <span style=color:#ff6ac1>&lt;</span> <span style=color:#ff9f43>0</span>:
</span></span><span style=display:flex><span>        <span style=color:#ff6ac1>return</span> <span style=color:#ff6ac1>-</span>np<span style=color:#ff6ac1>.</span>inf
</span></span><span style=display:flex><span>    correction <span style=color:#ff6ac1>=</span> data<span style=color:#ff6ac1>.</span>size <span style=color:#ff6ac1>*</span> np<span style=color:#ff6ac1>.</span>log(<span style=color:#ff9f43>0.5</span> <span style=color:#ff6ac1>*</span> erfc((alpha <span style=color:#ff6ac1>-</span> mu)<span style=color:#ff6ac1>/</span>(np<span style=color:#ff6ac1>.</span>sqrt(<span style=color:#ff9f43>2</span>) <span style=color:#ff6ac1>*</span> sigma)))
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> uncorrected_likelihood(xs, data) <span style=color:#ff6ac1>-</span> correction
</span></span></code></pre></div></div><p>Note here that I&rsquo;ve been cheeky and included flat priors and a prior boundary to keep $\sigma$ positive in the likehood, which means I should really call it the posterior, but let&rsquo;s not get bogged down on semantics.</p><p>With that, our model is fully defined. We can now try and fit it to the data to see how we go.</p><h3 id=model-fitting>Model Fitting</h3><p>Let&rsquo;s use my go-to solution, <code>emcee</code> to sample our likelihood given our dataset, and <code>ChainConsumer</code> to take those samples and turn them into handy plots. If you want more details check out the <a href=/tutorial/2019/07/27/BayesianLinearRegression.html>Bayesian Linear Regression tutorial</a> for implementation details.</p><div class="expanded-code width-81" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>import</span> emcee
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ndim <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>2</span>
</span></span><span style=display:flex><span>nwalkers <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>50</span>
</span></span><span style=display:flex><span>p0 <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>array([<span style=color:#ff9f43>95</span>, <span style=color:#ff9f43>0</span>]) <span style=color:#ff6ac1>+</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>uniform(low<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>, high<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>10</span>, size<span style=color:#ff6ac1>=</span>(nwalkers, ndim))
</span></span><span style=display:flex><span>results <span style=color:#ff6ac1>=</span> {}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>functions <span style=color:#ff6ac1>=</span> [corrected_likelihood, uncorrected_likelihood]
</span></span><span style=display:flex><span>names <span style=color:#ff6ac1>=</span> [<span style=color:#5af78e>&#34;Corrected Likelihood&#34;</span>, <span style=color:#5af78e>&#34;Uncorrected Likelihood&#34;</span>]
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> fn, name <span style=color:#ff6ac1>in</span> <span style=color:#ff5c57>zip</span>(functions, names):
</span></span><span style=display:flex><span>    sampler <span style=color:#ff6ac1>=</span> emcee<span style=color:#ff6ac1>.</span>EnsembleSampler(nwalkers, ndim, fn, args<span style=color:#ff6ac1>=</span>[d])
</span></span><span style=display:flex><span>    state <span style=color:#ff6ac1>=</span> sampler<span style=color:#ff6ac1>.</span>run_mcmc(p0, <span style=color:#ff9f43>2000</span>)
</span></span><span style=display:flex><span>    chain <span style=color:#ff6ac1>=</span> sampler<span style=color:#ff6ac1>.</span>chain[:, <span style=color:#ff9f43>300</span>:, :]
</span></span><span style=display:flex><span>    flat_chain <span style=color:#ff6ac1>=</span> chain<span style=color:#ff6ac1>.</span>reshape((<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>, ndim))
</span></span><span style=display:flex><span>    results[name] <span style=color:#ff6ac1>=</span> flat_chain
</span></span></code></pre></div></div><p>And now to plot these samples:</p><div class=width-72 markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>from</span> chainconsumer <span style=color:#ff6ac1>import</span> ChainConsumer
</span></span><span style=display:flex><span>c <span style=color:#ff6ac1>=</span> ChainConsumer()
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> name, flat_chain <span style=color:#ff6ac1>in</span> results<span style=color:#ff6ac1>.</span>items():
</span></span><span style=display:flex><span>    c<span style=color:#ff6ac1>.</span>add_chain(flat_chain, parameters<span style=color:#ff6ac1>=</span>[<span style=color:#5af78e>&#34;$\mu$&#34;</span>, <span style=color:#5af78e>&#34;$\sigma$&#34;</span>], name<span style=color:#ff6ac1>=</span>name)
</span></span><span style=display:flex><span>c<span style=color:#ff6ac1>.</span>configure()
</span></span><span style=display:flex><span>c<span style=color:#ff6ac1>.</span>plotter<span style=color:#ff6ac1>.</span>plot(truth<span style=color:#ff6ac1>=</span>[mu, sigma], figsize<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2.0</span>);
</span></span></code></pre></div></div><p><div><figure class=rounded><picture><source srcset=/tutorials/sampleselectionbias/2019-07-30-SampleSelectionBias_files/2019-07-30-SampleSelectionBias_11_1_hud3629aea017804ffa8f6737333aaca8f_233380_1920x0_resize_q90_h2_box_3.webp width=1920 height=1911 type=image/webp><source srcset=/tutorials/sampleselectionbias/2019-07-30-SampleSelectionBias_files/2019-07-30-SampleSelectionBias_11_1.png width=2344 height=2333 type=image/png><img width=2344 height=2333 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>Hopefully you can now see how the correction we applied to the likelihood unbiases its estimations.</p><p>You&rsquo;ll notice that the contours don&rsquo;t sit perfectly on the true value, but if we made a hundred realisations of the data and averaged out the contour positions, you&rsquo;d see they would. <a href=https://arxiv.org/abs/1706.03856>In fact, you can see it right here</a>.</p><p>Thinking about the problem in this way allows us to neatly separate out the selection effects and generic likelihood so we can treat them independently. Of course, when you get past the point where analytic approximations to your selection effects aren&rsquo;t good enough, you can expect a good numerical hit where you have to numerically compute the correction.</p><p>But one thing that is <em>correct</em> about this approach that a lot of other approaches miss (such as adding bias corrections to your data) is that the <em>correction</em> is dependent on where you are in parameter space. And this should make sense conceptually - the correction is just answering the question &ldquo;How efficient are we <em>in general</em> given our current model parametrisation&rdquo;. If we&rsquo;ve charactered our instrument cannot detect $d &lt; 85$, we expect to lose more events if the population mean is close to $85$ and less events if the population mean is at $200$.</p><hr><p>For your convenience, here&rsquo;s the code in one block:</p><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>import</span> numpy <span style=color:#ff6ac1>as</span> np
</span></span><span style=display:flex><span>np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>seed(<span style=color:#ff9f43>3</span>)
</span></span><span style=display:flex><span>mu, sigma, alpha, num_points <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>100</span>, <span style=color:#ff9f43>10</span>, <span style=color:#ff9f43>85</span>, <span style=color:#ff9f43>1000</span>
</span></span><span style=display:flex><span>d_all <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>normal(mu, sigma, size<span style=color:#ff6ac1>=</span>num_points)
</span></span><span style=display:flex><span>d <span style=color:#ff6ac1>=</span> d_all[d_all <span style=color:#ff6ac1>&gt;</span> alpha]
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> scipy.stats <span style=color:#ff6ac1>import</span> norm
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> scipy.special <span style=color:#ff6ac1>import</span> erfc
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>uncorrected_likelihood</span>(xs, data):
</span></span><span style=display:flex><span>    mu, sigma <span style=color:#ff6ac1>=</span> xs
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>if</span> sigma <span style=color:#ff6ac1>&lt;</span> <span style=color:#ff9f43>0</span>:
</span></span><span style=display:flex><span>        <span style=color:#ff6ac1>return</span> <span style=color:#ff6ac1>-</span>np<span style=color:#ff6ac1>.</span>inf
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> norm(mu, sigma)<span style=color:#ff6ac1>.</span>logpdf(data)<span style=color:#ff6ac1>.</span>sum()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>corrected_likelihood</span>(xs, data):
</span></span><span style=display:flex><span>    mu, sigma <span style=color:#ff6ac1>=</span> xs
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>if</span> sigma <span style=color:#ff6ac1>&lt;</span> <span style=color:#ff9f43>0</span>:
</span></span><span style=display:flex><span>        <span style=color:#ff6ac1>return</span> <span style=color:#ff6ac1>-</span>np<span style=color:#ff6ac1>.</span>inf
</span></span><span style=display:flex><span>    correction <span style=color:#ff6ac1>=</span> data<span style=color:#ff6ac1>.</span>size <span style=color:#ff6ac1>*</span> np<span style=color:#ff6ac1>.</span>log(<span style=color:#ff9f43>0.5</span> <span style=color:#ff6ac1>*</span> erfc((alpha <span style=color:#ff6ac1>-</span> mu)<span style=color:#ff6ac1>/</span>(np<span style=color:#ff6ac1>.</span>sqrt(<span style=color:#ff9f43>2</span>) <span style=color:#ff6ac1>*</span> sigma)))
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> uncorrected_likelihood(xs, data) <span style=color:#ff6ac1>-</span> correction
</span></span><span style=display:flex><span><span style=color:#ff6ac1>import</span> emcee
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ndim <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>2</span>
</span></span><span style=display:flex><span>nwalkers <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>50</span>
</span></span><span style=display:flex><span>p0 <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>array([<span style=color:#ff9f43>95</span>, <span style=color:#ff9f43>0</span>]) <span style=color:#ff6ac1>+</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>uniform(low<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>, high<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>10</span>, size<span style=color:#ff6ac1>=</span>(nwalkers, ndim))
</span></span><span style=display:flex><span>results <span style=color:#ff6ac1>=</span> {}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>functions <span style=color:#ff6ac1>=</span> [corrected_likelihood, uncorrected_likelihood]
</span></span><span style=display:flex><span>names <span style=color:#ff6ac1>=</span> [<span style=color:#5af78e>&#34;Corrected Likelihood&#34;</span>, <span style=color:#5af78e>&#34;Uncorrected Likelihood&#34;</span>]
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> fn, name <span style=color:#ff6ac1>in</span> <span style=color:#ff5c57>zip</span>(functions, names):
</span></span><span style=display:flex><span>    sampler <span style=color:#ff6ac1>=</span> emcee<span style=color:#ff6ac1>.</span>EnsembleSampler(nwalkers, ndim, fn, args<span style=color:#ff6ac1>=</span>[d])
</span></span><span style=display:flex><span>    state <span style=color:#ff6ac1>=</span> sampler<span style=color:#ff6ac1>.</span>run_mcmc(p0, <span style=color:#ff9f43>2000</span>)
</span></span><span style=display:flex><span>    chain <span style=color:#ff6ac1>=</span> sampler<span style=color:#ff6ac1>.</span>chain[:, <span style=color:#ff9f43>300</span>:, :]
</span></span><span style=display:flex><span>    flat_chain <span style=color:#ff6ac1>=</span> chain<span style=color:#ff6ac1>.</span>reshape((<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>, ndim))
</span></span><span style=display:flex><span>    results[name] <span style=color:#ff6ac1>=</span> flat_chain
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> chainconsumer <span style=color:#ff6ac1>import</span> ChainConsumer
</span></span><span style=display:flex><span>c <span style=color:#ff6ac1>=</span> ChainConsumer()
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> name, flat_chain <span style=color:#ff6ac1>in</span> results<span style=color:#ff6ac1>.</span>items():
</span></span><span style=display:flex><span>    c<span style=color:#ff6ac1>.</span>add_chain(flat_chain, parameters<span style=color:#ff6ac1>=</span>[<span style=color:#5af78e>&#34;$\mu$&#34;</span>, <span style=color:#5af78e>&#34;$\sigma$&#34;</span>], name<span style=color:#ff6ac1>=</span>name)
</span></span><span style=display:flex><span>c<span style=color:#ff6ac1>.</span>configure()
</span></span><span style=display:flex><span>c<span style=color:#ff6ac1>.</span>plotter<span style=color:#ff6ac1>.</span>plot(truth<span style=color:#ff6ac1>=</span>[mu, sigma], figsize<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2.0</span>);
</span></span></code></pre></div><div class="relative max-w-xl mx-auto my-20"><div class="fancy_card horizontal"><div class=card_translator><div class="card_rotator tiny_rot card_layer"><div class="card_layer newsletter p-2"><div class="newsletter-inner h-full"><div class="relative h-full sib-form-container" id=sib-form-container><div class="mb-6 lg:mb-12 text-center"><h3 class="text-main-200 mb-2 lg:mb-8">Stay in the loop!</h3><p class="text-main-100 text-lg text-opacity-70">For new releases, exclusive giveaways, and reviews.</p></div><form action="https://Cosmiccoding.us5.list-manage.com/subscribe/post?u=aebe5de1d2e40dccb3aeca491&id=3987dd4a1f" method=post id=mc-embedded-subscribe-form name=mc-embedded-subscribe-form class="validate w-full" target=_blank novalidate><div><input type=email class="w-full appearance-none bg-main-600 mb-4 border border-main-600 bg-opacity-5 focus:border-main-300 rounded-sm px-4 py-3 text-main-100 placeholder-main-100 required" placeholder="Your best email…" aria-label="Your best email…" name=EMAIL required data-required=true id=EMAIL><div style=position:absolute;left:-5000px aria-hidden=true><input type=text name=b_aebe5de1d2e40dccb3aeca491_3987dd4a1f tabindex=-1></div><input type=submit value=Subscribe name=subscribe id=mc-embedded-subscribe class="button btn bg-main-600 hover:bg-main-400 shadow w-full"></div><p id=mce-error-response style=display:none class="text-center mt-2 opacity-50 text-main-200">There was a problem, please try again.</p><p id=mce-success-response style=display:none class="text-center mt-2 opacity-50 text-main-200">Thanks mate, won't let ya down.</p></form></div></div></div><div class="card_layer card_effect card_soft_glare" style=pointer-events:none></div></div></div></div></div></div><script type=text/x-mathjax-config>
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script></div><script language=javascript type=text/javascript src=https://cosmiccoding.com.au/js/main.min.11673d10a962fb5205229607e62ea1d23424d35dad33db7bfe2a09a63d5409fa.js></script>
<script async defer src="https://www.googletagmanager.com/gtag/js?id=G-GRX6QE03YR"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-GRX6QE03YR")</script></div></body></html>