<!doctype html><html lang=en-gb><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>Bayesian Linear Regression in Python - Samuel Hinton</title><link rel=stylesheet href="https://cosmiccoding.com.au/css/main.min.7fbf65bef988e0cecd6d148a59756085deed3c480729d260d0a30b282b1d7da8.css" integrity="sha256-f79lvvmI4M7NbRSKWXVghd7tPEgHKdJg0KMLKCsdfag="><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel="shortcut icon" href=https://cosmiccoding.com.au/img/favicon.png type=image/x-icon></head><meta name=description content="A tutorial from creating data to plotting confidence intervals."><meta name=robots content="noodp"><link rel=canonical href=https://cosmiccoding.com.au/tutorials/bayesianlinearregression/><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://cosmiccoding.com.au/tutorials/bayesianlinearregression/cover.png"><meta name=twitter:title content="Bayesian Linear Regression in Python"><meta name=twitter:description content="A tutorial from creating data to plotting confidence intervals."><meta property="og:title" content="Bayesian Linear Regression in Python"><meta property="og:description" content="A tutorial from creating data to plotting confidence intervals."><meta property="og:type" content="article"><meta property="og:url" content="https://cosmiccoding.com.au/tutorials/bayesianlinearregression/"><meta property="og:image" content="https://cosmiccoding.com.au/tutorials/bayesianlinearregression/cover.png"><meta property="article:section" content="tutorials"><meta property="article:published_time" content="2019-07-27T00:00:00+00:00"><meta property="article:modified_time" content="2019-07-27T00:00:00+00:00"><meta property="article:section" content="tutorial"><meta property="article:published_time" content="2019-07-27 00:00:00 +0000 UTC"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Bayesian Linear Regression in Python","genre":"tutorial","url":"https:\/\/cosmiccoding.com.au\/tutorials\/bayesianlinearregression\/","datePublished":"2019-07-27 00:00:00 \u002b0000 UTC","description":"A tutorial from creating data to plotting confidence intervals.","author":{"@type":"Person","name":""}}</script><body><div class="flex flex-col min-h-screen overflow-hidden"><header class="absolute w-full z-30"><div class="max-w-6xl mx-auto px-4 sm:px-6"><div class="flex items-center justify-between h-20"><div class="flex-shrink-0 mr-4"><a class=block href=/ aria-label="Samuel Hinton"><h2 class=logo>SRH</h2></a></div><nav class="hidden md:flex md:flex-grow"><ul class="flex flex-grow justify-end flex-wrap items-center"><li><a href=/#books class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Books</a></li><li><a href=/reviews class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Reviews</a></li><li><a href=/tutorials class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Tutorials</a></li><li><a href=/blogs class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Blog</a></li><li><a href=/#courses class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">Courses</a></li><li><a href=/static/resume/HintonCV.pdf class="text-gray-300 hover:text-gray-200 px-4 py-2 flex items-center transition duration-150 ease-in-out">CV</a></li></ul></nav><div class=md:hidden x-data="{ expanded: false }"><button class=hamburger :class="{ 'active': expanded }" @click.stop="expanded = !expanded" aria-controls=mobile-nav :aria-expanded=expanded>
<span class=sr-only>Menu</span><svg class="w-6 h-6 fill-current text-gray-300 hover:text-gray-200 transition duration-150 ease-in-out" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><rect y="4" width="24" height="2" rx="1"/><rect y="11" width="24" height="2" rx="1"/><rect y="18" width="24" height="2" rx="1"/></svg></button><nav id=mobile-nav class="absolute top-full z-20 left-0 w-full px-4 sm:px-6 overflow-hidden transition-all duration-300 ease-in-out" x-ref=mobileNav :style="expanded ? 'max-height: ' + $refs.mobileNav.scrollHeight + 'px; opacity: 1' : 'max-height: 0; opacity: .8'" @click.away="expanded = false" @keydown.escape.window="expanded = false" x-cloak><ul class="bg-gray-800 px-4 py-2"><li><a href=/#books class="flex text-gray-300 hover:text-gray-200 py-2">Books</a></li><li><a href=/reviews class="flex text-gray-300 hover:text-gray-200 py-2">Reviews</a></li><li><a href=/tutorials class="flex text-gray-300 hover:text-gray-200 py-2">Tutorials</a></li><li><a href=/blogs class="flex text-gray-300 hover:text-gray-200 py-2">Blog</a></li><li><a href=/#courses class="flex text-gray-300 hover:text-gray-200 py-2">Courses</a></li><li><a href=/static/resume/HintonCV.pdf class="flex text-gray-300 hover:text-gray-200 py-2">CV</a></li></ul></nav></div></div></div></header><div class=flex-grow><div id=post-container class="content content-wider blog-post relative"><div class="section-header blog"><h1 class=title>Bayesian Linear Regression in Python</h1><p>6th July 2019</p><p>A tutorial from creating data to plotting confidence intervals.</p></div><div><ul class="grid gap-6 w-full md:grid-cols-2" style=list-style:none;padding-left:0><li><input type=radio id=show-code name=code-toggle value=show-code class="hidden peer" onchange=clickCheckbox(this) required checked>
<label for=show-code class="inline-flex justify-between items-center p-5 w-full text-gray-500 bg-gray-800 rounded-lg border border-gray-200 cursor-pointer peer-checked:border-2 peer-checked:border-main-300 peer-checked:text-white peer-checked:bg-gray-700 hover:text-gray-200 hover:bg-gray-700"><div class="block w-full"><div class="w-full text-center text-lg font-semibold">Show me everything!</div><div class="w-full text-center text-sm">Oh yeah, coding time.</div></div></label></li><li><input type=radio id=hide-code name=code-toggle value=hide-code class="hidden peer" onchange=clickCheckbox(this)>
<label for=hide-code class="inline-flex justify-between items-center p-5 w-full text-gray-500 bg-gray-800 rounded-lg border border-gray-200 cursor-pointer peer-checked:border-2 peer-checked:border-main-300 peer-checked:text-white peer-checked:bg-gray-700 hover:text-gray-200 hover:bg-gray-700"><div class="block w-full"><div class="w-full text-center text-lg font-semibold">Just the plots</div><div class="w-full text-center text-sm">Code is nasty.</div></div></label></li></ul></div><script>function clickCheckbox(e){e.id=="show-code"?document.getElementById("post-container").classList.remove("hide-code"):document.getElementById("post-container").classList.add("hide-code")}</script><p>Bayesian linear regression is a common topic, but allow me to put my own spin on it. We&rsquo;ll start at generating some data, defining a model, fitting it and plotting the results. It shouldn&rsquo;t take long.</p><h2 id=generating-data>Generating Data</h2><p>Let&rsquo;s start by generating some experimental data. For simplicity, let us assume some underlying process generates samples $f(x) = mx + c$ and our observations have some given Gaussian error $\sigma$.</p><div class=width-65 markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>import</span> matplotlib.pyplot <span style=color:#ff6ac1>as</span> plt
</span></span><span style=display:flex><span><span style=color:#ff6ac1>import</span> numpy <span style=color:#ff6ac1>as</span> np
</span></span><span style=display:flex><span>num_points <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>20</span>
</span></span><span style=display:flex><span>m, c <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>tan(np<span style=color:#ff6ac1>.</span>pi <span style=color:#ff6ac1>/</span> <span style=color:#ff9f43>4</span>), <span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>  <span style=color:#78787e># So our angle is 45 degrees, m = 1</span>
</span></span><span style=display:flex><span>np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>seed(<span style=color:#ff9f43>0</span>)
</span></span><span style=display:flex><span>xs <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>uniform(size<span style=color:#ff6ac1>=</span>num_points) <span style=color:#ff6ac1>*</span> <span style=color:#ff9f43>10</span> <span style=color:#ff6ac1>+</span> <span style=color:#ff9f43>2</span>
</span></span><span style=display:flex><span>ys <span style=color:#ff6ac1>=</span> m <span style=color:#ff6ac1>*</span> xs <span style=color:#ff6ac1>+</span> c
</span></span><span style=display:flex><span>err <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>sqrt(ys)
</span></span><span style=display:flex><span>ys <span style=color:#ff6ac1>+=</span> err <span style=color:#ff6ac1>*</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>normal(size<span style=color:#ff6ac1>=</span>num_points)
</span></span></code></pre></div></div><p>Now, let&rsquo;s plot our generated data to make sure it all looks good.</p><div class=width-66 markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>fig, ax <span style=color:#ff6ac1>=</span> plt<span style=color:#ff6ac1>.</span>subplots(figsize<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>8</span>,<span style=color:#ff9f43>3</span>))
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>errorbar(xs, ys, yerr<span style=color:#ff6ac1>=</span>err, fmt<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#39;.&#39;</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Observations&#34;</span>, ms<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>5</span>)
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>legend(frameon<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>False</span>, loc<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>)
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>set_xlabel(<span style=color:#5af78e>&#34;x&#34;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>set_ylabel(<span style=color:#5af78e>&#34;y&#34;</span>);
</span></span></code></pre></div></div><p><div><figure class=rounded><picture><source srcset=/tutorials/bayesianlinearregression/2019-07-27-BayesianLinearRegression_files/2019-07-27-BayesianLinearRegression_3_0_huc09b232597b15d4259b1308a7a5f3838_46796_1920x0_resize_q90_h2_box_3.webp width=1920 height=822 type=image/webp><source srcset=/tutorials/bayesianlinearregression/2019-07-27-BayesianLinearRegression_files/2019-07-27-BayesianLinearRegression_3_0.png width=2788 height=1193 type=image/png><img width=2788 height=1193 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>And it does. Notice that in this data, the further you are along the x-axis, the more uncertainty we have.</p><h2 id=defining-a-model>Defining a model</h2><p>So, let&rsquo;s recall Bayes&rsquo; theorem for a second:</p><p>$$ P(\theta | d) \propto P(d|\theta)P(\theta), $$</p><p>where $\theta$ is our model parametrisation and $d$ is our data. To sub in nomenclature, our posterior is proportional to our likelihood multiplied by our prior. So, we need to come up with a model to describe data, which one would think is fairly straightforward, given we just coded a model to <em>generate</em> our data. But before we jump the gun and code up $y = mx + c$, let us also consider the model $y = \tan(\phi) x + c$.</p><p>Why would we care about whether we use a gradient or an angle? Well, it comes down to simplifying our prior - in our case with no background knowledge we&rsquo;d want to sample all of our parameter space with the same probability. But what happens if we plot uniform probability in the two separate models?</p><p><div><figure class=rounded><picture><source srcset=/tutorials/bayesianlinearregression/2019-07-27-BayesianLinearRegression_files/2019-07-27-BayesianLinearRegression_5_0_hu024911a08bb73cfd5a4d0fcf4c9ce2e7_414939_1920x0_resize_q90_h2_box_3.webp width=1920 height=1082 type=image/webp><source srcset=/tutorials/bayesianlinearregression/2019-07-27-BayesianLinearRegression_files/2019-07-27-BayesianLinearRegression_5_0.png width=2858 height=1610 type=image/png><img width=2858 height=1610 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>Now it seems to me that uniformly sampling the angle, rather than the gradient, gives us an even distribution of coverage over our observational space.</p><p>So, if we lock in that model, we have two parameters of interest: $\theta = \lbrace \phi, c \rbrace$. Next up, we should think about the priors on those two parameters. Luckily, with the little investigation we did before, we can comfortably set flat (uniform) priors on both $\phi$ and $c$ and they will be non-informative. Aka, they will not contribute at all to our fitting locations. Note that we could have pursued the model parametrised by gradient, and simply given a non-uniform prior, but this way is easier. More formally, we have that:</p><p>$$ P(\phi) = U(-\frac{\pi}{2}, \frac{\pi}{2}) \
P(c) = U(-\infty, \infty) $$</p><p>Where yes, we&rsquo;re working in radians. In code, this is also as simple:</p><div class="reduced-code width-31" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>log_prior</span>(xs):
</span></span><span style=display:flex><span>    phi, c <span style=color:#ff6ac1>=</span> xs
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>if</span> np<span style=color:#ff6ac1>.</span>abs(phi) <span style=color:#ff6ac1>&gt;</span> np<span style=color:#ff6ac1>.</span>pi <span style=color:#ff6ac1>/</span> <span style=color:#ff9f43>2</span>:
</span></span><span style=display:flex><span>        <span style=color:#ff6ac1>return</span> <span style=color:#ff6ac1>-</span>np<span style=color:#ff6ac1>.</span>inf
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> <span style=color:#ff9f43>0</span>
</span></span></code></pre></div></div><p>Notice we don&rsquo;t even care about $c$ at all in the code, it can be any value, and the prior is constant over that value. And because Bayes&rsquo; theorem only cares about proportionality, if it doesn&rsquo;t change, we don&rsquo;t worry about it. We only care about $\phi$&rsquo;s boundary conditions for the same reasons, and when it crosses the boundary to a location we say it can&rsquo;t go, we return $-\infty$, which - as this is the log prior, is the same as saying probability zero. It can&rsquo;t happen.</p><p>As a note, we always work in log probability space, not probability space, because the numbers tend to span vast orders of magnitude.</p><p>Now we have the likelihood function $P(d|\theta)$ to think about. If we take the errors as normally distributed (which we know they are), we can write down</p><p>$$ P(d_i|\theta) = \mathcal{N}\left( \frac{y_{i}-[\tan(\phi) x_{i} + c]}{\sigma_{i}}\right), $$</p><p>where $\mathcal{N}$ is the unit normal. This makes the assumption our observations are independent, which holds for this case. Note here that the equation is for a single data point. For a dataset, we would want this for each point:</p><p>$$ P(d|\theta) = \prod_i \mathcal{N}\left( \frac{y_{i}-[\tan(\phi) x_{i} + c]}{\sigma_{i}}\right). $$</p><p>When working in log space, this product simply becomes a sum. Writing out this equation is normally the hard part, implementing it in code is simple:</p><div class="reduced-code width-41" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>from</span> scipy.stats <span style=color:#ff6ac1>import</span> norm
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>log_likelihood</span>(xs, data):
</span></span><span style=display:flex><span>    phi, c <span style=color:#ff6ac1>=</span> xs
</span></span><span style=display:flex><span>    xobs, yobs, eobs <span style=color:#ff6ac1>=</span> data
</span></span><span style=display:flex><span>    model <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>tan(phi) <span style=color:#ff6ac1>*</span> xobs <span style=color:#ff6ac1>+</span> c
</span></span><span style=display:flex><span>    diff <span style=color:#ff6ac1>=</span> model <span style=color:#ff6ac1>-</span> yobs
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> norm<span style=color:#ff6ac1>.</span>logpdf(diff <span style=color:#ff6ac1>/</span> eobs)<span style=color:#ff6ac1>.</span>sum()
</span></span></code></pre></div></div><p>And now we want a function that gets the log posterior, by combining the prior and likelihood. Notice that if the prior comes back as an impossible value, we won&rsquo;t waste time computing the likelihood, we&rsquo;ll just return straight away.</p><div class="reduced-code width-43" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>log_posterior</span>(xs, data):
</span></span><span style=display:flex><span>    prior <span style=color:#ff6ac1>=</span> log_prior(xs)
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>if</span> <span style=color:#ff6ac1>not</span> np<span style=color:#ff6ac1>.</span>isfinite(prior):
</span></span><span style=display:flex><span>        <span style=color:#ff6ac1>return</span> prior
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> prior <span style=color:#ff6ac1>+</span> log_likelihood(xs, data)
</span></span></code></pre></div></div><p>With that, our model is fully defined. We can now try and fit it to the data to see how we go.</p><h2 id=model-fitting>Model Fitting</h2><p>There are so many ways of doing this. Using some MCMC algorithm, using nested sampling, other algorithms&mldr; too many options. Initially I wanted to do this example using <code>dynesty</code> - a new nested sampling package for python. But I realised, better to start off with the simpler <code>emcee</code> implementation to begin with. <code>emcee</code> is an affine-invariant MCMC sampler, and if you want more detail on that, <a href=https://emcee.readthedocs.io/en/latest/>check out its documentation</a>, let&rsquo;s just jump into how you&rsquo;d use it.</p><div class="expanded-code width-84" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>import</span> emcee
</span></span><span style=display:flex><span>ndim <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>2</span>  <span style=color:#78787e># How many parameters we are fitting. This is our dimensionality.</span>
</span></span><span style=display:flex><span>nwalkers <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>50</span>  <span style=color:#78787e># Keep this well above your dimensionality.</span>
</span></span><span style=display:flex><span>p0 <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>uniform(low<span style=color:#ff6ac1>=-</span><span style=color:#ff9f43>1.5</span>, high<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1.5</span>, size<span style=color:#ff6ac1>=</span>(nwalkers, ndim))  <span style=color:#78787e># Start points</span>
</span></span><span style=display:flex><span>sampler <span style=color:#ff6ac1>=</span> emcee<span style=color:#ff6ac1>.</span>EnsembleSampler(nwalkers, ndim, log_posterior, args<span style=color:#ff6ac1>=</span>[(xs, ys, err)])
</span></span><span style=display:flex><span>state <span style=color:#ff6ac1>=</span> sampler<span style=color:#ff6ac1>.</span>run_mcmc(p0, <span style=color:#ff9f43>4000</span>)  <span style=color:#78787e># Tell each walker to take 4000 steps</span>
</span></span><span style=display:flex><span>chain <span style=color:#ff6ac1>=</span> sampler<span style=color:#ff6ac1>.</span>chain[:, <span style=color:#ff9f43>200</span>:, :]  <span style=color:#78787e># Throw out the first 200 steps</span>
</span></span><span style=display:flex><span>flat_chain <span style=color:#ff6ac1>=</span> chain<span style=color:#ff6ac1>.</span>reshape((<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>, ndim))  <span style=color:#78787e># Stack the steps from each walker </span>
</span></span></code></pre></div></div><p>So let&rsquo;s break this down. <code>ndim</code> is the number of parameters we have to fit, and we want to make sure that we have multiple times this for our <code>nwalkers</code> value, where each walker is a tracked position in parameter space that gets explored in a probabilistic fashion. I usually make sure there are a minimum of thirty or so walkers, but the more the merrier.</p><p>Next up, <code>p0</code> - each walker in the process needs to start somewhere! In this case, we pick a random position, it&rsquo;ll move from this quickly. We then make the <code>sampler</code>, and tell each walker in the sampler to take 4000 steps. The walkers should move around the parameter space in a way thats informed by the posterior (given our data). Now, the initial phase where the walkers move from the random positions we set to exploring the space properly is known as burn in, and we want to get rid of it, so we throw out the first 200 of the 4000 steps. How many you throw out depends on your problem, see the <code>emcee</code> documentation for more discussion on this, or just keep reading. Finally, we take the 3D chain (num walkers x num steps x num dimensions) and squish it down to 2D.</p><h2 id=interpreting-chains>Interpreting chains</h2><p>So, we have this &ldquo;chain&rdquo; thing back from the sampler. The important thing to know is that an MCMC samples areas in parameter space proportional to their probability. So a point in $\phi-c$ space which is twice as likely as another will have twice as many samples. There are libraries you can use where you throw in those samples and it will crunch the numbers for you and give you constraints on your parameters. We&rsquo;ll be using one I made, called <code>ChainConsumer</code>.</p><p>Let&rsquo;s check the state of the burn in removal:</p><div class=width-65 markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>from</span> chainconsumer <span style=color:#ff6ac1>import</span> ChainConsumer
</span></span><span style=display:flex><span>c <span style=color:#ff6ac1>=</span> ChainConsumer()
</span></span><span style=display:flex><span>c<span style=color:#ff6ac1>.</span>add_chain(flat_chain, parameters<span style=color:#ff6ac1>=</span>[<span style=color:#5af78e>r</span><span style=color:#5af78e>&#34;$\phi$&#34;</span>, <span style=color:#5af78e>&#34;$c$&#34;</span>], color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;b&#34;</span>)
</span></span><span style=display:flex><span>c<span style=color:#ff6ac1>.</span>add_chain(sampler<span style=color:#ff6ac1>.</span>chain<span style=color:#ff6ac1>.</span>reshape((<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>, ndim)), color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;r&#34;</span>)
</span></span><span style=display:flex><span>c<span style=color:#ff6ac1>.</span>plotter<span style=color:#ff6ac1>.</span>plot_walks(truth<span style=color:#ff6ac1>=</span>[np<span style=color:#ff6ac1>.</span>pi<span style=color:#ff6ac1>/</span><span style=color:#ff9f43>4</span>, <span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>], figsize<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>8</span>,<span style=color:#ff9f43>4</span>));
</span></span></code></pre></div></div><p><div><figure class=rounded><picture><source srcset=/tutorials/bayesianlinearregression/2019-07-27-BayesianLinearRegression_files/2019-07-27-BayesianLinearRegression_15_1_hu1b0e5f119813164178da9ff600971a31_2866581_1920x0_resize_q90_h2_box_3.webp width=1920 height=942 type=image/webp><source srcset=/tutorials/bayesianlinearregression/2019-07-27-BayesianLinearRegression_files/2019-07-27-BayesianLinearRegression_15_1.png width=2824 height=1385 type=image/png><img width=2824 height=1385 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>So here we can see the walks plotted, also known as a trace plot. The blue contains all the samples from the chain we removed the burn in from, and the red doesn&rsquo;t have it removed. Notice all the little ticks in $\phi$ and $c$ - thats the random position of each walker (there will be fifty ticks, one for each walker) as they quickly converge to the right area of parameter space. The fact we don&rsquo;t see this in the blue means we&rsquo;ve probably removed all burn in. There are diagnostics to check this in <code>ChainConsumer</code> too, but its not needed for this simple example.</p><p>Up next - let&rsquo;s get actual parameter constraints from this!</p><div class="reduced-code width-54" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>c <span style=color:#ff6ac1>=</span> ChainConsumer()
</span></span><span style=display:flex><span>c<span style=color:#ff6ac1>.</span>add_chain(flat_chain, parameters<span style=color:#ff6ac1>=</span>[<span style=color:#5af78e>r</span><span style=color:#5af78e>&#34;$\phi$&#34;</span>, <span style=color:#5af78e>&#34;$c$&#34;</span>])
</span></span><span style=display:flex><span>c<span style=color:#ff6ac1>.</span>configure(contour_labels<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;confidence&#34;</span>)
</span></span><span style=display:flex><span>c<span style=color:#ff6ac1>.</span>plotter<span style=color:#ff6ac1>.</span>plot(truth<span style=color:#ff6ac1>=</span>[np<span style=color:#ff6ac1>.</span>pi<span style=color:#ff6ac1>/</span><span style=color:#ff9f43>4</span>, <span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>], figsize<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2.0</span>)
</span></span><span style=display:flex><span>summary <span style=color:#ff6ac1>=</span> c<span style=color:#ff6ac1>.</span>analysis<span style=color:#ff6ac1>.</span>get_summary()
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> key, value <span style=color:#ff6ac1>in</span> summary<span style=color:#ff6ac1>.</span>items():
</span></span><span style=display:flex><span>    <span style=color:#ff5c57>print</span>(key, value)
</span></span></code></pre></div></div><pre><code>$\phi$ [0.7726110275094517, 0.8463251378245243, 0.9095535101305535]
$c$ [-2.71495855668443, -1.8055414255827276, -0.7402022694695001]
</code></pre><p><div><figure class=rounded><picture><source srcset=/tutorials/bayesianlinearregression/2019-07-27-BayesianLinearRegression_files/2019-07-27-BayesianLinearRegression_17_2_hu1b9140b04259435942d3870aa154ae9e_201320_1920x0_resize_q90_h2_box_3.webp width=1920 height=1984 type=image/webp><source srcset=/tutorials/bayesianlinearregression/2019-07-27-BayesianLinearRegression_files/2019-07-27-BayesianLinearRegression_17_2.png width=2355 height=2433 type=image/png><img width=2355 height=2433 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>So how do we read this? Well, if you look at the summary printed, that gives the bounds for the lower uncertainty, maximum value, and upper uncertainty respectively (uncertainty being the 68% confidence levels). In the actual plot, you can see a 2D surface which represents our posterior. For example, the inner circle, labelled 68%, says that 68% of the time the true value for $\phi$ and $c$ will lie in that contour. 95% of the time it will lie in the broader contour.</p><p>Finally, one thing we might want to do is to plot the best fitting model and its uncertainty against our data. The best fit part is easy, its the uncertainty on our model that is the trickier part. What we&rsquo;ll do is sample from our chain over a variety of x-values to determine the effect our parameter uncertainty has in observational space. Easier to do than explain.</p><div class="expanded-code width-85" markdown=1><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x_vals <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>linspace(<span style=color:#ff9f43>2</span>, <span style=color:#ff9f43>12</span>, <span style=color:#ff9f43>30</span>)
</span></span><span style=display:flex><span><span style=color:#78787e># Calculate best fit</span>
</span></span><span style=display:flex><span>phi_best <span style=color:#ff6ac1>=</span> summary[<span style=color:#5af78e>r</span><span style=color:#5af78e>&#34;$\phi$&#34;</span>][<span style=color:#ff9f43>1</span>]
</span></span><span style=display:flex><span>c_best <span style=color:#ff6ac1>=</span> summary[<span style=color:#5af78e>r</span><span style=color:#5af78e>&#34;$c$&#34;</span>][<span style=color:#ff9f43>1</span>]
</span></span><span style=display:flex><span>best_fit <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>tan(phi_best) <span style=color:#ff6ac1>*</span> x_vals <span style=color:#ff6ac1>+</span> c_best
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Calculate range our uncertainty gives using 2D matrix multplication</span>
</span></span><span style=display:flex><span>realisations <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>tan(flat_chain[:, <span style=color:#ff9f43>0</span>][:, <span style=color:#ff6ac1>None</span>]) <span style=color:#ff6ac1>*</span> x_vals <span style=color:#ff6ac1>+</span> flat_chain[:, <span style=color:#ff9f43>1</span>][:, <span style=color:#ff6ac1>None</span>]
</span></span><span style=display:flex><span>bounds <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>percentile(realisations, <span style=color:#ff9f43>100</span> <span style=color:#ff6ac1>*</span> norm<span style=color:#ff6ac1>.</span>cdf([<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>2</span>, <span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>2</span>]), axis<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Plot everything</span>
</span></span><span style=display:flex><span>fig, ax <span style=color:#ff6ac1>=</span> plt<span style=color:#ff6ac1>.</span>subplots()
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>errorbar(xs, ys, yerr<span style=color:#ff6ac1>=</span>err, fmt<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#39;.&#39;</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Observations&#34;</span>, ms<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>5</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>)
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>plot(x_vals, best_fit, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Best Fit&#34;</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;#ffb638&#34;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>plot(x_vals, x_vals <span style=color:#ff6ac1>-</span> <span style=color:#ff9f43>1</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Truth&#34;</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>, ls<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;:&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>fill_between(x_vals, bounds[<span style=color:#ff9f43>0</span>, :], bounds[<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>, :], 
</span></span><span style=display:flex><span>                 label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;95\</span><span style=color:#5af78e>% u</span><span style=color:#5af78e>ncertainty&#34;</span>, fc<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;#03A9F4&#34;</span>, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.1</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>fill_between(x_vals, bounds[<span style=color:#ff9f43>1</span>, :], bounds[<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>2</span>, :], 
</span></span><span style=display:flex><span>                 label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;68\</span><span style=color:#5af78e>% u</span><span style=color:#5af78e>ncertainty&#34;</span>, fc<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;#0288D1&#34;</span>, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.4</span>)
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>legend(frameon<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>False</span>, loc<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>)
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>set_xlabel(<span style=color:#5af78e>&#34;x&#34;</span>), ax<span style=color:#ff6ac1>.</span>set_ylabel(<span style=color:#5af78e>&#34;y&#34;</span>), ax<span style=color:#ff6ac1>.</span>set_xlim(<span style=color:#ff9f43>2</span>, <span style=color:#ff9f43>12</span>);
</span></span></code></pre></div></div><p><div><figure class="img-main rounded"><picture><source srcset=/tutorials/bayesianlinearregression/cover_hu7287fa30c468f38ca1d54421c39f23f5_158462_1920x0_resize_q90_h2_box_3.webp width=1920 height=1018 type=image/webp><source srcset=/tutorials/bayesianlinearregression/cover.png width=2787 height=1477 type=image/png><img width=2787 height=1477 loading=lazy decoding=async alt=png src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mPMn7vLFAAFRgH97g1QygAAAABJRU5ErkJggg=="></picture></figure></div></p><p>Notice how even with a linear model, our uncertainty is not just linear, it is smallest in the center of the dataset, as we might expect if we imagine the fit rocking the line like a see-saw during the fitting process. To reiterate, what we did to calculate the uncertainty was - instead of using some summary of the uncertainty like the standard deviation - we used the entire posterior surface to generate thousands of models, and looked at their uncertainty (using the <code>percentile</code>) function to get the $1-$ and $2-$ $\sigma$ bounds (the <code>norm.cdf</code> part) to display on the plot. <a href=https://cosmiccoding.com.au/tutorial/2019/08/02/Propagating.html>For more examples on this methd of propagating uncertainty, see here</a>.</p><p>And that&rsquo;s it, those are the basics.</p><ol><li>Define your model, think about parametrisation, priors and likelihoods</li><li>Create a sampler and sample your parameter space</li><li>Determine parameter constraints from your samples.</li><li>Plot everything.</li></ol><hr><p>For your convenience, here&rsquo;s the code in one block:</p><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>import</span> matplotlib.pyplot <span style=color:#ff6ac1>as</span> plt
</span></span><span style=display:flex><span><span style=color:#ff6ac1>import</span> numpy <span style=color:#ff6ac1>as</span> np
</span></span><span style=display:flex><span>num_points <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>20</span>
</span></span><span style=display:flex><span>m, c <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>tan(np<span style=color:#ff6ac1>.</span>pi <span style=color:#ff6ac1>/</span> <span style=color:#ff9f43>4</span>), <span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>  <span style=color:#78787e># So our angle is 45 degrees, m = 1</span>
</span></span><span style=display:flex><span>np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>seed(<span style=color:#ff9f43>0</span>)
</span></span><span style=display:flex><span>xs <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>uniform(size<span style=color:#ff6ac1>=</span>num_points) <span style=color:#ff6ac1>*</span> <span style=color:#ff9f43>10</span> <span style=color:#ff6ac1>+</span> <span style=color:#ff9f43>2</span>
</span></span><span style=display:flex><span>ys <span style=color:#ff6ac1>=</span> m <span style=color:#ff6ac1>*</span> xs <span style=color:#ff6ac1>+</span> c
</span></span><span style=display:flex><span>err <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>sqrt(ys)
</span></span><span style=display:flex><span>ys <span style=color:#ff6ac1>+=</span> err <span style=color:#ff6ac1>*</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>normal(size<span style=color:#ff6ac1>=</span>num_points)
</span></span><span style=display:flex><span>fig, ax <span style=color:#ff6ac1>=</span> plt<span style=color:#ff6ac1>.</span>subplots(figsize<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>8</span>,<span style=color:#ff9f43>3</span>))
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>errorbar(xs, ys, yerr<span style=color:#ff6ac1>=</span>err, fmt<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#39;.&#39;</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Observations&#34;</span>, ms<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>5</span>)
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>legend(frameon<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>False</span>, loc<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>)
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>set_xlabel(<span style=color:#5af78e>&#34;x&#34;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>set_ylabel(<span style=color:#5af78e>&#34;y&#34;</span>);
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>log_prior</span>(xs):
</span></span><span style=display:flex><span>    phi, c <span style=color:#ff6ac1>=</span> xs
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>if</span> np<span style=color:#ff6ac1>.</span>abs(phi) <span style=color:#ff6ac1>&gt;</span> np<span style=color:#ff6ac1>.</span>pi <span style=color:#ff6ac1>/</span> <span style=color:#ff9f43>2</span>:
</span></span><span style=display:flex><span>        <span style=color:#ff6ac1>return</span> <span style=color:#ff6ac1>-</span>np<span style=color:#ff6ac1>.</span>inf
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> <span style=color:#ff9f43>0</span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> scipy.stats <span style=color:#ff6ac1>import</span> norm
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>log_likelihood</span>(xs, data):
</span></span><span style=display:flex><span>    phi, c <span style=color:#ff6ac1>=</span> xs
</span></span><span style=display:flex><span>    xobs, yobs, eobs <span style=color:#ff6ac1>=</span> data
</span></span><span style=display:flex><span>    model <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>tan(phi) <span style=color:#ff6ac1>*</span> xobs <span style=color:#ff6ac1>+</span> c
</span></span><span style=display:flex><span>    diff <span style=color:#ff6ac1>=</span> model <span style=color:#ff6ac1>-</span> yobs
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> norm<span style=color:#ff6ac1>.</span>logpdf(diff <span style=color:#ff6ac1>/</span> eobs)<span style=color:#ff6ac1>.</span>sum()
</span></span><span style=display:flex><span><span style=color:#ff6ac1>def</span> <span style=color:#57c7ff>log_posterior</span>(xs, data):
</span></span><span style=display:flex><span>    prior <span style=color:#ff6ac1>=</span> log_prior(xs)
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>if</span> <span style=color:#ff6ac1>not</span> np<span style=color:#ff6ac1>.</span>isfinite(prior):
</span></span><span style=display:flex><span>        <span style=color:#ff6ac1>return</span> prior
</span></span><span style=display:flex><span>    <span style=color:#ff6ac1>return</span> prior <span style=color:#ff6ac1>+</span> log_likelihood(xs, data)
</span></span><span style=display:flex><span><span style=color:#ff6ac1>import</span> emcee
</span></span><span style=display:flex><span>ndim <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>2</span>  <span style=color:#78787e># How many parameters we are fitting. This is our dimensionality.</span>
</span></span><span style=display:flex><span>nwalkers <span style=color:#ff6ac1>=</span> <span style=color:#ff9f43>50</span>  <span style=color:#78787e># Keep this well above your dimensionality.</span>
</span></span><span style=display:flex><span>p0 <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>random<span style=color:#ff6ac1>.</span>uniform(low<span style=color:#ff6ac1>=-</span><span style=color:#ff9f43>1.5</span>, high<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1.5</span>, size<span style=color:#ff6ac1>=</span>(nwalkers, ndim))  <span style=color:#78787e># Start points</span>
</span></span><span style=display:flex><span>sampler <span style=color:#ff6ac1>=</span> emcee<span style=color:#ff6ac1>.</span>EnsembleSampler(nwalkers, ndim, log_posterior, args<span style=color:#ff6ac1>=</span>[(xs, ys, err)])
</span></span><span style=display:flex><span>state <span style=color:#ff6ac1>=</span> sampler<span style=color:#ff6ac1>.</span>run_mcmc(p0, <span style=color:#ff9f43>4000</span>)  <span style=color:#78787e># Tell each walker to take 4000 steps</span>
</span></span><span style=display:flex><span>chain <span style=color:#ff6ac1>=</span> sampler<span style=color:#ff6ac1>.</span>chain[:, <span style=color:#ff9f43>200</span>:, :]  <span style=color:#78787e># Throw out the first 200 steps</span>
</span></span><span style=display:flex><span>flat_chain <span style=color:#ff6ac1>=</span> chain<span style=color:#ff6ac1>.</span>reshape((<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>, ndim))  <span style=color:#78787e># Stack the steps from each walker </span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>from</span> chainconsumer <span style=color:#ff6ac1>import</span> ChainConsumer
</span></span><span style=display:flex><span>c <span style=color:#ff6ac1>=</span> ChainConsumer()
</span></span><span style=display:flex><span>c<span style=color:#ff6ac1>.</span>add_chain(flat_chain, parameters<span style=color:#ff6ac1>=</span>[<span style=color:#5af78e>r</span><span style=color:#5af78e>&#34;$\phi$&#34;</span>, <span style=color:#5af78e>&#34;$c$&#34;</span>], color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;b&#34;</span>)
</span></span><span style=display:flex><span>c<span style=color:#ff6ac1>.</span>add_chain(sampler<span style=color:#ff6ac1>.</span>chain<span style=color:#ff6ac1>.</span>reshape((<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>, ndim)), color<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;r&#34;</span>)
</span></span><span style=display:flex><span>c<span style=color:#ff6ac1>.</span>plotter<span style=color:#ff6ac1>.</span>plot_walks(truth<span style=color:#ff6ac1>=</span>[np<span style=color:#ff6ac1>.</span>pi<span style=color:#ff6ac1>/</span><span style=color:#ff9f43>4</span>, <span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>], figsize<span style=color:#ff6ac1>=</span>(<span style=color:#ff9f43>8</span>,<span style=color:#ff9f43>4</span>));
</span></span><span style=display:flex><span>c <span style=color:#ff6ac1>=</span> ChainConsumer()
</span></span><span style=display:flex><span>c<span style=color:#ff6ac1>.</span>add_chain(flat_chain, parameters<span style=color:#ff6ac1>=</span>[<span style=color:#5af78e>r</span><span style=color:#5af78e>&#34;$\phi$&#34;</span>, <span style=color:#5af78e>&#34;$c$&#34;</span>])
</span></span><span style=display:flex><span>c<span style=color:#ff6ac1>.</span>configure(contour_labels<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;confidence&#34;</span>)
</span></span><span style=display:flex><span>c<span style=color:#ff6ac1>.</span>plotter<span style=color:#ff6ac1>.</span>plot(truth<span style=color:#ff6ac1>=</span>[np<span style=color:#ff6ac1>.</span>pi<span style=color:#ff6ac1>/</span><span style=color:#ff9f43>4</span>, <span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>], figsize<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2.0</span>)
</span></span><span style=display:flex><span>summary <span style=color:#ff6ac1>=</span> c<span style=color:#ff6ac1>.</span>analysis<span style=color:#ff6ac1>.</span>get_summary()
</span></span><span style=display:flex><span><span style=color:#ff6ac1>for</span> key, value <span style=color:#ff6ac1>in</span> summary<span style=color:#ff6ac1>.</span>items():
</span></span><span style=display:flex><span>    <span style=color:#ff5c57>print</span>(key, value)
</span></span><span style=display:flex><span>x_vals <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>linspace(<span style=color:#ff9f43>2</span>, <span style=color:#ff9f43>12</span>, <span style=color:#ff9f43>30</span>)
</span></span><span style=display:flex><span><span style=color:#78787e># Calculate best fit</span>
</span></span><span style=display:flex><span>phi_best <span style=color:#ff6ac1>=</span> summary[<span style=color:#5af78e>r</span><span style=color:#5af78e>&#34;$\phi$&#34;</span>][<span style=color:#ff9f43>1</span>]
</span></span><span style=display:flex><span>c_best <span style=color:#ff6ac1>=</span> summary[<span style=color:#5af78e>r</span><span style=color:#5af78e>&#34;$c$&#34;</span>][<span style=color:#ff9f43>1</span>]
</span></span><span style=display:flex><span>best_fit <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>tan(phi_best) <span style=color:#ff6ac1>*</span> x_vals <span style=color:#ff6ac1>+</span> c_best
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Calculate range our uncertainty gives using 2D matrix multplication</span>
</span></span><span style=display:flex><span>realisations <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>tan(flat_chain[:, <span style=color:#ff9f43>0</span>][:, <span style=color:#ff6ac1>None</span>]) <span style=color:#ff6ac1>*</span> x_vals <span style=color:#ff6ac1>+</span> flat_chain[:, <span style=color:#ff9f43>1</span>][:, <span style=color:#ff6ac1>None</span>]
</span></span><span style=display:flex><span>bounds <span style=color:#ff6ac1>=</span> np<span style=color:#ff6ac1>.</span>percentile(realisations, <span style=color:#ff9f43>100</span> <span style=color:#ff6ac1>*</span> norm<span style=color:#ff6ac1>.</span>cdf([<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>2</span>, <span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>1</span>, <span style=color:#ff9f43>2</span>]), axis<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Plot everything</span>
</span></span><span style=display:flex><span>fig, ax <span style=color:#ff6ac1>=</span> plt<span style=color:#ff6ac1>.</span>subplots()
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>errorbar(xs, ys, yerr<span style=color:#ff6ac1>=</span>err, fmt<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#39;.&#39;</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Observations&#34;</span>, ms<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>5</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>)
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>plot(x_vals, best_fit, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Best Fit&#34;</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;#ffb638&#34;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>plot(x_vals, x_vals <span style=color:#ff6ac1>-</span> <span style=color:#ff9f43>1</span>, label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;Truth&#34;</span>, c<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;w&#34;</span>, ls<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;:&#34;</span>, lw<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>1</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>fill_between(x_vals, bounds[<span style=color:#ff9f43>0</span>, :], bounds[<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>1</span>, :], 
</span></span><span style=display:flex><span>                 label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;95\</span><span style=color:#5af78e>% u</span><span style=color:#5af78e>ncertainty&#34;</span>, fc<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;#03A9F4&#34;</span>, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.1</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff6ac1>.</span>fill_between(x_vals, bounds[<span style=color:#ff9f43>1</span>, :], bounds[<span style=color:#ff6ac1>-</span><span style=color:#ff9f43>2</span>, :], 
</span></span><span style=display:flex><span>                 label<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;68\</span><span style=color:#5af78e>% u</span><span style=color:#5af78e>ncertainty&#34;</span>, fc<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;#0288D1&#34;</span>, alpha<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>0.4</span>)
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>legend(frameon<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>False</span>, loc<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>2</span>)
</span></span><span style=display:flex><span>ax<span style=color:#ff6ac1>.</span>set_xlabel(<span style=color:#5af78e>&#34;x&#34;</span>), ax<span style=color:#ff6ac1>.</span>set_ylabel(<span style=color:#5af78e>&#34;y&#34;</span>), ax<span style=color:#ff6ac1>.</span>set_xlim(<span style=color:#ff9f43>2</span>, <span style=color:#ff9f43>12</span>);
</span></span></code></pre></div><div class="relative max-w-xl mx-auto my-20"><div class="fancy_card horizontal"><div class=card_translator><div class="card_rotator tiny_rot card_layer"><div class="card_layer newsletter p-2"><div class="newsletter-inner h-full"><div class="relative h-full sib-form-container" id=sib-form-container><div class="mb-6 lg:mb-12 text-center"><h3 class="text-main-200 mb-2 lg:mb-8">Stay in the loop!</h3><p class="text-main-100 text-lg text-opacity-70">For new releases, exclusive giveaways, and reviews.</p></div><form action="https://Cosmiccoding.us5.list-manage.com/subscribe/post?u=aebe5de1d2e40dccb3aeca491&id=3987dd4a1f" method=post id=mc-embedded-subscribe-form name=mc-embedded-subscribe-form class="validate w-full" target=_blank novalidate><div><input type=email class="w-full appearance-none bg-main-600 mb-4 border border-main-600 bg-opacity-5 focus:border-main-300 rounded-sm px-4 py-3 text-main-100 placeholder-main-100 required" placeholder="Your best email…" aria-label="Your best email…" name=EMAIL required data-required=true id=EMAIL><div style=position:absolute;left:-5000px aria-hidden=true><input type=text name=b_aebe5de1d2e40dccb3aeca491_3987dd4a1f tabindex=-1></div><input type=submit value=Subscribe name=subscribe id=mc-embedded-subscribe class="button btn bg-main-600 hover:bg-main-400 shadow w-full"></div><p id=mce-error-response style=display:none class="text-center mt-2 opacity-50 text-main-200">There was a problem, please try again.</p><p id=mce-success-response style=display:none class="text-center mt-2 opacity-50 text-main-200">Thanks mate, won't let ya down.</p></form></div></div></div><div class="card_layer card_effect card_soft_glare" style=pointer-events:none></div></div></div></div></div></div><script type=text/x-mathjax-config>
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script></div><script language=javascript type=text/javascript src=https://cosmiccoding.com.au/js/main.min.11673d10a962fb5205229607e62ea1d23424d35dad33db7bfe2a09a63d5409fa.js></script>
<script async defer src="https://www.googletagmanager.com/gtag/js?id=G-GRX6QE03YR"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-GRX6QE03YR")</script></div></body></html>