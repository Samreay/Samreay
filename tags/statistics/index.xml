<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>statistics on Samuel Hinton</title><link>https://cosmiccoding.com.au/tags/statistics/</link><description>Recent content in statistics on Samuel Hinton</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 26 Jan 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://cosmiccoding.com.au/tags/statistics/index.xml" rel="self" type="application/rss+xml"/><item><title>Finding the best Wordle opening</title><link>https://cosmiccoding.com.au/tutorials/wordle/</link><pubDate>Wed, 26 Jan 2022 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/wordle/</guid><description>Recently, I&amp;mdash;like many others&amp;mdash;have indulged in the Wordle competitions with friends. Which raised the question of how to open and ensure that I crushed all my mates with my vastly superior score.
So here we are. About to do an unneccessary but fun deep dive into Wordle words.
TL;DR If you care both about getting letters, and getting them in the right location, start with &amp;ldquo;lares&amp;rdquo; or &amp;ldquo;tares&amp;rdquo;.
If you try &amp;ldquo;lares&amp;rdquo; and get nothing, try &amp;ldquo;tonic&amp;rdquo; or &amp;ldquo;point&amp;rdquo;.</description></item><item><title>Low Poly Art</title><link>https://cosmiccoding.com.au/tutorials/lowpoly/</link><pubDate>Mon, 22 Mar 2021 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/lowpoly/</guid><description>So this is the goal for this tutorial - to turn a beautiful wallpaper (or three) into beautiful low poly art.
The approach we are going to take to get to this is fairly simple.
Load our image in Manipulate the image to highlight edges and areas of detail Draw potential vertices from those areas Calculate triangles from vertices Determine the colour of the triangles Plot the image Lets get our imports out of the way and then power through each section:</description></item><item><title>Genetic Algorithms 2: Painting Vermeer</title><link>https://cosmiccoding.com.au/tutorials/genetic_part_two/</link><pubDate>Sat, 20 Mar 2021 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/genetic_part_two/</guid><description>This is what we&amp;rsquo;re going to make in this tutorial. Building on from the previous discussion in part one, we now add population and genetic mixing into the algorithm.
In the prior article we evolved a painting by the process of having a single organism that we mutated over time. We aim to improve this algorithm in this step by adding multiple different organisms into the population, and allowing those organisms to mate and produce offspring.</description></item><item><title/><link>https://cosmiccoding.com.au/tutorials/genetic_part_one/</link><pubDate>Fri, 19 Mar 2021 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/genetic_part_one/</guid><description>This is what we&amp;rsquo;re going to make in this tutorial. It may not look like much, but then again, this is but the first step into genetic algorithms. If you&amp;rsquo;ve guessed that this is the Starry Night (or bothered to read the title, description or anything else), fantastic.
The idea behind Genetic Algorithms is simple - each algorithm can be evaluated to get its fitness, and algorithms are mutated, bred and culled, according to their performance.</description></item><item><title>Linear Regression Regularization Explained</title><link>https://cosmiccoding.com.au/tutorials/linear_regression_regularisation/</link><pubDate>Sun, 27 Dec 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/linear_regression_regularisation/</guid><description>In this small write up, I&amp;rsquo;ll be trying to fill a bit of a void I&amp;rsquo;ve seen online. That is, there are plenty of definitions of lasso, ridge, and elastic regularization, but most of them aren&amp;rsquo;t accompanied by useful examples showing when these terms become critically important! To address this, we&amp;rsquo;re going to look at regularization using three different use cases:
A perfect dataset! White noise on top of a direct linear relationship.</description></item><item><title>An Introduction to Gaussian Processes</title><link>https://cosmiccoding.com.au/tutorials/gaussian_processes/</link><pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/gaussian_processes/</guid><description>In this little write up, we&amp;rsquo;ll explore, construct and utilise Gaussian Processes for some simple interpolation models. The goal is - at the end - to know how they work under the hood, how they are trained, and how you can use them in weird and wonderful ways.
I&amp;rsquo;ll go through some basic interpolation, covariance and correlation concepts first. If they&amp;rsquo;re all familiar to you, scroll down the Gaussian Process section.</description></item><item><title>Principle Component Analaysis Explained</title><link>https://cosmiccoding.com.au/tutorials/pca/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/pca/</guid><description>In this small write up, we&amp;rsquo;ll cover Principal Component Analysis from its mathematical routes, visual explanations and how to effectively utilise PCA using the sklearn library. We&amp;rsquo;ll show PCA works by doing it manually in python without losing ourself in the mathematics!
What is PCA? PCA is a way of taking a dataset (a collection of points in some parameter/feature space) and determining what are the &amp;ldquo;principal components&amp;rdquo;. These principal components represent vectors that encapsulate the information in your dataset.</description></item><item><title>Fuzzy String Matching</title><link>https://cosmiccoding.com.au/tutorials/fuzzy_string_matching/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/fuzzy_string_matching/</guid><description>In this tweet, Steven Rich pointed out that Philadelphia is spelled at least 57 different ways in the PPP load data, and that this represents both a challenge to fix on the back-end, and a perfect example of why you should do as much work on the front-end to get better input.
In this write up, we&amp;rsquo;ll figure out an easy way of fixing up these spelling issues to produce a far better dataset to work on using the python library FuzzyWuzzy.</description></item><item><title>Training a Neural Network Embedding Layer with Keras</title><link>https://cosmiccoding.com.au/tutorials/encoding_colours/</link><pubDate>Sun, 26 Jul 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/encoding_colours/</guid><description>This little write is designed to try and explain what embeddings are, and how we can train a naive version of an embedding to understand and visualise the process. We&amp;rsquo;ll do this using a colour dataset, Keras and good old-fashioned matplotlib.
Introduction Let&amp;rsquo;s start simple: What is an embedding?
An embedding is a way to represent some categorical feature (like a word), as a dense parameter. Specifically, this is normally a unit vector in a high dimensional hypersphere.</description></item><item><title>PR vs ROC Curves - Which to Use?</title><link>https://cosmiccoding.com.au/tutorials/pr_vs_roc_curves/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/pr_vs_roc_curves/</guid><description>PR curves and ROC diagrams are presented everywhere in the machine learning sphere, and are often used relatively interchangably. In many cases, this is fine, because they are both providing information on the general question &amp;ldquo;How good is my classifier?&amp;rdquo;. But like many things which are often fine to do, there are cases where we&amp;rsquo;d want to specifically show either a PR or ROC curve.
So in the next few sections, we&amp;rsquo;ll generate a mock dataset, generate PR and ROC curves for them for two different classifiers, go into the definition of both of PR and ROC curves (both the intuition and mathematics), and then we&amp;rsquo;ll go how they subtly differ with another code example.</description></item><item><title>An Introduction to Logistic Regression</title><link>https://cosmiccoding.com.au/tutorials/logistic_regression/</link><pubDate>Sat, 11 Jul 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/logistic_regression/</guid><description>In this small write up, we&amp;rsquo;ll cover logistic functions, probabilities vs odds, logit functions, and how to perform logistic regression in Python.
Logistic regression is a method of calculating the probability that an event will pass or fail. That is, we utilise it for dichotomous results - 0 and 1, pass or fail. Is this patient going to survive or not? Is this email spam or not? This is specifically called binary logistic regression, and is important to note because we can do logistic regression in other contexts.</description></item><item><title>Introduction to Naive Bayes</title><link>https://cosmiccoding.com.au/tutorials/naivebayes/</link><pubDate>Sat, 11 Jul 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/naivebayes/</guid><description>Naive Bayes is a simple, extraordinarily fast, and incredibly useful categorical classification tool. The underlying crux of Naive Bayes is that we assume feature independence, plug it into Bayes theorem, and the math that falls out is incredibly simple.
The background math Let&amp;rsquo;s start with the famous Bayes Theorem:
$$ P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)} $$
On the left hand side, we have the posterior: the probability of the output given the input.</description></item><item><title>Monte-Carlo Integration made easy</title><link>https://cosmiccoding.com.au/tutorials/monte_carlo_integration/</link><pubDate>Sun, 21 Jun 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/monte_carlo_integration/</guid><description>Integrating a function is tricky. A lot of the time, the math is beyond us. Or beyond me, at the very least, and so I turn to my computer, placing the burden on its silent, silicon shoulders. And yet this isn&amp;rsquo;t the end of it, because there are a host of ways to perform numerical integration.
Do we want the simple rectangle rule? The superior trapezoidal rule? Simpson&amp;rsquo;s rule? Do we want to adaptively sample?</description></item><item><title>A visual tutorial of Inversion Sampling</title><link>https://cosmiccoding.com.au/tutorials/inversion_sampling/</link><pubDate>Fri, 19 Jun 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/inversion_sampling/</guid><description>Inversion sampling is a simple and very efficient way to generate samples of some arbitrary probability function. Unlike rejection sampling, there are no rejected samples, and by leveraing common python libraries, we don&amp;rsquo;t even have to do any tricky integrals or function inversion.
$$ P(x) = 2 x $$
between 0 and 1. It just happens this does integrate to 1, what luck! Now, the CDF (cumulative distribution function) is the integral of the probability density function (what you see above), integrated from the lower bound - in our case the bounds are 0 to 1, so the CDF is</description></item><item><title>A visual tutorial of Rejection Sampling</title><link>https://cosmiccoding.com.au/tutorials/rejection_sampling/</link><pubDate>Tue, 16 Jun 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/rejection_sampling/</guid><description>Rejection sampling is the conceptually simplest way to generate samples of some arbitrary probability function without having to do any transformations. No integration, no trickery, you simply trade computational efficiency away to keep everything as simple as possible.
Let&amp;rsquo;s pick a super simple example: let&amp;rsquo;s say you want to sample from the function
$$ f(x) = 1.2 - x^4 $$
between 0 and 1. It just happens this does integrate to 1, what luck!</description></item><item><title>Forward Modelling for Supernova Cosmology</title><link>https://cosmiccoding.com.au/tutorials/forwardmodelling/</link><pubDate>Wed, 08 Apr 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/forwardmodelling/</guid><description>So in this example, I&amp;rsquo;m simply trying to demonstrate the viability of forward models with supernova cosmology. In this, I&amp;rsquo;ll be simplifying a lot, and in this case, ignoring proper treatment of uncertainty contributions from Monte-Carlo uncertainty. We&amp;rsquo;ll start with a simple model in which simulated supernova have redshift, magnitude and colour, apply selection effects, and ensure that we can recover input cosmology without having to resimulate by exploiting the fact that our selection effects operate in the observer frame of reference.</description></item><item><title>A/B Test Significance in Python</title><link>https://cosmiccoding.com.au/tutorials/abtests/</link><pubDate>Sun, 12 Jan 2020 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/abtests/</guid><description>Recently I was asked to talk about A/B tests for my Python for Statistical Analysis course. Given my travel schedule, leaving me bereft of my microphone, I thought it would be better to condense down A/B tests into a tutorial or two.
In this little write up, we&amp;rsquo;ll cover what an A/B test is, run through it in first principles with frequentist hypothesis testing, apply some existing scipy tests to speed the process up, and then at the end we&amp;rsquo;ll approach the problem in a Bayesian framework.</description></item></channel></rss>