<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>bayesian on Samuel Hinton</title><link>https://cosmiccoding.com.au/tags/bayesian/</link><description>Recent content in bayesian on Samuel Hinton</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 02 Aug 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://cosmiccoding.com.au/tags/bayesian/index.xml" rel="self" type="application/rss+xml"/><item><title>Propagating (non-gaussian) uncertainty</title><link>https://cosmiccoding.com.au/tutorials/propagating/</link><pubDate>Fri, 02 Aug 2019 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/propagating/</guid><description>You&amp;rsquo;ve fit a model to some data. You have some mean, covariance or samples characterising that fit, but now how can you clearly show the $1$ and $2\sigma$ confidence regions of this fit back onto your model. In other words, how do you get this sort of image back at the end:
Let&amp;rsquo;s run through a bunch of different ways, depending on how you&amp;rsquo;ve fit your model to the data. Firstly, let&amp;rsquo;s generate a model and some fake data to fit it with.</description></item><item><title>Bayesian Sample Selection Effects</title><link>https://cosmiccoding.com.au/tutorials/sampleselectionbias/</link><pubDate>Tue, 30 Jul 2019 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/sampleselectionbias/</guid><description>In a perfect world our experiments would capture all the data that exists. This is not a perfect world, and we miss a lot of data. Let&amp;rsquo;s consider one method of accounting for this in a Bayesian formalism - integrating it out.
Let&amp;rsquo;s begin with a motivational dataset.
So it looks like for our example data, we&amp;rsquo;ve got some gaussian-like distribution of $x$ observations, but at some point it seems like our instrument is unable to pick up the observations.</description></item><item><title>Bayesian Linear Regression in Python</title><link>https://cosmiccoding.com.au/tutorials/bayesianlinearregression/</link><pubDate>Sat, 27 Jul 2019 00:00:00 +0000</pubDate><guid>https://cosmiccoding.com.au/tutorials/bayesianlinearregression/</guid><description>Bayesian linear regression is a common topic, but allow me to put my own spin on it. We&amp;rsquo;ll start at generating some data, defining a model, fitting it and plotting the results. It shouldn&amp;rsquo;t take long.
Generating Data Let&amp;rsquo;s start by generating some experimental data. For simplicity, let us assume some underlying process generates samples $f(x) = mx + c$ and our observations have some given Gaussian error $\sigma$.
import matplotlib.</description></item></channel></rss>